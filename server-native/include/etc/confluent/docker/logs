➜  server-native git:(7.4.x) ✗ docker run -e CONFLUENT_COMMAND_TOPIC_REPLICATION=1 -e CONFLUENT_LINK_METADATA_TOPIC_REPLICATION_FACTOR=1  -p 9092:9092 cp-server-native
/etc/confluent/docker/run: line 17: /etc/confluent/docker/bash-config: No such file or directory
===> User
uid=0(root) gid=0(root) groups=0(root)
===> Configuring ...
/etc/confluent/docker/configure: line 17: /etc/confluent/docker/bash-config: No such file or directory
Running in Zookeeper mode...
/etc/confluent/docker/configure: /usr/local/bin/dub: /usr/bin/python3: bad interpreter: No such file or directory
/etc/confluent/docker/configure: /usr/local/bin/dub: /usr/bin/python3: bad interpreter: No such file or directory
/etc/confluent/docker/configure: /usr/local/bin/cub: /usr/bin/python3: bad interpreter: No such file or directory
/etc/confluent/docker/configure: /usr/local/bin/dub: /usr/bin/python3: bad interpreter: No such file or directory
####################################
kafka.properties
log4j.properties
secrets
tools-log4j.properties
/etc/confluent/docker/configure: /usr/local/bin/dub: /usr/bin/python3: bad interpreter: No such file or directory
/etc/confluent/docker/configure: /usr/local/bin/dub: /usr/bin/python3: bad interpreter: No such file or directory
===> Running preflight checks ...
===> Check if /var/lib/kafka/data is writable ...
/etc/confluent/docker/ensure: line 17: /etc/confluent/docker/bash-config: No such file or directory
/etc/confluent/docker/ensure: /usr/local/bin/dub: /usr/bin/python3: bad interpreter: No such file or directory
===> Check if Zookeeper is healthy ...
/etc/confluent/docker/ensure: /usr/local/bin/cub: /usr/bin/python3: bad interpreter: No such file or directory
===> Launching ...
===> Launching kafka...
===> Kafka start...
06:57:09,122 INFO  kafka.utils.Log4jControllerRegistration$                      - Registered kafka:type=kafka.Log4jController MBean
06:57:09,128 INFO  org.apache.zookeeper.common.X509Util                          - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
06:57:09,132 INFO  ClusterBalanceManager                                         - Instantiating ClusterBalanceManager with an instance of io.confluent.databalancer.SbcDataBalanceManager
06:57:09,134 INFO  kafka.server.KafkaConfig                                      - KafkaConfig values:
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name =
	auto.create.topics.enable = false
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = SfquozklTIOMcOARB6rplQ
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name =
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 3
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic =
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name =
	confluent.cdc.lkc.metadata.topic =
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints =
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints =
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config =
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix =
	confluent.tier.backend =
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix =
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix =
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class =
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id =
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints =
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides =
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints =
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

06:57:09,135 INFO  kafka.server.KafkaConfig                                      - KafkaConfig values:
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name =
	auto.create.topics.enable = false
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = SfquozklTIOMcOARB6rplQ
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name =
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 3
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic =
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name =
	confluent.cdc.lkc.metadata.topic =
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints =
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints =
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config =
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix =
	confluent.tier.backend =
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix =
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix =
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class =
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id =
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints =
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides =
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints =
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

06:57:09,136 INFO  org.apache.kafka.common.utils.LoggingSignalHandler            - Registered signal handlers for TERM, INT, HUP
06:57:09,136 INFO  kafka.server.ControllerServer                                 - Starting controller
06:57:09,137 INFO  io.confluent.security.audit.AuditLogConfig                    - AuditLogConfig values:
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config =

06:57:09,137 INFO  io.confluent.security.audit.AuditLogConfig                    - AuditLogConfig values:
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config =

06:57:09,137 INFO  io.confluent.crn.CrnAuthorityConfig                           - CrnAuthorityConfig values:
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name =
	confluent.metadata.server.api.flavor = CP

06:57:09,137 INFO  io.confluent.crn.CrnAuthorityConfig                           - CrnAuthorityConfig values:
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name =
	confluent.metadata.server.api.flavor = CP

06:57:09,138 INFO  io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig  - MultiTenantAuditLogConfig values:
	confluent.security.event.logger.client.ip.enable = false
	confluent.security.event.logger.multitenant.enable = false

06:57:09,146 INFO  kafka.network.ConnectionQuotas                                - Updated connection-tokens max connection creation rate to 1.7976931348623157E308
06:57:09,146 INFO  kafka.network.DataPlaneAcceptor                               - Awaiting socket connections on localhost:29093.
06:57:09,147 INFO  org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config  - Config values:
	confluent.security.event.logger.detailed.audit.logs.disabled.apis =
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.metrics = false

06:57:09,148 INFO  org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config  - Config values:
	confluent.security.event.logger.detailed.audit.logs.disabled.apis =
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.metrics = false

06:57:09,148 INFO  org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config  - Config values:
	confluent.security.event.logger.detailed.audit.logs.disabled.apis =
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.metrics = false

06:57:09,149 INFO  kafka.network.SocketServer                                    - [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER)
06:57:09,149 INFO  kafka.server.SharedServer                                     - [SharedServer id=1] Starting SharedServer
06:57:09,152 INFO  kafka.log.MergedLog$                                          - [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2
06:57:09,152 INFO  kafka.log.MergedLog$                                          - [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0
06:57:09,152 INFO  kafka.log.MergedLog$                                          - [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0
06:57:09,152 INFO  kafka.raft.KafkaMetadataLog$                                  - Initialized snapshots with IDs SortedSet() from /var/lib/kafka/data/__cluster_metadata-0
06:57:09,153 INFO  kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper  - [raft-expiration-reaper]: Starting
06:57:09,155 INFO  org.apache.kafka.raft.QuorumState                             - [RaftManager nodeId=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1576)
06:57:09,155 INFO  org.apache.kafka.raft.QuorumState                             - [RaftManager nodeId=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, electionTimeoutMs=1125)
06:57:09,156 INFO  org.apache.kafka.raft.QuorumState                             - [RaftManager nodeId=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)})
06:57:09,159 INFO  kafka.raft.KafkaRaftManager$RaftIoThread                      - [kafka-raft-io-thread]: Starting
06:57:09,159 INFO  kafka.raft.RaftSendThread                                     - [kafka-raft-outbound-request-thread]: Starting
06:57:09,160 INFO  org.apache.kafka.image.loader.MetadataLoader                  - [MetadataLoader 1] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet.
06:57:09,160 INFO  org.apache.kafka.raft.LeaderState                             - [RaftManager nodeId=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)]
06:57:09,160 INFO  org.apache.kafka.controller.QuorumController                  - [Controller 1] Creating new QuorumController with clusterId MkU3OEVBNTcwNTJENDM2Qk, authorizer Optional.empty.
06:57:09,161 INFO  kafka.server.DiskUsageBasedThrottlingConfig$                  - Empty logDirs received! Disk based throttling won't be activated!
06:57:09,161 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Produce]: Starting
06:57:09,161 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Request]: Starting
06:57:09,161 INFO  kafka.server.DiskUsageBasedThrottlingConfig$                  - Empty logDirs received! Disk based throttling won't be activated!
06:57:09,161 INFO  kafka.server.DiskUsageBasedThrottlingConfig$                  - Empty logDirs received! Disk based throttling won't be activated!
06:57:09,161 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Fetch]: Starting
06:57:09,161 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-ControllerMutation]: Starting
06:57:09,162 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,162 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,162 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,162 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-AlterAcls]: Starting
06:57:09,162 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,162 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,163 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,163 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,163 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,163 INFO  kafka.network.SocketServer                                    - [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing.
06:57:09,164 INFO  kafka.server.BrokerServer                                     - [BrokerServer id=1] Transition from SHUTDOWN to STARTING
06:57:09,164 INFO  kafka.server.BrokerServer                                     - [BrokerServer id=1] Starting broker
06:57:09,166 INFO  org.apache.kafka.raft.KafkaRaftClient                         - [RaftManager nodeId=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1855970161
06:57:09,167 INFO  org.apache.kafka.raft.KafkaRaftClient                         - [RaftManager nodeId=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@1719170146
06:57:09,167 INFO  org.apache.kafka.controller.QuorumController                  - [Controller 1] Becoming the active controller at epoch 1, committed offset -1, committed epoch -1
06:57:09,167 INFO  org.apache.kafka.controller.QuorumController                  - [Controller 1] The metadata log appears to be empty. Appending 1 bootstrap record(s) at metadata.version 3.4-IV0 from the configured bootstrap with metadata.version 3.4-IV0.
06:57:09,167 INFO  org.apache.kafka.controller.FeatureControlManager             - [Controller 1] Setting confluent.metadata.version to 3.4-IV0
06:57:09,167 INFO  org.apache.kafka.image.loader.MetadataLoader                  - [MetadataLoader 1] handleCommit: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1
06:57:09,169 INFO  kafka.server.BrokerServer                                     - [BrokerServer id=1] FIPS mode enabled: false
06:57:09,170 INFO  kafka.server.DiskUsageBasedThrottlingConfig$                  - Empty logDirs received! Disk based throttling won't be activated!
06:57:09,170 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Produce]: Starting
06:57:09,185 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Request]: Starting
06:57:09,185 INFO  kafka.server.DiskUsageBasedThrottlingConfig$                  - Empty logDirs received! Disk based throttling won't be activated!
06:57:09,185 INFO  kafka.server.DiskUsageBasedThrottlingConfig$                  - Empty logDirs received! Disk based throttling won't be activated!
06:57:09,185 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Fetch]: Starting
06:57:09,186 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-ControllerMutation]: Starting
06:57:09,186 INFO  io.confluent.security.audit.AuditLogConfig                    - AuditLogConfig values:
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config =

06:57:09,186 INFO  io.confluent.security.audit.AuditLogConfig                    - AuditLogConfig values:
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config =

06:57:09,186 INFO  io.confluent.crn.CrnAuthorityConfig                           - CrnAuthorityConfig values:
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name =
	confluent.metadata.server.api.flavor = CP

06:57:09,186 INFO  io.confluent.crn.CrnAuthorityConfig                           - CrnAuthorityConfig values:
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name =
	confluent.metadata.server.api.flavor = CP

06:57:09,186 INFO  io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig  - MultiTenantAuditLogConfig values:
	confluent.security.event.logger.client.ip.enable = false
	confluent.security.event.logger.multitenant.enable = false

06:57:09,187 INFO  kafka.server.BrokerToControllerRequestThread                  - [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting
06:57:09,187 INFO  kafka.server.BrokerToControllerRequestThread                  - [BrokerToControllerChannelManager broker=1 name=forwarding]: Recorded new controller, from now on will use node localhost:29093 (id: 1 rack: null)
06:57:09,187 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-ClusterLink]: Starting
06:57:09,192 INFO  kafka.network.ConnectionQuotas                                - Updated connection-tokens max connection creation rate to 1.7976931348623157E308
06:57:09,192 INFO  kafka.network.DataPlaneAcceptor                               - Awaiting socket connections on localhost:29092.
06:57:09,192 INFO  org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config  - Config values:
	confluent.security.event.logger.detailed.audit.logs.disabled.apis =
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.metrics = false

06:57:09,192 INFO  org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config  - Config values:
	confluent.security.event.logger.detailed.audit.logs.disabled.apis =
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.metrics = false

06:57:09,193 INFO  org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config  - Config values:
	confluent.security.event.logger.detailed.audit.logs.disabled.apis =
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.metrics = false

06:57:09,193 INFO  kafka.network.SocketServer                                    - [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
06:57:09,193 INFO  org.apache.kafka.image.loader.MetadataLoader                  - [MetadataLoader 1] handleCommit: The loader finished catching up to the current high water mark of 2
06:57:09,193 INFO  kafka.network.ConnectionQuotas                                - Updated connection-tokens max connection creation rate to 1.7976931348623157E308
06:57:09,193 INFO  kafka.network.DataPlaneAcceptor                               - Awaiting socket connections on 0.0.0.0:9092.
06:57:09,194 INFO  org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config  - Config values:
	confluent.security.event.logger.detailed.audit.logs.disabled.apis =
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.metrics = false

06:57:09,195 INFO  org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config  - Config values:
	confluent.security.event.logger.detailed.audit.logs.disabled.apis =
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.metrics = false

06:57:09,196 INFO  org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config  - Config values:
	confluent.security.event.logger.detailed.audit.logs.disabled.apis =
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.metrics = false

06:57:09,196 INFO  kafka.network.SocketServer                                    - [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST)
06:57:09,197 INFO  kafka.server.BrokerToControllerRequestThread                  - [BrokerToControllerChannelManager broker=1 name=alterPartition]: Starting
06:57:09,197 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-Produce]: Starting
06:57:09,198 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-DeleteRecords]: Starting
06:57:09,197 INFO  kafka.server.BrokerToControllerRequestThread                  - [BrokerToControllerChannelManager broker=1 name=alterPartition]: Recorded new controller, from now on will use node localhost:29093 (id: 1 rack: null)
06:57:09,198 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-ElectLeader]: Starting
06:57:09,198 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-ListOffsets]: Starting
06:57:09,197 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-Fetch]: Starting
06:57:09,199 INFO  kafka.availability.BrokerHealthManager                        - [BrokerHealthManager]: Starting
06:57:09,199 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-Heartbeat]: Starting
06:57:09,199 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-Rebalance]: Starting
06:57:09,200 INFO  io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig  - EventEmitterConfig values:

06:57:09,201 INFO  io.confluent.telemetry.ConfluentTelemetryConfig               - Linux CPU collector enabled: true
06:57:09,201 INFO  io.confluent.telemetry.ConfluentTelemetryConfig               - Using cpu metric: io\.confluent\.kafka\.server/server/linux_system_cpu_utilization
06:57:09,201 INFO  io.confluent.telemetry.ConfluentTelemetryConfig               - Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed
06:57:09,201 WARN  io.confluent.telemetry.ConfluentTelemetryConfig               - no telemetry exporters are enabled
06:57:09,201 WARN  io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade  - Ignoring redefinition of existing telemetry label kafka.version
06:57:09,202 INFO  io.confluent.telemetry.ConfluentTelemetryConfig               - Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed
06:57:09,203 INFO  io.confluent.telemetry.ConfluentTelemetryConfig               - ConfluentTelemetryConfig values:
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|cluster_link/active_link_count|cluster_link/consumer_offset_committed_rate|cluster_link/consumer_offset_committed_total|cluster_link/fetch_throttle_time_avg|cluster_link/fetch_throttle_time_max|cluster_link/link_count|cluster_link/linked_leader_epoch_change_rate|cluster_link/linked_leader_epoch_change_total|cluster_link/linked_topic_partition_addition_rate|cluster_link/linked_topic_partition_addition_total|cluster_link/mirror_partition_count|cluster_link/mirror_topic_byte_total|cluster_link/mirror_topic_count|cluster_link/mirror_topic_lag|cluster_link/topic_config_update_rate|cluster_link/topic_config_update_total|cluster_link_fetcher/connection_count|cluster_link_fetcher/failed_reauthentication_rate|cluster_link_fetcher/failed_reauthentication_total|cluster_link_fetcher/incoming_byte_rate|cluster_link_fetcher/incoming_byte_total|cluster_link_fetcher/outgoing_byte_rate|cluster_link_fetcher/outgoing_byte_total|cluster_link_fetcher/reauthentication_latency_avg|cluster_link_fetcher_manager/max_lag|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/preferred_replica_imbalance_count|controller/tenant_partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|fetcher/bytes_per_sec|fetcher_lag/consumer_lag|group_coordinator/partition_load_time_max|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/blocked_on_mirror_source_partition_count|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_mirror_partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_mirror_partitions|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max|tenant/consumer_lag_offsets).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null

06:57:09,203 INFO  io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig  - VolumeMetricsCollectorConfig values:
	confluent.telemetry.metrics.collector.volume.update.ms = 15000

06:57:09,203 INFO  io.confluent.telemetry.exporter.http.HttpExporterConfig       - HttpExporterConfig values:
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.contentType = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

06:57:09,203 INFO  io.confluent.telemetry.exporter.kafka.KafkaExporterConfig     - KafkaExporterConfig values:
	enabled = true
	events.enabled = true
	metrics.enabled = true
	metrics.include = (io\.confluent\.kafka\.server/broker_load/broker_load_percent|io\.confluent\.kafka\.server/broker_topic/bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/messages_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_follower_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_produce_requests/rate/1_min|io\.confluent\.kafka\.server/log/size|io\.confluent\.kafka\.server/log_flush/log_flush_rate_and_time_ms|io\.confluent\.kafka\.server/log_flush/log_flush_rate_and_time_ms/rate/1_min|io\.confluent\.kafka\.server/request/local_time_ms|io\.confluent\.kafka\.server/request/request_queue_time_ms|io\.confluent\.kafka\.server/request/requests/rate/1_min|io\.confluent\.kafka\.server/request/total_time_ms|io\.confluent\.kafka\.server/request_channel/request_queue_size|io\.confluent\.kafka\.server/request_channel/response_queue_size|io\.confluent\.kafka\.server/request_handler_pool/request_handler_avg_idle_percent/rate/1_min|io\.confluent\.kafka\.server/server/linux_system_cpu_utilization|io\.confluent\.system/volume/disk_total_bytes)
	producer.bootstrap.servers = localhost:29092
	subset.partitioner.enabled = false
	topic.create = true
	topic.max.message.bytes = 10485760
	topic.name = _confluent-telemetry-metrics
	topic.partitions = 12
	topic.replicas = 3
	topic.retention.bytes = -1
	topic.retention.ms = 259200000
	topic.roll.ms = 14400000
	type = kafka

06:57:09,203 INFO  io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration  - RemoteConfigConfiguration values:
	enabled = true
	polling.interval.ms = 60000

06:57:09,203 INFO  io.confluent.telemetry.reporter.TelemetryReporter             - Initializing the event logger
06:57:09,203 INFO  io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig  - EventLoggerConfig values:
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter

06:57:09,203 INFO  io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig  - HttpExporterConfig values:
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

06:57:09,216 INFO  io.confluent.telemetry.reporter.TelemetryReporter             - Creating kafka exporter named '_local'
06:57:09,216 INFO  org.apache.kafka.clients.producer.ProducerConfig              - ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:29092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	compression.type = lz4
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.telemetry.serde.OpenTelemetryMetricsSerde

06:57:09,218 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 7.4.8-0-ce
06:57:09,218 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: 0ead35273d5fba75c45df3bc1ed2df05b2229f30
06:57:09,218 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1733727429218
06:57:09,219 INFO  io.confluent.telemetry.reporter.TelemetryReporter             - Starting Confluent telemetry reporter with an interval of 60000 ms)
06:57:09,237 INFO  org.apache.kafka.raft.KafkaRaftClient                         - [RaftManager nodeId=1] Registered the listener kafka.server.metadata.BrokerMetadataListener@1783097456
06:57:09,237 INFO  kafka.server.BrokerLifecycleManager                           - [BrokerLifecycleManager id=1] Incarnation C3a4WwSzRwePjHCYRyP9Zg of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qk is now STARTING.
06:57:09,237 INFO  kafka.server.BrokerToControllerRequestThread                  - [BrokerToControllerChannelManager broker=1 name=heartbeat]: Starting
06:57:09,237 INFO  kafka.server.BrokerToControllerRequestThread                  - [BrokerToControllerChannelManager broker=1 name=heartbeat]: Recorded new controller, from now on will use node localhost:29093 (id: 1 rack: null)
06:57:09,238 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,238 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-AlterAcls]: Starting
06:57:09,238 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,238 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,238 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,239 INFO  org.apache.kafka.controller.ClusterControlManager             - [Controller 1] Registered new broker: RegisterBrokerRecord(brokerId=1, isMigratingZkBroker=false, incarnationId=C3a4WwSzRwePjHCYRyP9Zg, brokerEpoch=2, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='localhost', port=29092, securityProtocol=0), BrokerEndpoint(name='PLAINTEXT_HOST', host='localhost', port=9092, securityProtocol=0)], features=[BrokerFeature(name='confluent.metadata.version', minSupportedVersion=1, maxSupportedVersion=108), BrokerFeature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=8)], rack=null, fenced=true, inControlledShutdown=false)
06:57:09,239 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,239 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,264 INFO  org.apache.kafka.image.loader.MetadataLoader                  - [MetadataLoader 1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 1
06:57:09,264 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,264 INFO  org.apache.kafka.common.requests.SamplingRequestLogFilter$Config  - Config values:
	confluent.request.log.api.samples.per.min =
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0

06:57:09,264 INFO  kafka.server.BrokerServer                                     - [BrokerServer id=1] Waiting for broker metadata to catch up.
06:57:09,265 INFO  kafka.server.BrokerLifecycleManager                           - [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 2
06:57:09,265 INFO  kafka.server.BrokerLifecycleManager                           - [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY.
06:57:09,265 INFO  kafka.server.metadata.BrokerMetadataListener                  - [BrokerMetadataListener id=1] Starting to publish metadata events at offset 2.
06:57:09,266 INFO  kafka.server.metadata.BrokerMetadataPublisher                 - [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=2, epoch=1) with metadata.version 3.4-IV0.
06:57:09,266 INFO  kafka.log.LogManager                                          - Loading logs from log dirs ArraySeq(/var/lib/kafka/data)
06:57:09,266 INFO  io.confluent.databalancer.event.SbcEvent                      - Handling event SbcLeaderUpdateEvent-1
06:57:09,266 INFO  io.confluent.databalancer.event.SbcEvent                      - This balancer node is now the metadata quorum leader. Activating kafkadatabalance manager without alive broker snapshot.
06:57:09,266 INFO  io.confluent.databalancer.event.SbcEvent                      - Handling event SbcKraftStartupEvent-2
06:57:09,266 INFO  io.confluent.databalancer.event.SbcEvent                      - Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed.
06:57:09,266 INFO  kafka.log.LogManager                                          - Attempting recovery for all logs in /var/lib/kafka/data since no clean shutdown file was found
06:57:09,266 INFO  kafka.log.LogManager                                          - Loaded 0 logs in 0ms.
06:57:09,266 INFO  kafka.log.LogManager                                          - Starting log cleanup with a period of 300000 ms.
06:57:09,266 INFO  kafka.log.LogManager                                          - Starting log flusher with a default period of 9223372036854775807 ms.
06:57:09,267 INFO  kafka.log.LogCleaner                                          - Starting the log cleaner
06:57:09,333 INFO  kafka.server.BrokerLifecycleManager                           - [BrokerLifecycleManager id=1] The broker is in RECOVERY.
06:57:09,333 INFO  kafka.log.LogCleaner                                          - [kafka-log-cleaner-thread-0]: Starting
06:57:09,333 INFO  kafka.server.ReplicaManager$LogDirFailureHandler              - [LogDirFailureHandler]: Starting
06:57:09,334 INFO  org.apache.kafka.clients.admin.AdminClientConfig              - AdminClientConfig values:
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = cluster-link--local-admin-1
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	confluent.use.controller.listener = false
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

06:57:09,335 WARN  org.apache.kafka.clients.admin.AdminClientConfig              - These configurations '[confluent.link.metadata.topic.replication.factor, confluent.command.topic.replication, confluent.balancer.topics.replication.factor]' were supplied but are not used yet.
06:57:09,335 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 7.4.8-0-ce
06:57:09,335 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: 0ead35273d5fba75c45df3bc1ed2df05b2229f30
06:57:09,335 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1733727429335
06:57:09,335 INFO  kafka.server.link.ClusterLinkManager                          - [ClusterLinkManager-broker-1] ClusterLinkManager has started up.
06:57:09,335 INFO  kafka.coordinator.group.GroupCoordinator                      - [GroupCoordinator 1]: Starting up.
06:57:09,335 INFO  kafka.coordinator.group.GroupCoordinator                      - [GroupCoordinator 1]: Startup complete.
06:57:09,335 INFO  kafka.coordinator.transaction.TransactionCoordinator          - [TransactionCoordinator id=1] Starting up.
06:57:09,335 INFO  kafka.coordinator.transaction.TransactionCoordinator          - [TransactionCoordinator id=1] Startup complete.
06:57:09,335 INFO  kafka.server.metadata.BrokerMetadataPublisher                 - [BrokerMetadataPublisher id=1] Updating metadata.version to 3.4-IV0 at offset OffsetAndEpoch(offset=2, epoch=1).
06:57:09,335 INFO  io.confluent.databalancer.event.SbcEvent                      - SBC Event SbcMetadataUpdateEvent-3 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-4]. Enqueuing...
06:57:09,335 INFO  io.confluent.databalancer.event.SbcEvent                      - Handling event SbcKraftStartupEvent-2
06:57:09,335 INFO  io.confluent.databalancer.event.SbcEvent                      - Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed.
06:57:09,335 INFO  io.confluent.databalancer.event.SbcEvent                      - Handling event SbcConfigUpdateEvent-4
06:57:09,335 INFO  io.confluent.databalancer.event.SbcEvent                      - Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed.
06:57:09,335 INFO  kafka.coordinator.transaction.TransactionMarkerChannelManager  - [Transaction Marker Channel Manager 1]: Starting
06:57:09,336 INFO  kafka.server.KafkaConfig                                      - KafkaConfig values:
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name =
	auto.create.topics.enable = false
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = SfquozklTIOMcOARB6rplQ
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name =
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 3
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic =
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name =
	confluent.cdc.lkc.metadata.topic =
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints =
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints =
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config =
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix =
	confluent.tier.backend =
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix =
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix =
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class =
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id =
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints =
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides =
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints =
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

06:57:09,336 INFO  kafka.network.SocketServer                                    - [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing.
06:57:09,337 INFO  org.apache.kafka.controller.BrokerHeartbeatManager            - [Controller 1] The request from broker 1 to unfence has been granted because it has caught up with the offset of it's register broker record 2.
06:57:09,365 INFO  io.confluent.databalancer.event.SbcEvent                      - SBC Event SbcMetadataUpdateEvent-5 generated 1 more events to enqueue in the following order - [SbcKraftBrokerAdditionEvent-6]. Enqueuing...
06:57:09,365 INFO  io.confluent.databalancer.event.SbcEvent                      - Handling event SbcKraftStartupEvent-2
06:57:09,365 INFO  io.confluent.databalancer.event.SbcEvent                      - Configs metadata not yet available, SBC startup delayed.
06:57:09,365 INFO  kafka.server.BrokerLifecycleManager                           - [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING.
06:57:09,365 INFO  io.confluent.databalancer.event.SbcEvent                      - Handling event SbcConfigUpdateEvent-4
06:57:09,365 INFO  io.confluent.databalancer.event.SbcEvent                      - Balancer notified of a config change: ConfigurationsDelta(changes={})
06:57:09,365 INFO  io.confluent.databalancer.event.SbcEvent                      - There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: []
06:57:09,365 INFO  io.confluent.databalancer.event.SbcEvent                      - Handling event SbcKraftBrokerAdditionEvent-6
06:57:09,365 INFO  io.confluent.databalancer.event.SbcKraftBrokerAdditionEvent   - Topics Image not present, pausing broker addition event of brokers (new brokers: [1]) until it is received.
06:57:09,365 INFO  io.confluent.http.server.KafkaHttpServerConfig                - KafkaHttpServerConfig values:
	access.control.allow.headers =
	access.control.allow.methods =
	access.control.allow.origin =
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm =
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	connector.connection.limit = 0
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.connection.per.sec = 25
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [http://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config =
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	server.connection.limit = 0
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm =
	ssl.keystore.location =
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = false
	ssl.keystore.type = JKS
	ssl.keystore.watch.location =
	ssl.protocol = TLS
	ssl.provider =
	ssl.trustmanager.algorithm =
	ssl.truststore.location =
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	suppress.stack.trace.response = true
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

06:57:09,366 INFO  io.confluent.http.server.KafkaHttpServerConfig                - KafkaHttpServerConfig values:
	access.control.allow.headers =
	access.control.allow.methods =
	access.control.allow.origin =
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm =
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	connector.connection.limit = 0
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.connection.per.sec = 25
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [http://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config =
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	server.connection.limit = 0
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm =
	ssl.keystore.location =
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = false
	ssl.keystore.type = JKS
	ssl.keystore.watch.location =
	ssl.protocol = TLS
	ssl.provider =
	ssl.trustmanager.algorithm =
	ssl.truststore.location =
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	suppress.stack.trace.response = true
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

06:57:09,366 INFO  io.confluent.http.server.KafkaHttpServerConfig                - KafkaHttpServerConfig values:
	access.control.allow.headers =
	access.control.allow.methods =
	access.control.allow.origin =
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm =
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	connector.connection.limit = 0
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.connection.per.sec = 25
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [http://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config =
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	server.connection.limit = 0
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm =
	ssl.keystore.location =
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = false
	ssl.keystore.type = JKS
	ssl.keystore.watch.location =
	ssl.protocol = TLS
	ssl.provider =
	ssl.trustmanager.algorithm =
	ssl.truststore.location =
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	suppress.stack.trace.response = true
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

06:57:09,366 INFO  kafka.server.KafkaConfig                                      - KafkaConfig values:
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name =
	auto.create.topics.enable = false
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = SfquozklTIOMcOARB6rplQ
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name =
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 3
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic =
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name =
	confluent.cdc.lkc.metadata.topic =
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints =
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints =
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config =
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix =
	confluent.tier.backend =
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix =
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix =
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class =
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id =
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints =
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides =
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints =
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

06:57:09,367 INFO  io.confluent.kafkarest.servlet.KafkaRestApplicationProvider   - Unexpected credentials store injected: null
06:57:09,367 INFO  io.confluent.kafkarest.KafkaRestConfig                        - KafkaRestConfig values:
	access.control.allow.headers =
	access.control.allow.methods =
	access.control.allow.origin =
	access.control.skip.options = true
	advertised.listeners = []
	api.endpoints.allowlist = []
	api.endpoints.blocklist = []
	api.v2.enable = false
	api.v3.enable = true
	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
	api.v3.produce.rate.limit.enabled = false
	api.v3.produce.rate.limit.max.bytes.global.per.sec = 10000000
	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
	api.v3.produce.rate.limit.max.requests.global.per.sec = 10000
	api.v3.produce.rate.limit.max.requests.per.sec = 10000
	api.v3.produce.response.thread.pool.size = 5
	authentication.method = NONE
	authentication.realm =
	authentication.roles = [*]
	authentication.skip.paths = []
	bootstrap.servers = localhost:29092
	client.init.timeout.ms = 60000
	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
	client.sasl.kerberos.min.time.before.relogin = 60000
	client.sasl.kerberos.service.name =
	client.sasl.kerberos.ticket.renew.jitter = 0.05
	client.sasl.kerberos.ticket.renew.window.factor = 0.8
	client.sasl.mechanism = GSSAPI
	client.security.protocol = PLAINTEXT
	client.ssl.cipher.suites =
	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
	client.ssl.endpoint.identification.algorithm =
	client.ssl.key.password = [hidden]
	client.ssl.keymanager.algorithm = SunX509
	client.ssl.keystore.location =
	client.ssl.keystore.password = [hidden]
	client.ssl.keystore.type = JKS
	client.ssl.protocol = TLS
	client.ssl.provider =
	client.ssl.trustmanager.algorithm = PKIX
	client.ssl.truststore.location =
	client.ssl.truststore.password = [hidden]
	client.ssl.truststore.type = JKS
	client.timeout.ms = 500
	client.zk.session.timeout.ms = 30000
	compression.enable = true
	confluent.resource.name.authority =
	connector.connection.limit = 0
	consumer.instance.timeout.ms = 300000
	consumer.iterator.backoff.ms = 50
	consumer.iterator.timeout.ms = 1
	consumer.request.max.bytes = 67108864
	consumer.request.timeout.ms = 1000
	consumer.threads = 50
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.connection.per.sec = 25
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	fetch.min.bytes = -1
	host.name =
	http2.enabled = true
	id =
	idle.timeout.ms = 30000
	kafka.rest.resource.extension.class = [io.confluent.kafkarest.KafkaRestResourceExtension]
	listener.protocol.map = []
	listeners = []
	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.jmx.prefix = kafka.rest
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8082
	producer.threads = 5
	proxy.protocol.enabled = false
	rate.limit.backend = guava
	rate.limit.costs =
	rate.limit.default.cost = 1
	rate.limit.enable = false
	rate.limit.per.cluster.cache.expiry.ms = 3600000
	rate.limit.per.cluster.permits.per.sec = 50
	rate.limit.permits.per.sec = 50
	rate.limit.timeout.ms = 0
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config =
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
	rest.servlet.initializor.classes = []
	schema.registry.url = http://localhost:8081
	server.connection.limit = 0
	shutdown.graceful.ms = 1000
	simpleconsumer.pool.size.max = 25
	simpleconsumer.pool.timeout.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm =
	ssl.keystore.location =
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = false
	ssl.keystore.type = JKS
	ssl.keystore.watch.location =
	ssl.protocol = TLS
	ssl.provider =
	ssl.trustmanager.algorithm =
	ssl.truststore.location =
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	streaming.connection.max.duration.grace.period.ms = 500
	streaming.connection.max.duration.ms = 86400000
	suppress.stack.trace.response = true
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []
	zookeeper.connect =

06:57:09,367 INFO  io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig  - EventEmitterConfig values:

06:57:09,367 INFO  io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig  - EventEmitterConfig values:

06:57:09,367 INFO  io.confluent.telemetry.ConfluentTelemetryConfig               - Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed
06:57:09,367 INFO  io.confluent.telemetry.ConfluentTelemetryConfig               - ConfluentTelemetryConfig values:
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null

06:57:09,367 INFO  io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig  - VolumeMetricsCollectorConfig values:
	confluent.telemetry.metrics.collector.volume.update.ms = 15000

06:57:09,367 INFO  io.confluent.telemetry.exporter.http.HttpExporterConfig       - HttpExporterConfig values:
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.contentType = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

06:57:09,367 WARN  io.confluent.telemetry.ConfluentTelemetryConfig               - no telemetry exporters are enabled
06:57:09,367 INFO  io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration  - RemoteConfigConfiguration values:
	enabled = true
	polling.interval.ms = 60000

06:57:09,367 WARN  io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade  - Ignoring redefinition of existing telemetry label kafka_rest.version
06:57:09,368 INFO  io.confluent.telemetry.ConfluentTelemetryConfig               - Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed
06:57:09,368 INFO  io.confluent.telemetry.ConfluentTelemetryConfig               - ConfluentTelemetryConfig values:
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null

06:57:09,368 INFO  io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig  - VolumeMetricsCollectorConfig values:
	confluent.telemetry.metrics.collector.volume.update.ms = 15000

06:57:09,368 INFO  io.confluent.telemetry.exporter.http.HttpExporterConfig       - HttpExporterConfig values:
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.contentType = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

06:57:09,368 WARN  io.confluent.telemetry.ConfluentTelemetryConfig               - no telemetry exporters are enabled
06:57:09,368 INFO  io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration  - RemoteConfigConfiguration values:
	enabled = true
	polling.interval.ms = 60000

06:57:09,368 INFO  io.confluent.telemetry.reporter.TelemetryReporter             - Initializing the event logger
06:57:09,368 INFO  io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig  - EventLoggerConfig values:
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter

06:57:09,368 INFO  io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig  - HttpExporterConfig values:
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

06:57:09,369 INFO  io.confluent.telemetry.reporter.TelemetryReporter             - Starting Confluent telemetry reporter with an interval of 60000 ms)
06:57:09,369 INFO  org.eclipse.jetty.util.log                                    - Logging initialized @252ms to org.eclipse.jetty.util.log.Slf4jLog
06:57:09,370 INFO  io.confluent.http.server.KafkaHttpApplicationLoader           - Application provider 'KafkaRestApplicationProvider' provided 1 instance(s).
06:57:09,370 INFO  io.confluent.rest.RestConfig                                  - RestConfig values:
	access.control.allow.headers =
	access.control.allow.methods =
	access.control.allow.origin =
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm =
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	connector.connection.limit = 0
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.connection.per.sec = 25
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = []
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8080
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config =
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	server.connection.limit = 0
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm =
	ssl.keystore.location =
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = false
	ssl.keystore.type = JKS
	ssl.keystore.watch.location =
	ssl.protocol = TLS
	ssl.provider =
	ssl.trustmanager.algorithm =
	ssl.truststore.location =
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	suppress.stack.trace.response = true
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

06:57:09,370 INFO  io.confluent.http.server.KafkaHttpApplicationLoader           - Application provider 'MetadataApiApplicationProvider' provided 1 instance(s).
06:57:09,370 INFO  org.apache.kafka.server.http.MetadataServerConfig             - MetadataServerConfig values:
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = null
	confluent.metadata.server.enable = false
	confluent.metadata.server.kraft.controller.enabled = false
	confluent.metadata.server.listeners = null

06:57:09,370 INFO  io.confluent.http.server.KafkaHttpApplicationLoader           - Application provider 'RbacApplicationProvider' did not provide any instances.
06:57:09,370 INFO  io.confluent.rest.ApplicationServer                           - Initial capacity 128, increased by 64, maximum capacity 2147483647.
06:57:09,372 INFO  io.confluent.rest.ApplicationServer                           - Adding listener with HTTP/2: NamedURI{uri=http://0.0.0.0:8090, name='null'}
06:57:09,372 INFO  io.confluent.kafka.http.server.KafkaHttpServerLoader          - Loaded KafkaHttpServer implementation class io.confluent.http.server.KafkaHttpServerImpl
06:57:09,373 INFO  io.confluent.http.server.KafkaHttpServerImpl                  - KafkaHttpServer transitioned from NEW to STARTING..
06:57:09,374 WARN  io.confluent.kafkarest.KafkaRestConfig                        - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled.
06:57:09,374 INFO  io.confluent.kafkarest.config.SchemaRegistryConfig            - SchemaRegistryConfig values:
	auto.register.schemas = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host =
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

06:57:09,375 INFO  io.confluent.rest.Application                                 - Binding EmbeddedKafkaRestApplication to all listeners.
06:57:09,376 INFO  io.confluent.rest.Application                                 - Binding MetadataApiApplication to all listeners.
06:57:09,379 INFO  org.eclipse.jetty.server.Server                               - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 21.0.2+13-jvmci-23.1-b30
06:57:09,383 INFO  org.eclipse.jetty.server.session                              - DefaultSessionIdManager workerName=node0
06:57:09,383 INFO  org.eclipse.jetty.server.session                              - No SessionScavenger set, using defaults
06:57:09,383 INFO  org.eclipse.jetty.server.session                              - node0 Scavenging every 600000ms
06:57:09,388 WARN  io.confluent.kafkarest.KafkaRestConfig                        - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled.
06:57:09,389 WARN  io.confluent.kafkarest.KafkaRestConfig                        - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled.
06:57:09,389 WARN  io.confluent.kafkarest.KafkaRestConfig                        - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled.
06:57:09,389 WARN  io.confluent.kafkarest.KafkaRestConfig                        - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled.
06:57:09,433 INFO  org.hibernate.validator.internal.util.Version                 - HV000001: Hibernate Validator 6.1.7.Final
06:57:09,437 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:09,438 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":4,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":29999,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":0.445,"requestQueueTimeMs":0.036,"localTimeMs":0.045,"remoteTimeMs":0.271,"throttleTimeMs":0,"responseQueueTimeMs":0.015,"sendTimeMs":0.076,"sendIoTimeMs":0.069,"responseSize":216,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372742943700101}
06:57:09,438 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":4,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":29999,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47342-0","clientAddress":"127.0.0.1","totalTimeMs":1.4,"requestQueueTimeMs":0.039,"localTimeMs":0.058,"remoteTimeMs":1.229,"throttleTimeMs":0,"responseQueueTimeMs":0.035,"sendTimeMs":0.036,"sendIoTimeMs":0.022,"responseSize":206,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372742943600102}
06:57:09,438 ERROR kafka.server.link.ClusterLinkMetadataManagerWithKRaftSupport  - [ClusterLinkMetadataManager-broker-1] Cluster link metadata topic creation failed: org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
06:57:09,528 INFO  org.eclipse.jetty.server.handler.ContextHandler               - Started o.e.j.s.ServletContextHandler@66ac1a2e{/kafka,null,AVAILABLE}
Dec 09, 2024 6:57:09 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider io.confluent.metadataapi.resources.MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.metadataapi.resources.MetadataResource will be ignored.
06:57:09,534 INFO  org.eclipse.jetty.server.handler.ContextHandler               - Started o.e.j.s.ServletContextHandler@4b62b716{/v1/metadata,null,AVAILABLE}
06:57:09,535 INFO  org.eclipse.jetty.server.handler.ContextHandler               - Started o.e.j.s.ServletContextHandler@76c0f9b0{/ws,null,AVAILABLE}
06:57:09,535 INFO  org.eclipse.jetty.server.handler.ContextHandler               - Started o.e.j.s.ServletContextHandler@15dc3961{/ws,null,AVAILABLE}
06:57:09,536 INFO  org.eclipse.jetty.server.AbstractConnector                    - Started NetworkTrafficServerConnector@19643a96{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8090}
06:57:09,536 INFO  org.eclipse.jetty.server.Server                               - Started @419ms
06:57:09,536 INFO  io.confluent.http.server.KafkaHttpServerImpl                  - KafkaHttpServer transitioned from STARTING to RUNNING..
06:57:09,536 INFO  kafka.server.BrokerServer                                     - [BrokerServer id=1] Skipping durability audit instantiation
06:57:09,536 INFO  io.confluent.license.validator.LicenseConfig                  - LicenseConfig values:
	confluent.license = [hidden]
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 3

06:57:09,536 INFO  io.confluent.license.validator.LicenseConfig                  - LicenseConfig values:
	confluent.license = [hidden]
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 3

06:57:09,536 INFO  org.apache.kafka.clients.admin.AdminClientConfig              - AdminClientConfig values:
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-1
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	confluent.use.controller.listener = false
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

06:57:09,537 WARN  org.apache.kafka.clients.admin.AdminClientConfig              - These configurations '[replication.factor, confluent.link.metadata.topic.replication.factor, confluent.command.topic.replication, confluent.balancer.topics.replication.factor, min.insync.replicas]' were supplied but are not used yet.
06:57:09,537 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 7.4.8-0-ce
06:57:09,537 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: 0ead35273d5fba75c45df3bc1ed2df05b2229f30
06:57:09,537 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1733727429537
06:57:09,539 INFO  org.apache.kafka.common.utils.AppInfoParser                   - App info kafka.admin.client for _confluent-license-admin-1 unregistered
06:57:09,539 INFO  org.apache.kafka.common.metrics.Metrics                       - Metrics scheduler closed
06:57:09,539 INFO  org.apache.kafka.common.metrics.Metrics                       - Closing reporter org.apache.kafka.common.metrics.JmxReporter
06:57:09,539 INFO  org.apache.kafka.common.metrics.Metrics                       - Metrics reporters closed
06:57:09,539 INFO  io.confluent.license.LicenseStore                             - Starting License Store
06:57:09,539 INFO  org.apache.kafka.connect.util.KafkaBasedLog                   - Starting KafkaBasedLog with topic _confluent-command
06:57:09,539 INFO  org.apache.kafka.clients.admin.AdminClientConfig              - AdminClientConfig values:
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-1
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	confluent.use.controller.listener = false
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

06:57:09,540 WARN  org.apache.kafka.clients.admin.AdminClientConfig              - These configurations '[replication.factor, confluent.link.metadata.topic.replication.factor, confluent.command.topic.replication, confluent.balancer.topics.replication.factor, min.insync.replicas]' were supplied but are not used yet.
06:57:09,540 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka version: 7.4.8-0-ce
06:57:09,540 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka commitId: 0ead35273d5fba75c45df3bc1ed2df05b2229f30
06:57:09,540 INFO  org.apache.kafka.common.utils.AppInfoParser                   - Kafka startTimeMs: 1733727429540
06:57:09,542 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:09,542 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":4,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-command","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":0.294,"requestQueueTimeMs":0.049,"localTimeMs":0.035,"remoteTimeMs":0.152,"throttleTimeMs":0,"responseQueueTimeMs":0.023,"sendTimeMs":0.032,"sendIoTimeMs":0.023,"responseSize":210,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372742954100001}
06:57:09,542 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":4,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-command","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47366-2","clientAddress":"127.0.0.1","totalTimeMs":0.706,"requestQueueTimeMs":0.03,"localTimeMs":0.053,"remoteTimeMs":0.526,"throttleTimeMs":0,"responseQueueTimeMs":0.049,"sendTimeMs":0.046,"sendIoTimeMs":0.034,"responseSize":200,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372742954100200}
06:57:09,542 INFO  io.confluent.license.LicenseStore                             - Creating topic _confluent-command with replication factor 3. At least 3 brokers must be started concurrently to complete license registration.
06:57:09,842 INFO  org.apache.kafka.clients.Metadata                             - [Producer clientId=confluent-telemetry-reporter-local-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk
06:57:10,447 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:10,448 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":2.813,"requestQueueTimeMs":0.198,"localTimeMs":0.9,"remoteTimeMs":1.315,"throttleTimeMs":0,"responseQueueTimeMs":0.129,"sendTimeMs":0.269,"sendIoTimeMs":0.167,"responseSize":216,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743044500001}
06:57:10,449 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47342-0","clientAddress":"127.0.0.1","totalTimeMs":7.392,"requestQueueTimeMs":0.3,"localTimeMs":0.309,"remoteTimeMs":6.32,"throttleTimeMs":0,"responseQueueTimeMs":0.317,"sendTimeMs":0.143,"sendIoTimeMs":0.103,"responseSize":206,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743044200002}
06:57:10,452 ERROR kafka.server.link.ClusterLinkMetadataManagerWithKRaftSupport  - [ClusterLinkMetadataManager-broker-1] Cluster link metadata topic creation failed: org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
06:57:10,550 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:10,552 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-command","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":2.519,"requestQueueTimeMs":0.11,"localTimeMs":0.393,"remoteTimeMs":1.39,"throttleTimeMs":0,"responseQueueTimeMs":0.433,"sendTimeMs":0.191,"sendIoTimeMs":0.131,"responseSize":210,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743054900001}
06:57:10,552 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-command","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47366-2","clientAddress":"127.0.0.1","totalTimeMs":4.671,"requestQueueTimeMs":0.434,"localTimeMs":0.282,"remoteTimeMs":3.747,"throttleTimeMs":0,"responseQueueTimeMs":0.093,"sendTimeMs":0.113,"sendIoTimeMs":0.08,"responseSize":200,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743054700000}
06:57:11,457 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:11,458 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":6,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":2.869,"requestQueueTimeMs":0.773,"localTimeMs":0.192,"remoteTimeMs":1.43,"throttleTimeMs":0,"responseQueueTimeMs":0.197,"sendTimeMs":0.275,"sendIoTimeMs":0.195,"responseSize":216,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743145500001}
06:57:11,460 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":6,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47342-0","clientAddress":"127.0.0.1","totalTimeMs":6.166,"requestQueueTimeMs":0.223,"localTimeMs":0.282,"remoteTimeMs":5.141,"throttleTimeMs":0,"responseQueueTimeMs":0.383,"sendTimeMs":0.135,"sendIoTimeMs":0.099,"responseSize":206,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743145400002}
06:57:11,461 ERROR kafka.server.link.ClusterLinkMetadataManagerWithKRaftSupport  - [ClusterLinkMetadataManager-broker-1] Cluster link metadata topic creation failed: org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
06:57:12,468 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:12,470 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":7,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":2.596,"requestQueueTimeMs":0.121,"localTimeMs":0.391,"remoteTimeMs":1.459,"throttleTimeMs":0,"responseQueueTimeMs":0.345,"sendTimeMs":0.278,"sendIoTimeMs":0.18,"responseSize":216,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743246700001}
06:57:12,471 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":7,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47342-0","clientAddress":"127.0.0.1","totalTimeMs":5.33,"requestQueueTimeMs":0.318,"localTimeMs":0.278,"remoteTimeMs":4.425,"throttleTimeMs":0,"responseQueueTimeMs":0.157,"sendTimeMs":0.15,"sendIoTimeMs":0.108,"responseSize":206,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743246500002}
06:57:12,475 ERROR kafka.server.link.ClusterLinkMetadataManagerWithKRaftSupport  - [ClusterLinkMetadataManager-broker-1] Cluster link metadata topic creation failed: org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
06:57:12,563 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:12,564 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":6,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-command","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":1.747,"requestQueueTimeMs":0.067,"localTimeMs":0.247,"remoteTimeMs":1.123,"throttleTimeMs":0,"responseQueueTimeMs":0.119,"sendTimeMs":0.189,"sendIoTimeMs":0.108,"responseSize":210,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743256200001}
06:57:12,565 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":6,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-command","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47366-2","clientAddress":"127.0.0.1","totalTimeMs":4.538,"requestQueueTimeMs":0.876,"localTimeMs":0.317,"remoteTimeMs":3.06,"throttleTimeMs":0,"responseQueueTimeMs":0.165,"sendTimeMs":0.119,"sendIoTimeMs":0.092,"responseSize":200,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743256000000}
06:57:13,481 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:13,482 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":8,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":2.568,"requestQueueTimeMs":0.145,"localTimeMs":0.382,"remoteTimeMs":1.384,"throttleTimeMs":0,"responseQueueTimeMs":0.154,"sendTimeMs":0.502,"sendIoTimeMs":0.194,"responseSize":216,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743347900001}
06:57:13,484 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":8,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47342-0","clientAddress":"127.0.0.1","totalTimeMs":5.904,"requestQueueTimeMs":0.286,"localTimeMs":0.289,"remoteTimeMs":4.127,"throttleTimeMs":0,"responseQueueTimeMs":0.743,"sendTimeMs":0.456,"sendIoTimeMs":0.317,"responseSize":206,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743347800002}
06:57:13,485 ERROR kafka.server.link.ClusterLinkMetadataManagerWithKRaftSupport  - [ClusterLinkMetadataManager-broker-1] Cluster link metadata topic creation failed: org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
06:57:14,491 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:14,492 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":9,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":2.193,"requestQueueTimeMs":0.136,"localTimeMs":0.184,"remoteTimeMs":1.382,"throttleTimeMs":0,"responseQueueTimeMs":0.264,"sendTimeMs":0.226,"sendIoTimeMs":0.157,"responseSize":216,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743449000001}
06:57:14,493 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":9,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47342-0","clientAddress":"127.0.0.1","totalTimeMs":5.335,"requestQueueTimeMs":0.244,"localTimeMs":0.281,"remoteTimeMs":4.538,"throttleTimeMs":0,"responseQueueTimeMs":0.11,"sendTimeMs":0.16,"sendIoTimeMs":0.116,"responseSize":206,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743448800002}
06:57:14,494 ERROR kafka.server.link.ClusterLinkMetadataManagerWithKRaftSupport  - [ClusterLinkMetadataManager-broker-1] Cluster link metadata topic creation failed: org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
06:57:15,503 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:15,505 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":10,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":2.349,"requestQueueTimeMs":0.156,"localTimeMs":0.189,"remoteTimeMs":1.447,"throttleTimeMs":0,"responseQueueTimeMs":0.342,"sendTimeMs":0.213,"sendIoTimeMs":0.14,"responseSize":216,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743550200001}
06:57:15,507 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":10,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47342-0","clientAddress":"127.0.0.1","totalTimeMs":5.906,"requestQueueTimeMs":0.213,"localTimeMs":0.294,"remoteTimeMs":4.583,"throttleTimeMs":0,"responseQueueTimeMs":0.595,"sendTimeMs":0.219,"sendIoTimeMs":0.137,"responseSize":206,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743550100002}
06:57:15,509 ERROR kafka.server.link.ClusterLinkMetadataManagerWithKRaftSupport  - [ClusterLinkMetadataManager-broker-1] Cluster link metadata topic creation failed: org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
06:57:16,515 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:16,516 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":11,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":2.558,"requestQueueTimeMs":0.185,"localTimeMs":0.253,"remoteTimeMs":1.252,"throttleTimeMs":0,"responseQueueTimeMs":0.66,"sendTimeMs":0.206,"sendIoTimeMs":0.147,"responseSize":216,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743651400001}
06:57:16,518 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":11,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47342-0","clientAddress":"127.0.0.1","totalTimeMs":5.646,"requestQueueTimeMs":0.246,"localTimeMs":0.258,"remoteTimeMs":4.592,"throttleTimeMs":0,"responseQueueTimeMs":0.41,"sendTimeMs":0.138,"sendIoTimeMs":0.097,"responseSize":206,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743651200002}
06:57:16,519 ERROR kafka.server.link.ClusterLinkMetadataManagerWithKRaftSupport  - [ClusterLinkMetadataManager-broker-1] Cluster link metadata topic creation failed: org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
06:57:16,576 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:16,577 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":7,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-command","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":2.238,"requestQueueTimeMs":0.071,"localTimeMs":0.131,"remoteTimeMs":1.24,"throttleTimeMs":0,"responseQueueTimeMs":0.59,"sendTimeMs":0.204,"sendIoTimeMs":0.142,"responseSize":210,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743657500001}
06:57:16,580 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":7,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-command","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47366-2","clientAddress":"127.0.0.1","totalTimeMs":6.977,"requestQueueTimeMs":0.854,"localTimeMs":0.224,"remoteTimeMs":4.348,"throttleTimeMs":0,"responseQueueTimeMs":1.385,"sendTimeMs":0.163,"sendIoTimeMs":0.102,"responseSize":200,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743657300000}
06:57:17,524 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:17,536 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":12,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":11.038,"requestQueueTimeMs":0.096,"localTimeMs":0.068,"remoteTimeMs":5.402,"throttleTimeMs":0,"responseQueueTimeMs":5.185,"sendTimeMs":0.286,"sendIoTimeMs":0.168,"responseSize":216,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743752400001}
06:57:17,554 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":12,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47342-0","clientAddress":"127.0.0.1","totalTimeMs":26.751,"requestQueueTimeMs":0.1,"localTimeMs":0.114,"remoteTimeMs":26.041,"throttleTimeMs":0,"responseQueueTimeMs":0.196,"sendTimeMs":0.299,"sendIoTimeMs":0.231,"responseSize":206,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743752300002}
06:57:17,554 ERROR kafka.server.link.ClusterLinkMetadataManagerWithKRaftSupport  - [ClusterLinkMetadataManager-broker-1] Cluster link metadata topic creation failed: org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
06:57:18,570 INFO  org.apache.kafka.controller.ReplicationControlManager         - [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.)
06:57:18,574 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":13,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29093-127.0.0.1:39992-0","clientAddress":"127.0.0.1","totalTimeMs":2.503,"requestQueueTimeMs":0.135,"localTimeMs":0.314,"remoteTimeMs":1.39,"throttleTimeMs":0,"responseQueueTimeMs":0.247,"sendTimeMs":0.415,"sendIoTimeMs":0.336,"responseSize":216,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":173372743856900001}
06:57:18,574 INFO  kafka.request.logger                                          - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":13,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":3,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"AAAAAAAAAAAAAAAAAAAAAA","errorCode":38,"errorMessage":"Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.","numPartitions":-1,"replicationFactor":-1,"configs":[]}]},"connection":"127.0.0.1:29092-127.0.0.1:47342-0","clientAddress":"127.0.0.1","totalTimeMs":7.137,"requestQueueTimeMs":0.313,"localTimeMs":0.53,"remoteTimeMs":5.202,"throttleTimeMs":0,"responseQueueTimeMs":0.911,"sendTimeMs":0.179,"sendIoTimeMs":0.132,"responseSize":206,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.4.8-0-ce"},"isDisconnectedClient":false,"requestId":173372743856700002}
06:57:18,577 ERROR kafka.server.link.ClusterLinkMetadataManagerWithKRaftSupport  - [ClusterLinkMetadataManager-broker-1] Cluster link metadata topic creation failed: org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
^C06:57:19,075 INFO  org.apache.kafka.common.utils.LoggingSignalHandler            - Terminating process due to signal SIGINT
06:57:19,076 INFO  kafka.server.ControllerServer                                 - [ControllerServer id=1] shutting down
06:57:19,076 INFO  kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper  - [raft-expiration-reaper]: Shutting down
06:57:19,080 INFO  org.eclipse.jetty.server.AbstractConnector                    - Stopped NetworkTrafficServerConnector@19643a96{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8090}
06:57:19,080 INFO  org.eclipse.jetty.server.session                              - node0 Stopped scavenging
06:57:19,080 INFO  org.eclipse.jetty.server.handler.ContextHandler               - Stopped o.e.j.s.ServletContextHandler@15dc3961{/ws,null,STOPPED}
06:57:19,080 INFO  org.eclipse.jetty.server.handler.ContextHandler               - Stopped o.e.j.s.ServletContextHandler@76c0f9b0{/ws,null,STOPPED}
06:57:19,082 INFO  org.eclipse.jetty.server.handler.ContextHandler               - Stopped o.e.j.s.ServletContextHandler@4b62b716{/v1/metadata,null,STOPPED}
06:57:19,083 INFO  org.eclipse.jetty.server.handler.ContextHandler               - Stopped o.e.j.s.ServletContextHandler@66ac1a2e{/kafka,null,STOPPED}
06:57:19,083 INFO  org.apache.kafka.common.metrics.Metrics                       - Metrics scheduler closed
06:57:19,083 INFO  org.apache.kafka.common.metrics.Metrics                       - Closing reporter io.confluent.telemetry.reporter.TelemetryReporter
06:57:19,083 INFO  io.confluent.telemetry.reporter.TelemetryReporter             - Stopping TelemetryReporter collectorTask
06:57:19,083 INFO  io.confluent.telemetry.reporter.TelemetryReporter             - Closing the event logger
06:57:19,084 INFO  io.confluent.telemetry.reporter.TelemetryReporter             - Stopping TelemetryReporter remoteConfigTask
06:57:19,086 INFO  org.apache.kafka.common.metrics.Metrics                       - Closing reporter org.apache.kafka.common.metrics.JmxReporter
06:57:19,086 INFO  org.apache.kafka.common.metrics.Metrics                       - Metrics reporters closed
06:57:19,086 INFO  org.apache.kafka.common.metrics.Metrics                       - Metrics scheduler closed
06:57:19,086 INFO  org.apache.kafka.common.metrics.Metrics                       - Closing reporter org.apache.kafka.common.metrics.JmxReporter
06:57:19,086 INFO  org.apache.kafka.common.metrics.Metrics                       - Metrics reporters closed
06:57:19,093 INFO  org.apache.kafka.common.utils.LoggingSignalHandler            - Terminating process due to signal SIGINT
06:57:19,269 INFO  kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper  - [raft-expiration-reaper]: Stopped
06:57:19,269 INFO  kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper  - [raft-expiration-reaper]: Shutdown completed
06:57:19,270 INFO  kafka.raft.KafkaRaftManager$RaftIoThread                      - [kafka-raft-io-thread]: Shutting down
06:57:19,270 INFO  org.apache.kafka.raft.KafkaRaftClient                         - [RaftManager nodeId=1] Beginning graceful shutdown
06:57:19,270 INFO  org.apache.kafka.raft.KafkaRaftClient                         - [RaftManager nodeId=1] Graceful shutdown completed
06:57:19,270 INFO  kafka.raft.KafkaRaftManager$RaftIoThread                      - [kafka-raft-io-thread]: Completed graceful shutdown of RaftClient
06:57:19,270 INFO  kafka.raft.KafkaRaftManager$RaftIoThread                      - [kafka-raft-io-thread]: Stopped
06:57:19,271 INFO  kafka.raft.KafkaRaftManager$RaftIoThread                      - [kafka-raft-io-thread]: Shutdown completed
06:57:19,277 INFO  kafka.raft.RaftSendThread                                     - [kafka-raft-outbound-request-thread]: Shutting down
06:57:19,278 INFO  kafka.raft.RaftSendThread                                     - [kafka-raft-outbound-request-thread]: Stopped
06:57:19,278 INFO  kafka.raft.RaftSendThread                                     - [kafka-raft-outbound-request-thread]: Shutdown completed
06:57:19,281 INFO  kafka.tier.state.FileTierPartitionState                       - Tier partition state for __cluster_metadata-0 closed.
06:57:19,282 INFO  kafka.log.ProducerStateManager                                - [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 24 with 0 producer ids in 0 ms.
06:57:19,289 INFO  kafka.network.SocketServer                                    - [SocketServer listenerType=CONTROLLER, nodeId=1] Stopping socket server request processors
06:57:19,290 INFO  org.apache.kafka.clients.NetworkClient                        - [BrokerToControllerChannelManager broker=1 name=forwarding] Node 1 disconnected.
06:57:19,290 INFO  org.apache.kafka.clients.NetworkClient                        - [BrokerToControllerChannelManager broker=1 name=heartbeat] Node 1 disconnected.
06:57:19,299 INFO  kafka.network.SocketServer                                    - [SocketServer listenerType=CONTROLLER, nodeId=1] Stopped socket server request processors
06:57:19,299 INFO  org.apache.kafka.queue.KafkaEventQueue                        - [Controller 1] QuorumController#beginShutdown: shutting down event queue.
06:57:19,299 INFO  kafka.network.SocketServer                                    - [SocketServer listenerType=CONTROLLER, nodeId=1] Shutting down socket server
06:57:19,299 ERROR org.apache.kafka.controller.QuorumController                  - [Controller 1] writeNoOpRecord: unable to start processing because of RejectedExecutionException. Reason: null
06:57:19,299 ERROR org.apache.kafka.controller.QuorumController                  - [Controller 1] maybeFenceReplicas: unable to start processing because of RejectedExecutionException. Reason: null
06:57:19,299 ERROR org.apache.kafka.controller.QuorumController                  - [Controller 1] Cancelling deferred write event maybeFenceReplicas because the event queue is now closed.
06:57:19,306 INFO  kafka.network.SocketServer                                    - [SocketServer listenerType=CONTROLLER, nodeId=1] Shutdown completed
06:57:19,306 INFO  kafka.server.KafkaRequestHandlerPool                          - [data-plane Kafka Request Handler on Broker 1], shutting down
06:57:19,307 INFO  kafka.server.KafkaRequestHandlerPool                          - [data-plane Kafka Request Handler on Broker 1], shut down completely
06:57:19,307 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-AlterAcls]: Shutting down
06:57:19,308 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-AlterAcls]: Stopped
06:57:19,308 INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper  - [ExpirationReaper-1-AlterAcls]: Shutdown completed
06:57:19,308 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Fetch]: Shutting down
06:57:19,308 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Fetch]: Stopped
06:57:19,308 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Fetch]: Shutdown completed
06:57:19,308 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Produce]: Shutting down
06:57:19,308 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Produce]: Stopped
06:57:19,308 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Produce]: Shutdown completed
06:57:19,308 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Request]: Shutting down
06:57:19,308 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Request]: Stopped
06:57:19,308 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-Request]: Shutdown completed
06:57:19,309 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-ControllerMutation]: Shutting down
06:57:19,309 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-ControllerMutation]: Stopped
06:57:19,309 INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper        - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
06:57:19,309 INFO  org.apache.kafka.queue.KafkaEventQueue                        - [Controller 1] closed event queue.
06:57:19,309 INFO  org.apache.kafka.common.utils.AppInfoParser                   - App info kafka.server for 1 unregistered
➜  server-native git:(7.4.x) ✗