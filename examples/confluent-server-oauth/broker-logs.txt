Generate keys and certificates used for MDS
writing RSA key
===> User
/etc/confluent/docker/run: line 17: /etc/confluent/docker/bash-config: No such file or directory
uid=0(root) gid=0(root) groups=0(root)
===> Configuring ...
/etc/confluent/docker/configure: line 17: /etc/confluent/docker/bash-config: No such file or directory
Running in KRaft mode...
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
####################################
kafka.properties
log4j2.yaml
secrets
server.properties
tools-log4j2.yaml
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
===> Running preflight checks ... 
/etc/confluent/docker/ensure: line 17: /etc/confluent/docker/bash-config: No such file or directory
===> Check if /var/lib/kafka/data is writable ...
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
===> Running in KRaft mode, skipping Zookeeper health check...
===> Using provided cluster id vHCgQyIrRHG8Jv27qI2h3Q ...
grep: error while loading shared libraries: libpcre.so.1: cannot open shared object file: No such file or directory
/usr/bin/kafka-run-class: line 450: exec: java: not found
===> Done kafka-storage
===> Launching ... 
===> Launching kafka... 
===> Kafka start...
2025-06-22T05:26:38.686644Z main ERROR Class class org.apache.logging.log4j.core.pattern.LineLocationPatternConverter does not contain a static newInstance method
2025-06-22T05:26:38.687549Z main ERROR Unrecognized conversion specifier [L] starting at position 17 in conversion pattern.
[2025-06-22 05:26:38,699] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$:%L)
[2025-06-22 05:26:38,751] INFO KafkaConfig values: 
	add.partitions.to.txn.retry.backoff.max.ms = 100
	add.partitions.to.txn.retry.backoff.ms = 20
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = f4iDkhvfTGqMLnNPxFcjWw
	client.quota.callback.class = null
	client.quota.max.throttle.time.in.response.ms = 60000
	client.quota.max.throttle.time.ms = 5000
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	confluent.accp.enabled = false
	confluent.acks.equal.to.one.request.replication.lag.threshold.ms = -1
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.api.visibility = DEFAULT
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
	confluent.balancer.capacity.threshold.upper.limit = 0.95
	confluent.balancer.cell.load.upper.bound = 0.7
	confluent.balancer.cell.overload.detection.interval.ms = 3600000
	confluent.balancer.cell.overload.duration.ms = 86400000
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.cpu.balance.threshold = 1.1
	confluent.balancer.cpu.goal.act.as.capacity.goal = false
	confluent.balancer.cpu.low.utilization.threshold = 0.2
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.enable.network.capacity.metric.ingestion = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.flex.fanout.network.capacity.metrics.avg.period.ms = 1800000
	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.min.valid.windows = 5
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.inter.cell.balancing.enabled = false
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.minimum.reported.brokers.with.network.capacity.metrics.percentage = 0.8
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.num.concurrent.replica.movements.as.destination.per.broker = 18
	confluent.balancer.num.concurrent.replica.movements.as.source.per.broker = 12
	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.sbc.metrics.parser.enabled = false
	confluent.balancer.self.healing.maximum.rounds = 1
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.tenant.maximum.movements = 0
	confluent.balancer.tenant.suspension.ms = 86400000
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.balancing.itrdg.with.hard.goals.enabled = false
	confluent.balancer.topic.partition.maximum.movements = 3
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.movements.history.limit = 900
	confluent.balancer.topic.partition.suspension.ms = 3600000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.balancer.v2.addition.enabled = false
	confluent.balancer.v2.addition.reassignment.cancellations.enabled = false
	confluent.balancer.v2.executor.enabled = false
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.assertion.claim.aud = null
	confluent.bearer.assertion.claim.exp.minutes = null
	confluent.bearer.assertion.claim.iss = null
	confluent.bearer.assertion.claim.jti.include = null
	confluent.bearer.assertion.claim.nbf.include = null
	confluent.bearer.assertion.claim.sub = null
	confluent.bearer.assertion.file = null
	confluent.bearer.assertion.private.key.file = null
	confluent.bearer.assertion.private.key.passphrase = null
	confluent.bearer.assertion.template.file = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.advertised.limit.load = 0.8
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.tenant.metric.enable = false
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.calling.resource.identity.type.map = 
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.full.configs.enable = false
	confluent.catalog.collector.max.bytes.per.snapshot = 850000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.multitenant.topics.enable = true
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cell.metrics.refresh.period.ms = 60000
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.k2.base.broker.index = -1
	confluent.cells.load.refresher.enable = true
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.client.topic.max.metrics.count = 1000
	confluent.client.topic.metrics.expiry.sec = 3600
	confluent.client.topic.metrics.manager = class org.apache.kafka.server.metrics.ClientTopicMetricsManager$NoOpClientTopicMetricsManager
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.list.object.thread_pool.size = 1
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.admin.max.in.flight.requests = 1000
	confluent.cluster.link.admin.request.batch.size = 1
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.allow.legacy.message.format = false
	confluent.cluster.link.allow.truncation.below.hwm = false
	confluent.cluster.link.availability.check.mode = ALL
	confluent.cluster.link.background.thread.affinity = LINK
	confluent.cluster.link.bootstrap.translation.feature.enable = true
	confluent.cluster.link.clients.max.idle.ms = 3153600000000
	confluent.cluster.link.enable = true
	confluent.cluster.link.enable.local.admin = false
	confluent.cluster.link.enable.metrics.reduction = false
	confluent.cluster.link.enable.metrics.reduction.advanced = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.fetcher.auto.tune.enable = false
	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
	confluent.cluster.link.insync.fetch.response.min.bytes = 1
	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.intranet.connectivity.denied.org.ids = []
	confluent.cluster.link.intranet.connectivity.enable = false
	confluent.cluster.link.intranet.connectivity.migration.enable = false
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.local.admin.multitenant.enable = false
	confluent.cluster.link.local.reverse.connection.listener.map = null
	confluent.cluster.link.max.client.connections = 2147483647
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.mirror.transition.batch.size = 10
	confluent.cluster.link.num.background.threads = 1
	confluent.cluster.link.periodic.task.batch.size = 2147483647
	confluent.cluster.link.periodic.task.min.interval.ms = 1000
	confluent.cluster.link.persistent.connection.backoff.max.ms = 0
	confluent.cluster.link.replica.fetch.connections.mode = combined
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.cluster.link.request.quota.capacity = 400
	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
	confluent.cluster.link.switchover.disabled.principals = []
	confluent.cluster.link.switchover.enable = false
	confluent.cluster.link.switchover.listeners = []
	confluent.cluster.link.switchover.server.states = []
	confluent.cluster.link.tenant.replication.quota.enable = false
	confluent.cluster.link.tenant.request.quota.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
	confluent.cluster.metadata.snapshot.tier.upload.enable = false
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
	confluent.consumer.fetch.partition.pruning.enable = true
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.dataflow.policy.watch.monitor.ms = 300000
	confluent.default.data.policy.enforcement = true
	confluent.defer.isr.shrink.enable = false
	confluent.describe.topic.partitions.enabled = true
	confluent.disk.io.manager.enable = false
	confluent.disk.throughput.headroom = 10485760
	confluent.disk.throughput.limit = 10485760000
	confluent.disk.throughput.quota.tier.archive = 1048576000
	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.log.ignored.event.types = 
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 1
	confluent.e2e_checksum.protection.enabled = false
	confluent.e2e_checksum.protection.files = [none]
	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
	confluent.elastic.cku.enabled = false
	confluent.elastic.cku.scaletozero.enabled = false
	confluent.eligible.controllers = []
	confluent.enable.broker.reporting.min.usage.mode = true
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fail.unsatisfied.placement.constraints = false
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.flexible.fanout.broker.max.fetch.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.max.produce.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.min.producer.percentage = 10.0
	confluent.flexible.fanout.broker.network.out.bytes.per.second = 6200000
	confluent.flexible.fanout.broker.recompute.interval.ms = 30000
	confluent.flexible.fanout.broker.storage.bytes.per.second = 512000000
	confluent.flexible.fanout.enabled = false
	confluent.flexible.fanout.lazy.evaluation.threshold = 0.5
	confluent.flexible.fanout.mode = TENANT_QUOTA
	confluent.floor.connection.rate.per.ip = -1.0
	confluent.floor.connection.rate.per.tenant = -1.0
	confluent.group.coordinator.dynamic.append.linger.enable = false
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.coordinator.txn.offset.validation.enable = false
	confluent.group.highest.offset.commit.rates.log.count = 10
	confluent.group.highest.offset.commit.rates.log.enable = false
	confluent.group.highest.offset.commit.rates.log.interval.ms = 300000
	confluent.group.metadata.load.threads = 32
	confluent.group.subscription.pattern.log.interval.ms = -1
	confluent.heap.tenured.notify.bytes = 0
	confluent.heap.tenured.notify.enabled = false
	confluent.hot.partition.ratio = 0.8
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.internal.rest.server.ssl.enable = false
	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
	confluent.leader.epoch.checkpoint.checksum.enabled = false
	confluent.listener.protocol = TCP
	confluent.log.cleaner.timestamp.validation.enable = true
	confluent.log.placement.constraints = 
	confluent.max.broker.load = 1.0
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
	confluent.max.connection.rate.per.ip = -1.0
	confluent.max.connection.rate.per.tenant = -1.0
	confluent.max.connection.throttle.ms = null
	confluent.max.segment.ms = 9223372036854775807
	confluent.metadata.active.encryptor = null
	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.required = false
	confluent.metadata.encryptor.secret.file = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.jvm.warmup.ms = 60000
	confluent.metadata.leader.balance.slice.delay.ms = 100
	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
	confluent.metadata.max.leader.balance.changes.per.slice = 1000
	confluent.metadata.reject.when.throttled.enable = false
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.connection.throttle.ms = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 20000
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.mtls.build.client.cert.chain.enable = false
	confluent.mtls.enable = false
	confluent.mtls.listener.name = EXTERNAL
	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
	confluent.mtls.truststore.alter.configs.timeout.ms = 300000
	confluent.mtls.truststore.manager.class.name = null
	confluent.multitenant.authorizer.enable.acl.state = false
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.lkc.id.enable = false
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.external.listener.name = EXTERNAL
	confluent.network.health.manager.externalconnectivitystartup.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.oauth.flat.networking.verification.enable = false
	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
	confluent.offsets.topic.placement.constraints = 
	confluent.omit.network.processor.metric.tag = false
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.ppv2.endpoint.scheme.bootstrap.broker.template.mappings = 
	confluent.ppv2.endpoint.scheme.enable = false
	confluent.ppv2.endpoint.scheme.map.broker.zone.to.gateway.zone = false
	confluent.ppv2.endpoint.scheme.template.variable.cloud = 
	confluent.ppv2.endpoint.scheme.template.variable.domain = 
	confluent.ppv2.endpoint.scheme.template.variable.region = 
	confluent.ppv2.endpoint.scheme.template.variables = 
	confluent.ppv2.endpoint.scheme.templates = 
	confluent.prefer.tier.fetch.ms = -1
	confluent.produce.throttle.pre.check.enable = false
	confluent.produce.throttle.pre.check.for.new.connection.enable = false
	confluent.producer.id.cache.broker.hard.limit = -1
	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
	confluent.producer.id.cache.extra.eviction.percentage = 0
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.cache.partition.hard.limit = -1
	confluent.producer.id.cache.tenant.hard.limit = -1
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.quota.window.num = 11
	confluent.producer.id.quota.window.size.seconds = 1
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.mode.local.default = false
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.computing.usage.adjustment = 0.5
	confluent.quota.dynamic.adjustment.min.usage = 102400
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.internal.broker.max.consumer.rate = 9223372036854775807
	confluent.quota.tenant.internal.broker.max.producer.rate = 9223372036854775807
	confluent.quota.tenant.internal.throttling.enable = false
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.rack.id.mapping = null
	confluent.regional.metadata.client.class = null
	confluent.regional.resource.manager.client.scheduler.threads = 2
	confluent.regional.resource.manager.endpoint = null
	confluent.regional.resource.manager.watch.endpoint = null
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.replication.mode = PULL
	confluent.replication.push.feature.enable = false
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = true
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.calling.resource.identity = false
	confluent.require.compatible.keystore.updates = true
	confluent.require.confluent.issuer = false
	confluent.roll.check.interval.ms = 300000
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validation.context.name.enable = false
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.bc.approved.mode.enable = false
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.config = 
	confluent.security.revoked.certificate.ids = 
	confluent.segment.eager.roll.enable = false
	confluent.segment.speculative.prefetch.enable = false
	confluent.share.metadata.load.threads = 32
	confluent.spiffe.id.principal.extraction.rules = 
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.step.connection.rate.per.ip = -1.0
	confluent.step.connection.rate.per.tenant = -1.0
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.subdomain.prefix = null
	confluent.subdomain.separator.map = null
	confluent.subdomain.separator.variable = %sep
	confluent.system.time.roll.enable = false
	confluent.telemetry.enabled = false
	confluent.telemetry.external.client.metrics.delta.temporality = true
	confluent.telemetry.external.client.metrics.instance.cache.size = 16384
	confluent.telemetry.external.client.metrics.push.enabled = false
	confluent.telemetry.external.client.metrics.subscription.interval.ms.list = null
	confluent.telemetry.external.client.metrics.subscription.match.list = null
	confluent.telemetry.external.client.metrics.subscription.metrics.list = null
	confluent.tenant.latency.metric.enabled = false
	confluent.tenantaware.encryption.key.manager.enable = false
	confluent.tenantaware.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.tenantaware.encryption.key.manager.tenant.cache.eviction.time.sec = 172800
	confluent.tenantaware.encryption.key.manager.tenant.cache.size = 100
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.async.enable = false
	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 1
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.prefetch.cache.enable = false
	confluent.tier.prefetch.cache.entry.size.bytes = 1048576
	confluent.tier.prefetch.cache.range.bytes = 5242880
	confluent.tier.prefetch.cache.total.size.bytes = 209715200
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.ipv6.enabled = true
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.security.providers = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.provider = null
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.storage.class.override = 
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.s3.v2.enabled = false
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
	confluent.tier.topic.data.loss.validation.fencing.enable = false
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.head.data.loss.validation.enable = true
	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
	confluent.tier.topic.materialization.from.snapshot.enable = false
	confluent.tier.topic.producer.enable.idempotence = true
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.metadata.throttle.pre.check.partition.count.threshold = 1000
	confluent.topic.partition.default.placement = 2
	confluent.topic.policy.use.computed.assignments = false
	confluent.topic.replica.assignor.builder.class = 
	confluent.track.api.key.per.ip = false
	confluent.track.per.ip.max.size = 100000
	confluent.track.tenant.id.per.ip = false
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.traffic.network.type = 
	confluent.transaction.2pc.timeout.ms = -1
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.unique.deprecated.request.metrics.per.tenant = 1000
	confluent.valid.broker.rack.set = null
	confluent.valid.sni.hostnames = 
	confluent.valid.sni.hostnames.exclude.suffix = 
	confluent.verify.group.subscription.prefix = false
	confluent.virtual.topic.creation.enabled = false
	confluent.zone.tagged.request.metrics.enable = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	controlled.shutdown.enable = true
	controller.listener.names = CONTROLLER
	controller.performance.always.log.threshold.ms = 2000
	controller.performance.sample.period.ms = 60000
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.consumer.assignors = [uniform, range]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = bidirectional
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 5
	group.coordinator.new.enable = true
	group.coordinator.rebalance.protocols = [classic, consumer]
	group.coordinator.threads = 4
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	group.share.delivery.count.limit = 5
	group.share.enable = false
	group.share.heartbeat.interval.ms = 5000
	group.share.max.groups = 10
	group.share.max.heartbeat.interval.ms = 15000
	group.share.max.record.lock.duration.ms = 60000
	group.share.max.session.timeout.ms = 60000
	group.share.max.size = 200
	group.share.min.heartbeat.interval.ms = 5000
	group.share.min.record.lock.duration.ms = 15000
	group.share.min.session.timeout.ms = 45000
	group.share.partition.max.record.locks = 200
	group.share.persister.class.name = org.apache.kafka.server.share.persister.DefaultStatePersister
	group.share.record.lock.duration.ms = 30000
	group.share.session.timeout.ms = 45000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	k2.stack.builder.class.name = null
	k2.startup.timeout.ms = 60000
	k2.topic.metadata.refresh.ms = 10000
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.cleanup.policy.empty.validation = none
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.timestamp.after.max.ms = 3600000
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.connections.per.tenant = 0
	max.connections.protected.listeners = []
	max.connections.reap.amount = 0
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.authorizer.support.resource.ids = false
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 2
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	otel.exporter.otlp.custom.endpoint = default
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.fetch.max.wait.ms = 500
	remote.list.offsets.request.timeout.ms = 30000
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 2
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	share.coordinator.append.linger.ms = 5
	share.coordinator.load.buffer.size = 5242880
	share.coordinator.snapshot.update.records.per.snapshot = 500
	share.coordinator.state.topic.compression.codec = 0
	share.coordinator.state.topic.min.isr = 2
	share.coordinator.state.topic.num.partitions = 50
	share.coordinator.state.topic.prune.interval.ms = 300000
	share.coordinator.state.topic.replication.factor = 3
	share.coordinator.state.topic.segment.bytes = 104857600
	share.coordinator.threads = 1
	share.coordinator.write.timeout.ms = 5000
	share.fetch.max.fetch.records = 2147483647
	share.fetch.purgatory.purge.interval.requests = 1000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	throughput.quota.window.num = 11
	token.impersonation.validation = true
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.metadata.load.threads = 32
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unclean.leader.election.interval.ms = 300000
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,758] INFO KafkaConfig values: 
	add.partitions.to.txn.retry.backoff.max.ms = 100
	add.partitions.to.txn.retry.backoff.ms = 20
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = f4iDkhvfTGqMLnNPxFcjWw
	client.quota.callback.class = null
	client.quota.max.throttle.time.in.response.ms = 60000
	client.quota.max.throttle.time.ms = 5000
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	confluent.accp.enabled = false
	confluent.acks.equal.to.one.request.replication.lag.threshold.ms = -1
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.api.visibility = DEFAULT
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
	confluent.balancer.capacity.threshold.upper.limit = 0.95
	confluent.balancer.cell.load.upper.bound = 0.7
	confluent.balancer.cell.overload.detection.interval.ms = 3600000
	confluent.balancer.cell.overload.duration.ms = 86400000
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.cpu.balance.threshold = 1.1
	confluent.balancer.cpu.goal.act.as.capacity.goal = false
	confluent.balancer.cpu.low.utilization.threshold = 0.2
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.enable.network.capacity.metric.ingestion = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.flex.fanout.network.capacity.metrics.avg.period.ms = 1800000
	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.min.valid.windows = 5
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.inter.cell.balancing.enabled = false
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.minimum.reported.brokers.with.network.capacity.metrics.percentage = 0.8
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.num.concurrent.replica.movements.as.destination.per.broker = 18
	confluent.balancer.num.concurrent.replica.movements.as.source.per.broker = 12
	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.sbc.metrics.parser.enabled = false
	confluent.balancer.self.healing.maximum.rounds = 1
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.tenant.maximum.movements = 0
	confluent.balancer.tenant.suspension.ms = 86400000
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.balancing.itrdg.with.hard.goals.enabled = false
	confluent.balancer.topic.partition.maximum.movements = 3
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.movements.history.limit = 900
	confluent.balancer.topic.partition.suspension.ms = 3600000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.balancer.v2.addition.enabled = false
	confluent.balancer.v2.addition.reassignment.cancellations.enabled = false
	confluent.balancer.v2.executor.enabled = false
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.assertion.claim.aud = null
	confluent.bearer.assertion.claim.exp.minutes = null
	confluent.bearer.assertion.claim.iss = null
	confluent.bearer.assertion.claim.jti.include = null
	confluent.bearer.assertion.claim.nbf.include = null
	confluent.bearer.assertion.claim.sub = null
	confluent.bearer.assertion.file = null
	confluent.bearer.assertion.private.key.file = null
	confluent.bearer.assertion.private.key.passphrase = null
	confluent.bearer.assertion.template.file = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.advertised.limit.load = 0.8
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.tenant.metric.enable = false
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.calling.resource.identity.type.map = 
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.full.configs.enable = false
	confluent.catalog.collector.max.bytes.per.snapshot = 850000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.multitenant.topics.enable = true
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cell.metrics.refresh.period.ms = 60000
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.k2.base.broker.index = -1
	confluent.cells.load.refresher.enable = true
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.client.topic.max.metrics.count = 1000
	confluent.client.topic.metrics.expiry.sec = 3600
	confluent.client.topic.metrics.manager = class org.apache.kafka.server.metrics.ClientTopicMetricsManager$NoOpClientTopicMetricsManager
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.list.object.thread_pool.size = 1
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.admin.max.in.flight.requests = 1000
	confluent.cluster.link.admin.request.batch.size = 1
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.allow.legacy.message.format = false
	confluent.cluster.link.allow.truncation.below.hwm = false
	confluent.cluster.link.availability.check.mode = ALL
	confluent.cluster.link.background.thread.affinity = LINK
	confluent.cluster.link.bootstrap.translation.feature.enable = true
	confluent.cluster.link.clients.max.idle.ms = 3153600000000
	confluent.cluster.link.enable = true
	confluent.cluster.link.enable.local.admin = false
	confluent.cluster.link.enable.metrics.reduction = false
	confluent.cluster.link.enable.metrics.reduction.advanced = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.fetcher.auto.tune.enable = false
	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
	confluent.cluster.link.insync.fetch.response.min.bytes = 1
	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.intranet.connectivity.denied.org.ids = []
	confluent.cluster.link.intranet.connectivity.enable = false
	confluent.cluster.link.intranet.connectivity.migration.enable = false
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.local.admin.multitenant.enable = false
	confluent.cluster.link.local.reverse.connection.listener.map = null
	confluent.cluster.link.max.client.connections = 2147483647
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.mirror.transition.batch.size = 10
	confluent.cluster.link.num.background.threads = 1
	confluent.cluster.link.periodic.task.batch.size = 2147483647
	confluent.cluster.link.periodic.task.min.interval.ms = 1000
	confluent.cluster.link.persistent.connection.backoff.max.ms = 0
	confluent.cluster.link.replica.fetch.connections.mode = combined
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.cluster.link.request.quota.capacity = 400
	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
	confluent.cluster.link.switchover.disabled.principals = []
	confluent.cluster.link.switchover.enable = false
	confluent.cluster.link.switchover.listeners = []
	confluent.cluster.link.switchover.server.states = []
	confluent.cluster.link.tenant.replication.quota.enable = false
	confluent.cluster.link.tenant.request.quota.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
	confluent.cluster.metadata.snapshot.tier.upload.enable = false
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
	confluent.consumer.fetch.partition.pruning.enable = true
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.dataflow.policy.watch.monitor.ms = 300000
	confluent.default.data.policy.enforcement = true
	confluent.defer.isr.shrink.enable = false
	confluent.describe.topic.partitions.enabled = true
	confluent.disk.io.manager.enable = false
	confluent.disk.throughput.headroom = 10485760
	confluent.disk.throughput.limit = 10485760000
	confluent.disk.throughput.quota.tier.archive = 1048576000
	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.log.ignored.event.types = 
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 1
	confluent.e2e_checksum.protection.enabled = false
	confluent.e2e_checksum.protection.files = [none]
	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
	confluent.elastic.cku.enabled = false
	confluent.elastic.cku.scaletozero.enabled = false
	confluent.eligible.controllers = []
	confluent.enable.broker.reporting.min.usage.mode = true
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fail.unsatisfied.placement.constraints = false
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.flexible.fanout.broker.max.fetch.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.max.produce.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.min.producer.percentage = 10.0
	confluent.flexible.fanout.broker.network.out.bytes.per.second = 6200000
	confluent.flexible.fanout.broker.recompute.interval.ms = 30000
	confluent.flexible.fanout.broker.storage.bytes.per.second = 512000000
	confluent.flexible.fanout.enabled = false
	confluent.flexible.fanout.lazy.evaluation.threshold = 0.5
	confluent.flexible.fanout.mode = TENANT_QUOTA
	confluent.floor.connection.rate.per.ip = -1.0
	confluent.floor.connection.rate.per.tenant = -1.0
	confluent.group.coordinator.dynamic.append.linger.enable = false
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.coordinator.txn.offset.validation.enable = false
	confluent.group.highest.offset.commit.rates.log.count = 10
	confluent.group.highest.offset.commit.rates.log.enable = false
	confluent.group.highest.offset.commit.rates.log.interval.ms = 300000
	confluent.group.metadata.load.threads = 32
	confluent.group.subscription.pattern.log.interval.ms = -1
	confluent.heap.tenured.notify.bytes = 0
	confluent.heap.tenured.notify.enabled = false
	confluent.hot.partition.ratio = 0.8
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.internal.rest.server.ssl.enable = false
	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
	confluent.leader.epoch.checkpoint.checksum.enabled = false
	confluent.listener.protocol = TCP
	confluent.log.cleaner.timestamp.validation.enable = true
	confluent.log.placement.constraints = 
	confluent.max.broker.load = 1.0
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
	confluent.max.connection.rate.per.ip = -1.0
	confluent.max.connection.rate.per.tenant = -1.0
	confluent.max.connection.throttle.ms = null
	confluent.max.segment.ms = 9223372036854775807
	confluent.metadata.active.encryptor = null
	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.required = false
	confluent.metadata.encryptor.secret.file = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.jvm.warmup.ms = 60000
	confluent.metadata.leader.balance.slice.delay.ms = 100
	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
	confluent.metadata.max.leader.balance.changes.per.slice = 1000
	confluent.metadata.reject.when.throttled.enable = false
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.connection.throttle.ms = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 20000
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.mtls.build.client.cert.chain.enable = false
	confluent.mtls.enable = false
	confluent.mtls.listener.name = EXTERNAL
	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
	confluent.mtls.truststore.alter.configs.timeout.ms = 300000
	confluent.mtls.truststore.manager.class.name = null
	confluent.multitenant.authorizer.enable.acl.state = false
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.lkc.id.enable = false
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.external.listener.name = EXTERNAL
	confluent.network.health.manager.externalconnectivitystartup.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.oauth.flat.networking.verification.enable = false
	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
	confluent.offsets.topic.placement.constraints = 
	confluent.omit.network.processor.metric.tag = false
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.ppv2.endpoint.scheme.bootstrap.broker.template.mappings = 
	confluent.ppv2.endpoint.scheme.enable = false
	confluent.ppv2.endpoint.scheme.map.broker.zone.to.gateway.zone = false
	confluent.ppv2.endpoint.scheme.template.variable.cloud = 
	confluent.ppv2.endpoint.scheme.template.variable.domain = 
	confluent.ppv2.endpoint.scheme.template.variable.region = 
	confluent.ppv2.endpoint.scheme.template.variables = 
	confluent.ppv2.endpoint.scheme.templates = 
	confluent.prefer.tier.fetch.ms = -1
	confluent.produce.throttle.pre.check.enable = false
	confluent.produce.throttle.pre.check.for.new.connection.enable = false
	confluent.producer.id.cache.broker.hard.limit = -1
	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
	confluent.producer.id.cache.extra.eviction.percentage = 0
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.cache.partition.hard.limit = -1
	confluent.producer.id.cache.tenant.hard.limit = -1
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.quota.window.num = 11
	confluent.producer.id.quota.window.size.seconds = 1
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.mode.local.default = false
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.computing.usage.adjustment = 0.5
	confluent.quota.dynamic.adjustment.min.usage = 102400
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.internal.broker.max.consumer.rate = 9223372036854775807
	confluent.quota.tenant.internal.broker.max.producer.rate = 9223372036854775807
	confluent.quota.tenant.internal.throttling.enable = false
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.rack.id.mapping = null
	confluent.regional.metadata.client.class = null
	confluent.regional.resource.manager.client.scheduler.threads = 2
	confluent.regional.resource.manager.endpoint = null
	confluent.regional.resource.manager.watch.endpoint = null
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.replication.mode = PULL
	confluent.replication.push.feature.enable = false
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = true
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.calling.resource.identity = false
	confluent.require.compatible.keystore.updates = true
	confluent.require.confluent.issuer = false
	confluent.roll.check.interval.ms = 300000
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validation.context.name.enable = false
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.bc.approved.mode.enable = false
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.config = 
	confluent.security.revoked.certificate.ids = 
	confluent.segment.eager.roll.enable = false
	confluent.segment.speculative.prefetch.enable = false
	confluent.share.metadata.load.threads = 32
	confluent.spiffe.id.principal.extraction.rules = 
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.step.connection.rate.per.ip = -1.0
	confluent.step.connection.rate.per.tenant = -1.0
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.subdomain.prefix = null
	confluent.subdomain.separator.map = null
	confluent.subdomain.separator.variable = %sep
	confluent.system.time.roll.enable = false
	confluent.telemetry.enabled = false
	confluent.telemetry.external.client.metrics.delta.temporality = true
	confluent.telemetry.external.client.metrics.instance.cache.size = 16384
	confluent.telemetry.external.client.metrics.push.enabled = false
	confluent.telemetry.external.client.metrics.subscription.interval.ms.list = null
	confluent.telemetry.external.client.metrics.subscription.match.list = null
	confluent.telemetry.external.client.metrics.subscription.metrics.list = null
	confluent.tenant.latency.metric.enabled = false
	confluent.tenantaware.encryption.key.manager.enable = false
	confluent.tenantaware.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.tenantaware.encryption.key.manager.tenant.cache.eviction.time.sec = 172800
	confluent.tenantaware.encryption.key.manager.tenant.cache.size = 100
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.async.enable = false
	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 1
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.prefetch.cache.enable = false
	confluent.tier.prefetch.cache.entry.size.bytes = 1048576
	confluent.tier.prefetch.cache.range.bytes = 5242880
	confluent.tier.prefetch.cache.total.size.bytes = 209715200
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.ipv6.enabled = true
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.security.providers = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.provider = null
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.storage.class.override = 
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.s3.v2.enabled = false
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
	confluent.tier.topic.data.loss.validation.fencing.enable = false
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.head.data.loss.validation.enable = true
	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
	confluent.tier.topic.materialization.from.snapshot.enable = false
	confluent.tier.topic.producer.enable.idempotence = true
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.metadata.throttle.pre.check.partition.count.threshold = 1000
	confluent.topic.partition.default.placement = 2
	confluent.topic.policy.use.computed.assignments = false
	confluent.topic.replica.assignor.builder.class = 
	confluent.track.api.key.per.ip = false
	confluent.track.per.ip.max.size = 100000
	confluent.track.tenant.id.per.ip = false
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.traffic.network.type = 
	confluent.transaction.2pc.timeout.ms = -1
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.unique.deprecated.request.metrics.per.tenant = 1000
	confluent.valid.broker.rack.set = null
	confluent.valid.sni.hostnames = 
	confluent.valid.sni.hostnames.exclude.suffix = 
	confluent.verify.group.subscription.prefix = false
	confluent.virtual.topic.creation.enabled = false
	confluent.zone.tagged.request.metrics.enable = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	controlled.shutdown.enable = true
	controller.listener.names = CONTROLLER
	controller.performance.always.log.threshold.ms = 2000
	controller.performance.sample.period.ms = 60000
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.consumer.assignors = [uniform, range]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = bidirectional
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 5
	group.coordinator.new.enable = true
	group.coordinator.rebalance.protocols = [classic, consumer]
	group.coordinator.threads = 4
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	group.share.delivery.count.limit = 5
	group.share.enable = false
	group.share.heartbeat.interval.ms = 5000
	group.share.max.groups = 10
	group.share.max.heartbeat.interval.ms = 15000
	group.share.max.record.lock.duration.ms = 60000
	group.share.max.session.timeout.ms = 60000
	group.share.max.size = 200
	group.share.min.heartbeat.interval.ms = 5000
	group.share.min.record.lock.duration.ms = 15000
	group.share.min.session.timeout.ms = 45000
	group.share.partition.max.record.locks = 200
	group.share.persister.class.name = org.apache.kafka.server.share.persister.DefaultStatePersister
	group.share.record.lock.duration.ms = 30000
	group.share.session.timeout.ms = 45000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	k2.stack.builder.class.name = null
	k2.startup.timeout.ms = 60000
	k2.topic.metadata.refresh.ms = 10000
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.cleanup.policy.empty.validation = none
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.timestamp.after.max.ms = 3600000
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.connections.per.tenant = 0
	max.connections.protected.listeners = []
	max.connections.reap.amount = 0
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.authorizer.support.resource.ids = false
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 2
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	otel.exporter.otlp.custom.endpoint = default
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.fetch.max.wait.ms = 500
	remote.list.offsets.request.timeout.ms = 30000
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 2
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	share.coordinator.append.linger.ms = 5
	share.coordinator.load.buffer.size = 5242880
	share.coordinator.snapshot.update.records.per.snapshot = 500
	share.coordinator.state.topic.compression.codec = 0
	share.coordinator.state.topic.min.isr = 2
	share.coordinator.state.topic.num.partitions = 50
	share.coordinator.state.topic.prune.interval.ms = 300000
	share.coordinator.state.topic.replication.factor = 3
	share.coordinator.state.topic.segment.bytes = 104857600
	share.coordinator.threads = 1
	share.coordinator.write.timeout.ms = 5000
	share.fetch.max.fetch.records = 2147483647
	share.fetch.purgatory.purge.interval.requests = 1000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	throughput.quota.window.num = 11
	token.impersonation.validation = true
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.metadata.load.threads = 32
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unclean.leader.election.interval.ms = 300000
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,762] INFO Registering metric ActiveBalancerCount (io.confluent.databalancer.KafkaDataBalanceManager:%L)
[2025-06-22 05:26:38,763] INFO Instantiating ClusterBalanceManager with an instance of io.confluent.databalancer.SbcDataBalanceManager (ClusterBalanceManager:%L)
[2025-06-22 05:26:38,764] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler:%L)
[2025-06-22 05:26:38,764] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,766] INFO [ControllerServer id=1] FIPS mode enabled: false (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,768] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,769] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,769] INFO NamedConfigEnabled: false (io.confluent.security.audit.provider.ConfluentAuditLogProvider:%L)
[2025-06-22 05:26:38,769] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,770] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,770] INFO MultiTenantAuditLogConfig values: 
	confluent.security.event.logger.client.ip.enable = false
	confluent.security.event.logger.multitenant.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,770] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,770] INFO Audit Log rate limiter reconfigured: Authn: -1, Authz: -1, Kafka request: -1 (io.confluent.security.audit.provider.AuditLogRateLimiter:%L)
[2025-06-22 05:26:38,824] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,827] INFO Quota CONTROLLER-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota:%L)
[2025-06-22 05:26:38,827] INFO Quota CONTROLLER-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota:%L)
[2025-06-22 05:26:38,827] INFO Quota CONTROLLER-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota:%L)
[2025-06-22 05:26:38,827] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas:%L)
[2025-06-22 05:26:38,830] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,832] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,834] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,836] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) used TCP protocol (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,837] INFO [SharedServer id=1] Using localhost:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer:%L)
[2025-06-22 05:26:38,837] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer:%L)
[2025-06-22 05:26:38,864] INFO [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:38,865] INFO [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:38,865] INFO [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset $lastOffset (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:38,866] INFO Initialized snapshots with IDs SortedSet() from /var/lib/kafka/data/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$:%L)
[2025-06-22 05:26:38,873] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper:%L)
[2025-06-22 05:26:38,875] INFO [RaftManager id=1] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,876] INFO [RaftManager id=1] Starting voters are VoterSet(voters={1=VoterNode(voterKey=ReplicaKey(id=1, directoryId=<undefined>), listeners=Endpoints(endpoints={ListenerName(CONTROLLER)=localhost/127.0.0.1:29093}), supportedKRaftVersion=SupportedVersionRange[min_version:0, max_version:0])}) (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,876] INFO [RaftManager id=1] Starting request manager with static voters: [localhost:29093 (id: 1 rack: null isFenced: false)] (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,876] INFO [RaftManager id=1] Attempting durable transition to UnattachedState(epoch=0, leaderId=OptionalInt.empty, votedKey=Optional.empty, voters=[1], electionTimeoutMs=1859, highWatermark=Optional.empty) from null (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,880] INFO [RaftManager id=1] Completed transition to UnattachedState(epoch=0, leaderId=OptionalInt.empty, votedKey=Optional.empty, voters=[1], electionTimeoutMs=1859, highWatermark=Optional.empty) from null (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,881] INFO [RaftManager id=1] Completed transition to ProspectiveState(epoch=0, leaderId=OptionalInt.empty, retries=1, votedKey=Optional.empty, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), electionTimeoutMs=1818, highWatermark=Optional.empty) from UnattachedState(epoch=0, leaderId=OptionalInt.empty, votedKey=Optional.empty, voters=[1], electionTimeoutMs=1859, highWatermark=Optional.empty) (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,881] INFO [RaftManager id=1] Attempting durable transition to CandidateState(localId=1, localDirectoryId=y2hnhBSrRN9M9zSpC-BgBg, epoch=1, retries=1, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), highWatermark=Optional.empty, electionTimeoutMs=1201) from ProspectiveState(epoch=0, leaderId=OptionalInt.empty, retries=1, votedKey=Optional.empty, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), electionTimeoutMs=1818, highWatermark=Optional.empty) (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,883] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, localDirectoryId=y2hnhBSrRN9M9zSpC-BgBg, epoch=1, retries=1, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), highWatermark=Optional.empty, electionTimeoutMs=1201) from ProspectiveState(epoch=0, leaderId=OptionalInt.empty, retries=1, votedKey=Optional.empty, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), electionTimeoutMs=1818, highWatermark=Optional.empty) (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,883] INFO [RaftManager id=1] Attempting durable transition to Leader(localVoterNode=VoterNode(voterKey=ReplicaKey(id=1, directoryId=y2hnhBSrRN9M9zSpC-BgBg), listeners=Endpoints(endpoints={ListenerName(CONTROLLER)=localhost/<unresolved>:29093}), supportedKRaftVersion=SupportedVersionRange[min_version:0, max_version:1]), epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, localDirectoryId=y2hnhBSrRN9M9zSpC-BgBg, epoch=1, retries=1, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), highWatermark=Optional.empty, electionTimeoutMs=1201) (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,884] INFO [RaftManager id=1] Completed transition to Leader(localVoterNode=VoterNode(voterKey=ReplicaKey(id=1, directoryId=y2hnhBSrRN9M9zSpC-BgBg), listeners=Endpoints(endpoints={ListenerName(CONTROLLER)=localhost/<unresolved>:29093}), supportedKRaftVersion=SupportedVersionRange[min_version:0, max_version:1]), epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, localDirectoryId=y2hnhBSrRN9M9zSpC-BgBg, epoch=1, retries=1, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), highWatermark=Optional.empty, electionTimeoutMs=1201) (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,945] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread:%L)
[2025-06-22 05:26:38,945] INFO [kafka-1-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver:%L)
[2025-06-22 05:26:38,946] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState:%L)
[2025-06-22 05:26:38,947] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,947] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,947] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,950] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@2104880776 (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,950] INFO [RaftManager id=1] Setting the next offset of org.apache.kafka.image.loader.MetadataLoader@2104880776 to 0 since there are no snapshots (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,950] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,950] INFO [ControllerServer id=1] Registering periodic task writeNoOpRecord to run every 500 ms (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,950] INFO [ControllerServer id=1] Registering periodic task maybeFenceStaleBroker to run every 1125 ms (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,950] INFO [ControllerServer id=1] Registering periodic task electUnclean to run every 300000 ms (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,950] INFO [ControllerServer id=1] Registering periodic task expireDelegationTokens to run every 3600000 ms (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,950] INFO [ControllerServer id=1] Registering periodic task generatePeriodicPerformanceMessage to run every 60000 ms (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,951] INFO [ControllerServer id=1] Creating new QuorumController with clusterId MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.controller.QuorumController:%L)
[2025-06-22 05:26:38,951] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@1740701627 (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,951] INFO [RaftManager id=1] Setting the next offset of org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@1740701627 to 0 since there are no snapshots (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,951] INFO [ControllerServer id=1] Becoming the active controller at epoch 1, next write offset 1. (org.apache.kafka.controller.QuorumController:%L)
[2025-06-22 05:26:38,951] WARN [ControllerServer id=1] Performing controller activation. The metadata log appears to be empty. Appending 1 bootstrap record(s) in metadata transaction at metadata.version 4.0-IV3A from bootstrap source 'the default bootstrap'. (org.apache.kafka.controller.QuorumController:%L)
[2025-06-22 05:26:38,951] INFO [ControllerServer id=1] Replayed BeginTransactionRecord(name='Bootstrap records') at offset 1. (org.apache.kafka.controller.OffsetControlManager:%L)
[2025-06-22 05:26:38,951] INFO [ControllerServer id=1] Replayed a Confluent FeatureLevelRecord setting metadata version to 4.0-IV3A (org.apache.kafka.controller.FeatureControlManager:%L)
[2025-06-22 05:26:38,952] INFO [ControllerServer id=1] Replayed EndTransactionRecord() at offset 3. (org.apache.kafka.controller.OffsetControlManager:%L)
[2025-06-22 05:26:38,952] INFO [ControllerServer id=1] Activated periodic tasks: electUnclean, expireDelegationTokens, generatePeriodicPerformanceMessage, maybeFenceStaleBroker, writeNoOpRecord (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,952] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,952] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,953] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,953] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,953] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,953] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,953] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClientRequestQuotaManager:%L)
[2025-06-22 05:26:38,953] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,953] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,954] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClusterLinkRequestQuotaManager:%L)
[2025-06-22 05:26:38,954] INFO [controller-1-ThrottledChannelReaper-ClusterLinkRequest]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,954] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,955] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:38,955] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,955] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,957] INFO [ControllerServer id=1] Self-Balancing Kafka is enabled and will be installed as a metadata publisher. (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,959] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,959] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,959] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,959] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,959] INFO Awaiting socket connections on localhost:29093. (kafka.network.DataPlaneAcceptor:%L)
[2025-06-22 05:26:38,960] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Waiting for multi-tenant metadata loader to be started (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Finished waiting for multi-tenant metadata loader to be started (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:38,961] INFO [ControllerRegistrationManager id=1 incarnation=fPcYExt9Rl-z07xud2d9qQ] initialized channel manager. (kafka.server.ControllerRegistrationManager:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerRegistrationManager id=1 incarnation=fPcYExt9Rl-z07xud2d9qQ] maybeSendControllerRegistration: cannot register yet because the metadata.version is not known yet. (kafka.server.ControllerRegistrationManager:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Finished waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:29093 (id: 1 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:38,961] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:38,961] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:38,965] INFO [BrokerServer id=1] FIPS mode enabled: false (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:38,965] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,965] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,965] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,965] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,965] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,966] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClientRequestQuotaManager:%L)
[2025-06-22 05:26:38,966] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,966] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,966] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClusterLinkRequestQuotaManager:%L)
[2025-06-22 05:26:38,966] INFO [broker-1-ThrottledChannelReaper-ClusterLinkRequest]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,966] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,966] INFO FlexFanout is not enabled or there is no ClientQuotaCallback so does not start FlexFanoutQuotaManager. (kafka.server.FlexFanoutQuotaManager:%L)
[2025-06-22 05:26:38,968] INFO Skip DiskIOManager init: confluent.disk.io.manager.enable = false (kafka.server.resource.DiskIOManager:%L)
[2025-06-22 05:26:38,968] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO NamedConfigEnabled: false (io.confluent.security.audit.provider.ConfluentAuditLogProvider:%L)
[2025-06-22 05:26:38,968] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO MultiTenantAuditLogConfig values: 
	confluent.security.event.logger.client.ip.enable = false
	confluent.security.event.logger.multitenant.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO Audit Log rate limiter reconfigured: Authn: -1, Authz: -1, Kafka request: -1 (io.confluent.security.audit.provider.AuditLogRateLimiter:%L)
[2025-06-22 05:26:38,968] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:38,968] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:38,969] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:38,969] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:29093 (id: 1 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:38,969] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper:%L)
[2025-06-22 05:26:38,971] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:38,977] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 4 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [ControllerServer id=1] Loaded new metadata Features(metadataVersion=4.0-IV3A, finalizedFeatures={confluent.metadata.version=127}, finalizedFeaturesEpoch=3). (org.apache.kafka.metadata.publisher.FeaturesPublisher:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ConfluentControllerMetricsPublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [ConfluentControllerMetricsChanges id=1] Finished reloading all Confluent controller metrics in 0 ms. (org.apache.kafka.controller.metrics.ConfluentControllerMetricsChanges:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing CellControllerMetadataMetricsPublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SbcDataBalanceManager with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO Balancer received a new Replica Exclusions Image (image: , delta: BrokerReplicaExclusionsDelta{newExclusions=[], removedExclusions=[]}) (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO Handling event SbcAlteredExclusionsEvent-4 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO SBC Event SbcMetadataUpdateEvent-1 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-3]. Enqueuing... (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO Handling event SbcLeaderUpdateEvent-2 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO This balancer node is now the metadata quorum leader. Activating kafkadatabalance manager without alive broker snapshot. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO [ControllerRegistrationManager id=1 incarnation=fPcYExt9Rl-z07xud2d9qQ] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=fPcYExt9Rl-z07xud2d9qQ, zkMigrationReady=false, listeners=[Listener(name='CONTROLLER', host='localhost', port=29093, securityProtocol=0)], features=[Feature(name='group.version', minSupportedVersion=0, maxSupportedVersion=1), Feature(name='confluent.metadata.version', minSupportedVersion=7, maxSupportedVersion=127), Feature(name='transaction.version', minSupportedVersion=0, maxSupportedVersion=2), Feature(name='eligible.leader.replicas.version', minSupportedVersion=0, maxSupportedVersion=1), Feature(name='kraft.version', minSupportedVersion=0, maxSupportedVersion=1), Feature(name='metadata.version', minSupportedVersion=7, maxSupportedVersion=25)], metadataEncryptors=[]) (kafka.server.ControllerRegistrationManager:%L)
[2025-06-22 05:26:38,978] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,979] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,979] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,982] INFO [SocketServer listenerType=BROKER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,982] INFO [ControllerServer id=1] Node 1 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:38,982] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:38,982] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:38,982] INFO [ControllerServer id=1] Replayed RegisterControllerRecord containing ControllerRegistration(id=1, incarnationId=fPcYExt9Rl-z07xud2d9qQ, zkMigrationReady=false, listeners=[Endpoint(listenerName='CONTROLLER', securityProtocol=PLAINTEXT, host='localhost', port=29093)], supportedFeatures={confluent.metadata.version: 7-127, eligible.leader.replicas.version: 0-1, group.version: 0-1, kraft.version: 0-1, metadata.version: 7-25, transaction.version: 0-2}, metadataEncryptors=[]). (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:38,983] INFO Quota PLAINTEXT-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota:%L)
[2025-06-22 05:26:38,983] INFO Quota PLAINTEXT-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota:%L)
[2025-06-22 05:26:38,983] INFO Quota PLAINTEXT-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota:%L)
[2025-06-22 05:26:38,983] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas:%L)
[2025-06-22 05:26:38,984] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,986] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,987] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,988] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) used TCP protocol (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,988] INFO [SocketServer listenerType=BROKER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,989] INFO Quota PLAINTEXT_HOST-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota:%L)
[2025-06-22 05:26:38,989] INFO Quota PLAINTEXT_HOST-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota:%L)
[2025-06-22 05:26:38,989] INFO Quota PLAINTEXT_HOST-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota:%L)
[2025-06-22 05:26:38,989] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas:%L)
[2025-06-22 05:26:38,990] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,997] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,001] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,005] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) used TCP protocol (kafka.network.SocketServer:%L)
[2025-06-22 05:26:39,009] INFO [ControllerRegistrationManager id=1 incarnation=fPcYExt9Rl-z07xud2d9qQ] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager:%L)
[2025-06-22 05:26:39,009] INFO [ControllerRegistrationManager id=1 incarnation=fPcYExt9Rl-z07xud2d9qQ] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager:%L)
[2025-06-22 05:26:39,009] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,009] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:29093 (id: 1 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,013] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,013] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:29093 (id: 1 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,015] INFO [BrokerHealthManager]: Starting (kafka.availability.BrokerHealthManager:%L)
[2025-06-22 05:26:39,017] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,018] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,018] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,018] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,019] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,019] INFO ReplicationConfig values: 
	confluent.replication.linger.ms = 0
	confluent.replication.max.in.flight.requests = 1
	confluent.replication.max.memory.buffer.bytes = 209715200
	confluent.replication.max.replica.pushers = 4
	confluent.replication.max.wait.ms = 500
	confluent.replication.mode = PULL
	confluent.replication.num.pushers.per.broker = 1
	confluent.replication.push.internal.topics.enable = false
	confluent.replication.request.max.bytes = 52428800
	confluent.replication.request.max.partition.bytes = 52428800
	confluent.replication.request.timeout.ms = 5000
	confluent.replication.retry.timeout.ms = 10000
	confluent.replication.socket.send.buffer.bytes = 1048576
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,020] INFO [BrokerServer id=1] Using no op persister (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,021] INFO [group-coordinator-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper:%L)
[2025-06-22 05:26:39,076] INFO [group-coordinator-event-processor-0]: Starting (org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread:%L)
[2025-06-22 05:26:39,076] INFO [group-coordinator-event-processor-2]: Starting (org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread:%L)
[2025-06-22 05:26:39,076] INFO [group-coordinator-event-processor-1]: Starting (org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread:%L)
[2025-06-22 05:26:39,076] INFO [group-coordinator-event-processor-3]: Starting (org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread:%L)
[2025-06-22 05:26:39,078] INFO Unable to read the broker epoch in /var/lib/kafka/data. (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,078] INFO [SharedServer id=1] Using localhost:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer:%L)
[2025-06-22 05:26:39,078] INFO [SharedServer id=1] Using localhost:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer:%L)
[2025-06-22 05:26:39,078] INFO [BrokerLifecycleManager id=1] Incarnation HAVdWLvgQLyNZfKdxKnPRg of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qk is now STARTING. (kafka.server.BrokerLifecycleManager:%L)
[2025-06-22 05:26:39,078] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,078] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:29093 (id: 1 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,079] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,079] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,079] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,079] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,079] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,079] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,080] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,080] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,080] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,080] INFO [BrokerServer id=1] Waiting for broker metadata to catch up (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,080] INFO [ControllerServer id=1] No previous registration found for broker 1. New incarnation ID is HAVdWLvgQLyNZfKdxKnPRg.  Generated 0 record(s) to clean up previous incarnations. New broker epoch is 5. (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,080] INFO [ControllerServer id=1] Node 1 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,080] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,080] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,081] INFO [ControllerServer id=1] Replayed initial RegisterBrokerRecord for broker 1: RegisterBrokerRecord(brokerId=1, isMigratingZkBroker=false, incarnationId=HAVdWLvgQLyNZfKdxKnPRg, brokerEpoch=5, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='localhost', port=29092, securityProtocol=0), BrokerEndpoint(name='PLAINTEXT_HOST', host='localhost', port=9092, securityProtocol=0)], features=[BrokerFeature(name='group.version', minSupportedVersion=0, maxSupportedVersion=1), BrokerFeature(name='confluent.metadata.version', minSupportedVersion=7, maxSupportedVersion=127), BrokerFeature(name='transaction.version', minSupportedVersion=0, maxSupportedVersion=2), BrokerFeature(name='eligible.leader.replicas.version', minSupportedVersion=0, maxSupportedVersion=1), BrokerFeature(name='kraft.version', minSupportedVersion=0, maxSupportedVersion=1), BrokerFeature(name='metadata.version', minSupportedVersion=7, maxSupportedVersion=25)], rack=null, fenced=true, inControlledShutdown=false, degradedComponents=[], metadataEncryptors=[], logDirs=[y2hnhBSrRN9M9zSpC-BgBg]) (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,106] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,106] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,106] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,106] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,107] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 5 (kafka.server.BrokerLifecycleManager:%L)
[2025-06-22 05:26:39,107] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager:%L)
[2025-06-22 05:26:39,107] INFO [BrokerServer id=1] Finished waiting for broker metadata to catch up (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO Starting DynamicMetricsReportersScheduler. (kafka.server.DynamicMetricsReportersScheduler:%L)
[2025-06-22 05:26:39,109] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO Attempting to initiate DynamicMetricsReporters. Attempt: 1 (kafka.server.DynamicMetricsReportersScheduler:%L)
[2025-06-22 05:26:39,109] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing MetadataVersionPublisher(id=1) with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:39,109] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:39,109] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version Optional[4.0-IV3A]. (kafka.server.metadata.BrokerMetadataPublisher:%L)
[2025-06-22 05:26:39,109] INFO Loading logs from log dirs ArrayBuffer(/var/lib/kafka/data) (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,109] INFO No logs found to be loaded in /var/lib/kafka/data (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,110] INFO Loaded 0 logs in 1ms (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,110] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,110] INFO EventEmitterConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,110] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,110] INFO Starting log roller with a period of 300000 ms. (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,111] INFO Linux CPU collector enabled: true (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,111] INFO Using cpu metric: io\.confluent\.kafka\.server/server/linux_system_cpu_utilization_1m (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,111] INFO Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,112] INFO Starting the log cleaner (kafka.log.LogCleaner:%L)
[2025-06-22 05:26:39,112] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig:%L)
[2025-06-22 05:26:39,112] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,363] WARN Ignoring redefinition of existing telemetry label kafka.version (io.confluent.telemetry.ResourceBuilderFacade:%L)
[2025-06-22 05:26:39,363] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager:%L)
[2025-06-22 05:26:39,365] INFO Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,365] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread:%L)
[2025-06-22 05:26:39,365] INFO ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.cluster.id = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.external.client.metrics.exclude.labels = 
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io\.confluent\.system/(?:.*/)?(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|client_broker_topic/client_bytes_in/delta|client_broker_topic/client_bytes_out/delta|client_broker_topic/client_records_in/delta|client_broker_topic/client_records_out/delta|cluster_link/active_link_count|cluster_link/consumer_offset_committed_rate|cluster_link/consumer_offset_committed_total|cluster_link/fetch_throttle_time_avg|cluster_link/fetch_throttle_time_max|cluster_link/link_count|cluster_link/linked_leader_epoch_change_rate|cluster_link/linked_leader_epoch_change_total|cluster_link/linked_topic_partition_addition_rate|cluster_link/linked_topic_partition_addition_total|cluster_link/mirror_partition_count|cluster_link/mirror_topic_byte_total|cluster_link/mirror_topic_count|cluster_link/mirror_topic_lag|cluster_link/topic_config_update_rate|cluster_link/topic_config_update_total|cluster_link_fetcher/connection_count|cluster_link_fetcher/failed_reauthentication_rate|cluster_link_fetcher/failed_reauthentication_total|cluster_link_fetcher/incoming_byte_rate|cluster_link_fetcher/incoming_byte_total|cluster_link_fetcher/outgoing_byte_rate|cluster_link_fetcher/outgoing_byte_total|cluster_link_fetcher/reauthentication_latency_avg|cluster_link_fetcher_manager/max_lag|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/preferred_replica_imbalance_count|controller/tenant_partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|fetcher/bytes_per_sec|fetcher_lag/consumer_lag|group_coordinator/partition_load_time_max|log/log_end_offset|log/log_start_offset|log/total_size|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/blocked_on_mirror_source_partition_count|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_mirror_partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_mirror_partitions|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max|tenant/consumer_lag_offsets).*|.*org\.apache\.kafka\.(producer\.connection\.creation\.rate|producer\.node\.request\.latency\.avg|producer\.node\.request\.latency\.max|producer\.produce\.throttle\.time\.avg|producer\.produce\.throttle\.time\.max|producer\.record\.queue\.time\.avg|producer\.record\.queue\.time\.max|producer\.connection\.creation\.total|consumer\.connection\.creation\.rate|consumer\.connection\.creation\.total|consumer\.node\.request\.latency\.avg|consumer\.node\.request\.latency\.max|consumer\.poll\.idle\.ratio\.avg|consumer\.coordinator\.commit\.latency\.avg|consumer\.coordinator\.commit\.latency\.max|consumer\.coordinator\.assigned\.partitions|consumer\.coordinator\.rebalance\.latency\.avg|consumer\.coordinator\.rebalance\.latency\.max|consumer\.coordinator\.rebalance\.latency\.total|consumer\.fetch\.manager\.fetch\.latency\.avg|consumer\.fetch\.manager\.fetch\.latency\.max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,365] INFO VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,365] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:%L)
[2025-06-22 05:26:39,365] INFO HttpClientConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = httpTelemetryClient
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,365] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client = _confluentClient
	client.attempts.max = null
	client.base.url = null
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	remote.configurable = true
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,365] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig:%L)
[2025-06-22 05:26:39,366] INFO HttpExporterConfig values: 
	api.key = unused
	api.secret = [hidden]
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = 2000
	buffer.inflight.submissions.max = 10
	buffer.pending.batches.max = 25
	client = 
	client.attempts.max = null
	client.base.url = http://localhost:9090/api/v1/otlp
	client.compression = gzip
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = (io.confluent.kafka.server.request.(?!.*delta).*|io.confluent.kafka.server.server.broker.state|io.confluent.kafka.server.replica.manager.leader.count|io.confluent.kafka.server.request.queue.size|io.confluent.kafka.server.broker.topic.failed.produce.requests.rate.1_min|io.confluent.kafka.server.tier.archiver.total.lag|io.confluent.kafka.server.request.total.time.ms.p99|io.confluent.kafka.server.broker.topic.failed.fetch.requests.rate.1_min|io.confluent.kafka.server.log.total.size|io.confluent.kafka.server.broker.topic.total.fetch.requests.rate.1_min|io.confluent.kafka.server.partition.caught.up.replicas.count|io.confluent.kafka.server.partition.observer.replicas.count|io.confluent.kafka.server.tier.tasks.num.partitions.in.error|io.confluent.kafka.server.broker.topic.bytes.out.rate.1_min|io.confluent.kafka.server.request.total.time.ms.p95|io.confluent.kafka.server.controller.active.controller.count|io.confluent.kafka.server.session.expire.listener.zookeeper.disconnects.total|io.confluent.kafka.server.request.total.time.ms.p999|io.confluent.kafka.server.controller.active.broker.count|io.confluent.kafka.server.request.handler.pool.request.handler.avg.idle.percent.rate.1_min|io.confluent.kafka.server.session.expire.listener.zookeeper.disconnects.rate.1_min|io.confluent.kafka.server.controller.unclean.leader.elections.rate.1_min|io.confluent.kafka.server.replica.manager.partition.count|io.confluent.kafka.server.controller.unclean.leader.elections.total|io.confluent.kafka.server.partition.replicas.count|io.confluent.kafka.server.broker.topic.total.produce.requests.rate.1_min|io.confluent.kafka.server.controller.offline.partitions.count|io.confluent.kafka.server.socket.server.network.processor.avg.idle.percent|io.confluent.kafka.server.partition.under.replicated|io.confluent.kafka.server.log.log.start.offset|io.confluent.kafka.server.log.tier.size|io.confluent.kafka.server.log.size|io.confluent.kafka.server.tier.fetcher.bytes.fetched.total|io.confluent.kafka.server.request.total.time.ms.p50|io.confluent.kafka.server.tenant.consumer.lag.offsets|io.confluent.kafka.server.session.expire.listener.zookeeper.expires.rate.1_min|io.confluent.kafka.server.log.log.end.offset|io.confluent.kafka.server.log.num.log.segments|io.confluent.kafka.server.broker.topic.bytes.in.rate.1_min|io.confluent.kafka.server.partition.under.min.isr|io.confluent.kafka.server.partition.in.sync.replicas.count|io.confluent.telemetry.http.exporter.batches.dropped|io.confluent.telemetry.http.exporter.items.total|io.confluent.telemetry.http.exporter.items.succeeded|io.confluent.telemetry.http.exporter.send.time.total.millis|io.confluent.kafka.server.controller.leader.election.rate.(?!.*delta).*|io.confluent.telemetry.http.exporter.batches.failed)
	proxy.password = null
	proxy.url = null
	proxy.username = null
	remote.configurable = true
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,366] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager:%L)
[2025-06-22 05:26:39,366] INFO KafkaExporterConfig values: 
	client = 
	enabled = true
	events.enabled = true
	metrics.enabled = true
	metrics.include = (io\.confluent\.kafka\.server/fetch/broker_quota|io\.confluent\.kafka\.server/produce/broker_quota|io\.confluent\.kafka\.server/broker_load/broker_load_percent|io\.confluent\.kafka\.server/broker_topic/bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/fetch_from_follower_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/messages_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_follower_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/mirror_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_from_follower_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_produce_requests/rate/1_min|io\.confluent\.kafka\.server/log/size|io\.confluent\.kafka\.server/request/requests/rate/1_min|io\.confluent\.kafka\.server/request_handler_pool/request_handler_avg_idle_percent/rate/1_min|io\.confluent\.kafka\.server/server/linux_system_cpu_utilization_1m|io\.confluent\.kafka\.server/replica_manager/partition_count|io\.confluent\.system/volume/disk_total_bytes)
	producer.bootstrap.servers = localhost:29092
	remote.configurable = false
	topic.create = true
	topic.max.message.bytes = 10485760
	topic.name = _confluent-telemetry-metrics
	topic.partitions = 12
	topic.replicas = 1
	topic.retention.bytes = -1
	topic.retention.ms = 259200000
	topic.roll.ms = 14400000
	type = kafka
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,366] INFO PollingRemoteConfigurationConfig values: 
	enabled = true
	refresh.interval.ms = 60000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,366] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter:%L)
[2025-06-22 05:26:39,367] INFO EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.telemetry.events.exporter.http.EventHttpExporter
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,367] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = true
	events.enabled = true
	filtering.enabled = false
	filtering.routes.allowed = []
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,368] INFO [ClusterLinkTaskManager brokerId=1] Creating local admin client for task manager 0 (kafka.server.link.ClusterLinkTaskManager:%L)
[2025-06-22 05:26:39,368] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = cluster-link--local-admin-1
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,372] INFO These configurations '[confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, cluster.link.metadata.topic.replication.factor, confluent.license.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,372] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,372] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,372] INFO Kafka startTimeMs: 1750569999372 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,372] INFO [ClusterLinkRegionalMetadata-broker-1] Set local network type to persisted value NOT_SET (kafka.server.link.ClusterLinkRegionalMetadata:%L)
[2025-06-22 05:26:39,372] INFO [ClusterLinkManager-broker-1] ClusterLinkManager has started up. (kafka.server.link.ClusterLinkManager:%L)
[2025-06-22 05:26:39,372] INFO [GroupCoordinator id=1] Starting up. (org.apache.kafka.coordinator.group.GroupCoordinatorService:%L)
[2025-06-22 05:26:39,372] INFO [GroupCoordinator id=1] Startup complete. (org.apache.kafka.coordinator.group.GroupCoordinatorService:%L)
[2025-06-22 05:26:39,372] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:%L)
[2025-06-22 05:26:39,372] INFO [AdminClient clientId=cluster-link--local-admin-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:26:39,373] WARN [AdminClient clientId=cluster-link--local-admin-1] Connection to node -1 (localhost/127.0.0.1:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:26:39,373] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:%L)
[2025-06-22 05:26:39,373] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:%L)
[2025-06-22 05:26:39,373] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=1) with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:39,373] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,373] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ClusterLinkCoordinatorListener with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:39,374] INFO KafkaConfig values: 
	add.partitions.to.txn.retry.backoff.max.ms = 100
	add.partitions.to.txn.retry.backoff.ms = 20
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = f4iDkhvfTGqMLnNPxFcjWw
	client.quota.callback.class = null
	client.quota.max.throttle.time.in.response.ms = 60000
	client.quota.max.throttle.time.ms = 5000
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	confluent.accp.enabled = false
	confluent.acks.equal.to.one.request.replication.lag.threshold.ms = -1
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.api.visibility = DEFAULT
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
	confluent.balancer.capacity.threshold.upper.limit = 0.95
	confluent.balancer.cell.load.upper.bound = 0.7
	confluent.balancer.cell.overload.detection.interval.ms = 3600000
	confluent.balancer.cell.overload.duration.ms = 86400000
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.cpu.balance.threshold = 1.1
	confluent.balancer.cpu.goal.act.as.capacity.goal = false
	confluent.balancer.cpu.low.utilization.threshold = 0.2
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.enable.network.capacity.metric.ingestion = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.flex.fanout.network.capacity.metrics.avg.period.ms = 1800000
	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.min.valid.windows = 5
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.inter.cell.balancing.enabled = false
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.minimum.reported.brokers.with.network.capacity.metrics.percentage = 0.8
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.num.concurrent.replica.movements.as.destination.per.broker = 18
	confluent.balancer.num.concurrent.replica.movements.as.source.per.broker = 12
	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.sbc.metrics.parser.enabled = false
	confluent.balancer.self.healing.maximum.rounds = 1
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.tenant.maximum.movements = 0
	confluent.balancer.tenant.suspension.ms = 86400000
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.balancing.itrdg.with.hard.goals.enabled = false
	confluent.balancer.topic.partition.maximum.movements = 3
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.movements.history.limit = 900
	confluent.balancer.topic.partition.suspension.ms = 3600000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.balancer.v2.addition.enabled = false
	confluent.balancer.v2.addition.reassignment.cancellations.enabled = false
	confluent.balancer.v2.executor.enabled = false
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.assertion.claim.aud = null
	confluent.bearer.assertion.claim.exp.minutes = null
	confluent.bearer.assertion.claim.iss = null
	confluent.bearer.assertion.claim.jti.include = null
	confluent.bearer.assertion.claim.nbf.include = null
	confluent.bearer.assertion.claim.sub = null
	confluent.bearer.assertion.file = null
	confluent.bearer.assertion.private.key.file = null
	confluent.bearer.assertion.private.key.passphrase = null
	confluent.bearer.assertion.template.file = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.advertised.limit.load = 0.8
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.tenant.metric.enable = false
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.calling.resource.identity.type.map = 
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.full.configs.enable = false
	confluent.catalog.collector.max.bytes.per.snapshot = 850000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.multitenant.topics.enable = true
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cell.metrics.refresh.period.ms = 60000
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.k2.base.broker.index = -1
	confluent.cells.load.refresher.enable = true
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.client.topic.max.metrics.count = 1000
	confluent.client.topic.metrics.expiry.sec = 3600
	confluent.client.topic.metrics.manager = class org.apache.kafka.server.metrics.ClientTopicMetricsManager$NoOpClientTopicMetricsManager
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.list.object.thread_pool.size = 1
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.admin.max.in.flight.requests = 1000
	confluent.cluster.link.admin.request.batch.size = 1
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.allow.legacy.message.format = false
	confluent.cluster.link.allow.truncation.below.hwm = false
	confluent.cluster.link.availability.check.mode = ALL
	confluent.cluster.link.background.thread.affinity = LINK
	confluent.cluster.link.bootstrap.translation.feature.enable = true
	confluent.cluster.link.clients.max.idle.ms = 3153600000000
	confluent.cluster.link.enable = true
	confluent.cluster.link.enable.local.admin = false
	confluent.cluster.link.enable.metrics.reduction = false
	confluent.cluster.link.enable.metrics.reduction.advanced = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.fetcher.auto.tune.enable = false
	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
	confluent.cluster.link.insync.fetch.response.min.bytes = 1
	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.intranet.connectivity.denied.org.ids = []
	confluent.cluster.link.intranet.connectivity.enable = false
	confluent.cluster.link.intranet.connectivity.migration.enable = false
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.local.admin.multitenant.enable = false
	confluent.cluster.link.local.reverse.connection.listener.map = null
	confluent.cluster.link.max.client.connections = 2147483647
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.mirror.transition.batch.size = 10
	confluent.cluster.link.num.background.threads = 1
	confluent.cluster.link.periodic.task.batch.size = 2147483647
	confluent.cluster.link.periodic.task.min.interval.ms = 1000
	confluent.cluster.link.persistent.connection.backoff.max.ms = 0
	confluent.cluster.link.replica.fetch.connections.mode = combined
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.cluster.link.request.quota.capacity = 400
	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
	confluent.cluster.link.switchover.disabled.principals = []
	confluent.cluster.link.switchover.enable = false
	confluent.cluster.link.switchover.listeners = []
	confluent.cluster.link.switchover.server.states = []
	confluent.cluster.link.tenant.replication.quota.enable = false
	confluent.cluster.link.tenant.request.quota.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
	confluent.cluster.metadata.snapshot.tier.upload.enable = false
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
	confluent.consumer.fetch.partition.pruning.enable = true
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.dataflow.policy.watch.monitor.ms = 300000
	confluent.default.data.policy.enforcement = true
	confluent.defer.isr.shrink.enable = false
	confluent.describe.topic.partitions.enabled = true
	confluent.disk.io.manager.enable = false
	confluent.disk.throughput.headroom = 10485760
	confluent.disk.throughput.limit = 10485760000
	confluent.disk.throughput.quota.tier.archive = 1048576000
	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.log.ignored.event.types = 
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 1
	confluent.e2e_checksum.protection.enabled = false
	confluent.e2e_checksum.protection.files = [none]
	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
	confluent.elastic.cku.enabled = false
	confluent.elastic.cku.scaletozero.enabled = false
	confluent.eligible.controllers = []
	confluent.enable.broker.reporting.min.usage.mode = true
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fail.unsatisfied.placement.constraints = false
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.flexible.fanout.broker.max.fetch.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.max.produce.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.min.producer.percentage = 10.0
	confluent.flexible.fanout.broker.network.out.bytes.per.second = 6200000
	confluent.flexible.fanout.broker.recompute.interval.ms = 30000
	confluent.flexible.fanout.broker.storage.bytes.per.second = 512000000
	confluent.flexible.fanout.enabled = false
	confluent.flexible.fanout.lazy.evaluation.threshold = 0.5
	confluent.flexible.fanout.mode = TENANT_QUOTA
	confluent.floor.connection.rate.per.ip = -1.0
	confluent.floor.connection.rate.per.tenant = -1.0
	confluent.group.coordinator.dynamic.append.linger.enable = false
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.coordinator.txn.offset.validation.enable = false
	confluent.group.highest.offset.commit.rates.log.count = 10
	confluent.group.highest.offset.commit.rates.log.enable = false
	confluent.group.highest.offset.commit.rates.log.interval.ms = 300000
	confluent.group.metadata.load.threads = 32
	confluent.group.subscription.pattern.log.interval.ms = -1
	confluent.heap.tenured.notify.bytes = 0
	confluent.heap.tenured.notify.enabled = false
	confluent.hot.partition.ratio = 0.8
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.internal.rest.server.ssl.enable = false
	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
	confluent.leader.epoch.checkpoint.checksum.enabled = false
	confluent.listener.protocol = TCP
	confluent.log.cleaner.timestamp.validation.enable = true
	confluent.log.placement.constraints = 
	confluent.max.broker.load = 1.0
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
	confluent.max.connection.rate.per.ip = -1.0
	confluent.max.connection.rate.per.tenant = -1.0
	confluent.max.connection.throttle.ms = null
	confluent.max.segment.ms = 9223372036854775807
	confluent.metadata.active.encryptor = null
	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.required = false
	confluent.metadata.encryptor.secret.file = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.jvm.warmup.ms = 60000
	confluent.metadata.leader.balance.slice.delay.ms = 100
	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
	confluent.metadata.max.leader.balance.changes.per.slice = 1000
	confluent.metadata.reject.when.throttled.enable = false
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.connection.throttle.ms = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 20000
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.mtls.build.client.cert.chain.enable = false
	confluent.mtls.enable = false
	confluent.mtls.listener.name = EXTERNAL
	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
	confluent.mtls.truststore.alter.configs.timeout.ms = 300000
	confluent.mtls.truststore.manager.class.name = null
	confluent.multitenant.authorizer.enable.acl.state = false
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.lkc.id.enable = false
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.external.listener.name = EXTERNAL
	confluent.network.health.manager.externalconnectivitystartup.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.oauth.flat.networking.verification.enable = false
	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
	confluent.offsets.topic.placement.constraints = 
	confluent.omit.network.processor.metric.tag = false
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.ppv2.endpoint.scheme.bootstrap.broker.template.mappings = 
	confluent.ppv2.endpoint.scheme.enable = false
	confluent.ppv2.endpoint.scheme.map.broker.zone.to.gateway.zone = false
	confluent.ppv2.endpoint.scheme.template.variable.cloud = 
	confluent.ppv2.endpoint.scheme.template.variable.domain = 
	confluent.ppv2.endpoint.scheme.template.variable.region = 
	confluent.ppv2.endpoint.scheme.template.variables = 
	confluent.ppv2.endpoint.scheme.templates = 
	confluent.prefer.tier.fetch.ms = -1
	confluent.produce.throttle.pre.check.enable = false
	confluent.produce.throttle.pre.check.for.new.connection.enable = false
	confluent.producer.id.cache.broker.hard.limit = -1
	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
	confluent.producer.id.cache.extra.eviction.percentage = 0
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.cache.partition.hard.limit = -1
	confluent.producer.id.cache.tenant.hard.limit = -1
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.quota.window.num = 11
	confluent.producer.id.quota.window.size.seconds = 1
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.mode.local.default = false
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.computing.usage.adjustment = 0.5
	confluent.quota.dynamic.adjustment.min.usage = 102400
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.internal.broker.max.consumer.rate = 9223372036854775807
	confluent.quota.tenant.internal.broker.max.producer.rate = 9223372036854775807
	confluent.quota.tenant.internal.throttling.enable = false
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.rack.id.mapping = null
	confluent.regional.metadata.client.class = null
	confluent.regional.resource.manager.client.scheduler.threads = 2
	confluent.regional.resource.manager.endpoint = null
	confluent.regional.resource.manager.watch.endpoint = null
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.replication.mode = PULL
	confluent.replication.push.feature.enable = false
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = true
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.calling.resource.identity = false
	confluent.require.compatible.keystore.updates = true
	confluent.require.confluent.issuer = false
	confluent.roll.check.interval.ms = 300000
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validation.context.name.enable = false
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.bc.approved.mode.enable = false
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.config = 
	confluent.security.revoked.certificate.ids = 
	confluent.segment.eager.roll.enable = false
	confluent.segment.speculative.prefetch.enable = false
	confluent.share.metadata.load.threads = 32
	confluent.spiffe.id.principal.extraction.rules = 
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.step.connection.rate.per.ip = -1.0
	confluent.step.connection.rate.per.tenant = -1.0
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.subdomain.prefix = null
	confluent.subdomain.separator.map = null
	confluent.subdomain.separator.variable = %sep
	confluent.system.time.roll.enable = false
	confluent.telemetry.enabled = false
	confluent.telemetry.external.client.metrics.delta.temporality = true
	confluent.telemetry.external.client.metrics.instance.cache.size = 16384
	confluent.telemetry.external.client.metrics.push.enabled = false
	confluent.telemetry.external.client.metrics.subscription.interval.ms.list = null
	confluent.telemetry.external.client.metrics.subscription.match.list = null
	confluent.telemetry.external.client.metrics.subscription.metrics.list = null
	confluent.tenant.latency.metric.enabled = false
	confluent.tenantaware.encryption.key.manager.enable = false
	confluent.tenantaware.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.tenantaware.encryption.key.manager.tenant.cache.eviction.time.sec = 172800
	confluent.tenantaware.encryption.key.manager.tenant.cache.size = 100
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.async.enable = false
	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 1
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.prefetch.cache.enable = false
	confluent.tier.prefetch.cache.entry.size.bytes = 1048576
	confluent.tier.prefetch.cache.range.bytes = 5242880
	confluent.tier.prefetch.cache.total.size.bytes = 209715200
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.ipv6.enabled = true
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.security.providers = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.provider = null
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.storage.class.override = 
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.s3.v2.enabled = false
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
	confluent.tier.topic.data.loss.validation.fencing.enable = false
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.head.data.loss.validation.enable = true
	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
	confluent.tier.topic.materialization.from.snapshot.enable = false
	confluent.tier.topic.producer.enable.idempotence = true
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.metadata.throttle.pre.check.partition.count.threshold = 1000
	confluent.topic.partition.default.placement = 2
	confluent.topic.policy.use.computed.assignments = false
	confluent.topic.replica.assignor.builder.class = 
	confluent.track.api.key.per.ip = false
	confluent.track.per.ip.max.size = 100000
	confluent.track.tenant.id.per.ip = false
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.traffic.network.type = 
	confluent.transaction.2pc.timeout.ms = -1
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.unique.deprecated.request.metrics.per.tenant = 1000
	confluent.valid.broker.rack.set = null
	confluent.valid.sni.hostnames = 
	confluent.valid.sni.hostnames.exclude.suffix = 
	confluent.verify.group.subscription.prefix = false
	confluent.virtual.topic.creation.enabled = false
	confluent.zone.tagged.request.metrics.enable = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	controlled.shutdown.enable = true
	controller.listener.names = CONTROLLER
	controller.performance.always.log.threshold.ms = 2000
	controller.performance.sample.period.ms = 60000
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.consumer.assignors = [uniform, range]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = bidirectional
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 5
	group.coordinator.new.enable = true
	group.coordinator.rebalance.protocols = [classic, consumer]
	group.coordinator.threads = 4
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	group.share.delivery.count.limit = 5
	group.share.enable = false
	group.share.heartbeat.interval.ms = 5000
	group.share.max.groups = 10
	group.share.max.heartbeat.interval.ms = 15000
	group.share.max.record.lock.duration.ms = 60000
	group.share.max.session.timeout.ms = 60000
	group.share.max.size = 200
	group.share.min.heartbeat.interval.ms = 5000
	group.share.min.record.lock.duration.ms = 15000
	group.share.min.session.timeout.ms = 45000
	group.share.partition.max.record.locks = 200
	group.share.persister.class.name = org.apache.kafka.server.share.persister.DefaultStatePersister
	group.share.record.lock.duration.ms = 30000
	group.share.session.timeout.ms = 45000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	k2.stack.builder.class.name = null
	k2.startup.timeout.ms = 60000
	k2.topic.metadata.refresh.ms = 10000
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.cleanup.policy.empty.validation = none
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.timestamp.after.max.ms = 3600000
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.connections.per.tenant = 0
	max.connections.protected.listeners = []
	max.connections.reap.amount = 0
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.authorizer.support.resource.ids = false
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 2
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	otel.exporter.otlp.custom.endpoint = default
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.fetch.max.wait.ms = 500
	remote.list.offsets.request.timeout.ms = 30000
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 2
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	share.coordinator.append.linger.ms = 5
	share.coordinator.load.buffer.size = 5242880
	share.coordinator.snapshot.update.records.per.snapshot = 500
	share.coordinator.state.topic.compression.codec = 0
	share.coordinator.state.topic.min.isr = 2
	share.coordinator.state.topic.num.partitions = 50
	share.coordinator.state.topic.prune.interval.ms = 300000
	share.coordinator.state.topic.replication.factor = 3
	share.coordinator.state.topic.segment.bytes = 104857600
	share.coordinator.threads = 1
	share.coordinator.write.timeout.ms = 5000
	share.fetch.max.fetch.records = 2147483647
	share.fetch.purgatory.purge.interval.requests = 1000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	throughput.quota.window.num = 11
	token.impersonation.validation = true
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.metadata.load.threads = 32
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unclean.leader.election.interval.ms = 300000
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,375] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,375] INFO [ControllerServer id=1] The request from broker 1 to unfence has been granted because it has caught up with the offset of its register broker record 5. (org.apache.kafka.controller.BrokerHeartbeatManager:%L)
[2025-06-22 05:26:39,375] INFO [ControllerServer id=1] Replayed BrokerRegistrationChangeRecord modifying the registration for broker 1: BrokerRegistrationChangeRecord(brokerId=1, brokerEpoch=5, fenced=-1, inControlledShutdown=0, degradedComponents=null, logDirs=[]) (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,387] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = true
	events.enabled = true
	filtering.enabled = false
	filtering.routes.allowed = []
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,395] INFO Creating kafka exporter named '_local' (io.confluent.telemetry.reporter.TelemetryReporter:%L)
[2025-06-22 05:26:39,395] INFO Kafka Exporter _local getting producer client  (io.confluent.telemetry.exporter.kafka.KafkaExporter:%L)
[2025-06-22 05:26:39,395] INFO Creating new non-static producer client (io.confluent.telemetry.exporter.kafka.KafkaClientFactory:%L)
[2025-06-22 05:26:39,396] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:29092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = zstd
	compression.zstd.level = 3
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = class io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.telemetry.serde.OpenTelemetryMetricsSerde
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,397] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,397] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,397] INFO Kafka startTimeMs: 1750569999397 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,398] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:26:39,398] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Connection to node -1 (localhost/127.0.0.1:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:26:39,398] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Bootstrap broker localhost:29092 (id: -1 rack: null isFenced: false) disconnected (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:26:39,401] INFO SBC Event SbcMetadataUpdateEvent-8 generated 1 more events to enqueue in the following order - [SbcKraftBrokerAdditionEvent-9]. Enqueuing... (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,401] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO Balancer notified of a config change: ConfigurationsDelta(changes={}) (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO Configs metadata not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO Handling event SbcKraftBrokerAdditionEvent-9 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO Topics Image not present, pausing broker addition event of brokers (new brokers: [1]) until it is received. (io.confluent.databalancer.event.SbcKraftBrokerAdditionEvent:%L)
[2025-06-22 05:26:39,402] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager:%L)
[2025-06-22 05:26:39,402] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,402] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer:%L)
[2025-06-22 05:26:39,402] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor:%L)
[2025-06-22 05:26:39,403] INFO Awaiting socket connections on localhost:29092. (kafka.network.DataPlaneAcceptor:%L)
[2025-06-22 05:26:39,403] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,403] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,403] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,403] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,405] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter:%L)
[2025-06-22 05:26:39,405] INFO KafkaConfig values: 
	add.partitions.to.txn.retry.backoff.max.ms = 100
	add.partitions.to.txn.retry.backoff.ms = 20
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = f4iDkhvfTGqMLnNPxFcjWw
	client.quota.callback.class = null
	client.quota.max.throttle.time.in.response.ms = 60000
	client.quota.max.throttle.time.ms = 5000
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	confluent.accp.enabled = false
	confluent.acks.equal.to.one.request.replication.lag.threshold.ms = -1
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.api.visibility = DEFAULT
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
	confluent.balancer.capacity.threshold.upper.limit = 0.95
	confluent.balancer.cell.load.upper.bound = 0.7
	confluent.balancer.cell.overload.detection.interval.ms = 3600000
	confluent.balancer.cell.overload.duration.ms = 86400000
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.cpu.balance.threshold = 1.1
	confluent.balancer.cpu.goal.act.as.capacity.goal = false
	confluent.balancer.cpu.low.utilization.threshold = 0.2
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.enable.network.capacity.metric.ingestion = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.flex.fanout.network.capacity.metrics.avg.period.ms = 1800000
	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.min.valid.windows = 5
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.inter.cell.balancing.enabled = false
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.minimum.reported.brokers.with.network.capacity.metrics.percentage = 0.8
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.num.concurrent.replica.movements.as.destination.per.broker = 18
	confluent.balancer.num.concurrent.replica.movements.as.source.per.broker = 12
	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.sbc.metrics.parser.enabled = false
	confluent.balancer.self.healing.maximum.rounds = 1
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.tenant.maximum.movements = 0
	confluent.balancer.tenant.suspension.ms = 86400000
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.balancing.itrdg.with.hard.goals.enabled = false
	confluent.balancer.topic.partition.maximum.movements = 3
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.movements.history.limit = 900
	confluent.balancer.topic.partition.suspension.ms = 3600000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.balancer.v2.addition.enabled = false
	confluent.balancer.v2.addition.reassignment.cancellations.enabled = false
	confluent.balancer.v2.executor.enabled = false
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.assertion.claim.aud = null
	confluent.bearer.assertion.claim.exp.minutes = null
	confluent.bearer.assertion.claim.iss = null
	confluent.bearer.assertion.claim.jti.include = null
	confluent.bearer.assertion.claim.nbf.include = null
	confluent.bearer.assertion.claim.sub = null
	confluent.bearer.assertion.file = null
	confluent.bearer.assertion.private.key.file = null
	confluent.bearer.assertion.private.key.passphrase = null
	confluent.bearer.assertion.template.file = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.advertised.limit.load = 0.8
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.tenant.metric.enable = false
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.calling.resource.identity.type.map = 
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.full.configs.enable = false
	confluent.catalog.collector.max.bytes.per.snapshot = 850000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.multitenant.topics.enable = true
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cell.metrics.refresh.period.ms = 60000
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.k2.base.broker.index = -1
	confluent.cells.load.refresher.enable = true
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.client.topic.max.metrics.count = 1000
	confluent.client.topic.metrics.expiry.sec = 3600
	confluent.client.topic.metrics.manager = class org.apache.kafka.server.metrics.ClientTopicMetricsManager$NoOpClientTopicMetricsManager
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.list.object.thread_pool.size = 1
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.admin.max.in.flight.requests = 1000
	confluent.cluster.link.admin.request.batch.size = 1
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.allow.legacy.message.format = false
	confluent.cluster.link.allow.truncation.below.hwm = false
	confluent.cluster.link.availability.check.mode = ALL
	confluent.cluster.link.background.thread.affinity = LINK
	confluent.cluster.link.bootstrap.translation.feature.enable = true
	confluent.cluster.link.clients.max.idle.ms = 3153600000000
	confluent.cluster.link.enable = true
	confluent.cluster.link.enable.local.admin = false
	confluent.cluster.link.enable.metrics.reduction = false
	confluent.cluster.link.enable.metrics.reduction.advanced = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.fetcher.auto.tune.enable = false
	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
	confluent.cluster.link.insync.fetch.response.min.bytes = 1
	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.intranet.connectivity.denied.org.ids = []
	confluent.cluster.link.intranet.connectivity.enable = false
	confluent.cluster.link.intranet.connectivity.migration.enable = false
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.local.admin.multitenant.enable = false
	confluent.cluster.link.local.reverse.connection.listener.map = null
	confluent.cluster.link.max.client.connections = 2147483647
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.mirror.transition.batch.size = 10
	confluent.cluster.link.num.background.threads = 1
	confluent.cluster.link.periodic.task.batch.size = 2147483647
	confluent.cluster.link.periodic.task.min.interval.ms = 1000
	confluent.cluster.link.persistent.connection.backoff.max.ms = 0
	confluent.cluster.link.replica.fetch.connections.mode = combined
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.cluster.link.request.quota.capacity = 400
	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
	confluent.cluster.link.switchover.disabled.principals = []
	confluent.cluster.link.switchover.enable = false
	confluent.cluster.link.switchover.listeners = []
	confluent.cluster.link.switchover.server.states = []
	confluent.cluster.link.tenant.replication.quota.enable = false
	confluent.cluster.link.tenant.request.quota.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
	confluent.cluster.metadata.snapshot.tier.upload.enable = false
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
	confluent.consumer.fetch.partition.pruning.enable = true
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.dataflow.policy.watch.monitor.ms = 300000
	confluent.default.data.policy.enforcement = true
	confluent.defer.isr.shrink.enable = false
	confluent.describe.topic.partitions.enabled = true
	confluent.disk.io.manager.enable = false
	confluent.disk.throughput.headroom = 10485760
	confluent.disk.throughput.limit = 10485760000
	confluent.disk.throughput.quota.tier.archive = 1048576000
	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.log.ignored.event.types = 
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 1
	confluent.e2e_checksum.protection.enabled = false
	confluent.e2e_checksum.protection.files = [none]
	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
	confluent.elastic.cku.enabled = false
	confluent.elastic.cku.scaletozero.enabled = false
	confluent.eligible.controllers = []
	confluent.enable.broker.reporting.min.usage.mode = true
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fail.unsatisfied.placement.constraints = false
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.flexible.fanout.broker.max.fetch.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.max.produce.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.min.producer.percentage = 10.0
	confluent.flexible.fanout.broker.network.out.bytes.per.second = 6200000
	confluent.flexible.fanout.broker.recompute.interval.ms = 30000
	confluent.flexible.fanout.broker.storage.bytes.per.second = 512000000
	confluent.flexible.fanout.enabled = false
	confluent.flexible.fanout.lazy.evaluation.threshold = 0.5
	confluent.flexible.fanout.mode = TENANT_QUOTA
	confluent.floor.connection.rate.per.ip = -1.0
	confluent.floor.connection.rate.per.tenant = -1.0
	confluent.group.coordinator.dynamic.append.linger.enable = false
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.coordinator.txn.offset.validation.enable = false
	confluent.group.highest.offset.commit.rates.log.count = 10
	confluent.group.highest.offset.commit.rates.log.enable = false
	confluent.group.highest.offset.commit.rates.log.interval.ms = 300000
	confluent.group.metadata.load.threads = 32
	confluent.group.subscription.pattern.log.interval.ms = -1
	confluent.heap.tenured.notify.bytes = 0
	confluent.heap.tenured.notify.enabled = false
	confluent.hot.partition.ratio = 0.8
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.internal.rest.server.ssl.enable = false
	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
	confluent.leader.epoch.checkpoint.checksum.enabled = false
	confluent.listener.protocol = TCP
	confluent.log.cleaner.timestamp.validation.enable = true
	confluent.log.placement.constraints = 
	confluent.max.broker.load = 1.0
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
	confluent.max.connection.rate.per.ip = -1.0
	confluent.max.connection.rate.per.tenant = -1.0
	confluent.max.connection.throttle.ms = null
	confluent.max.segment.ms = 9223372036854775807
	confluent.metadata.active.encryptor = null
	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.required = false
	confluent.metadata.encryptor.secret.file = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.jvm.warmup.ms = 60000
	confluent.metadata.leader.balance.slice.delay.ms = 100
	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
	confluent.metadata.max.leader.balance.changes.per.slice = 1000
	confluent.metadata.reject.when.throttled.enable = false
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.connection.throttle.ms = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 20000
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.mtls.build.client.cert.chain.enable = false
	confluent.mtls.enable = false
	confluent.mtls.listener.name = EXTERNAL
	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
	confluent.mtls.truststore.alter.configs.timeout.ms = 300000
	confluent.mtls.truststore.manager.class.name = null
	confluent.multitenant.authorizer.enable.acl.state = false
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.lkc.id.enable = false
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.external.listener.name = EXTERNAL
	confluent.network.health.manager.externalconnectivitystartup.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.oauth.flat.networking.verification.enable = false
	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
	confluent.offsets.topic.placement.constraints = 
	confluent.omit.network.processor.metric.tag = false
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.ppv2.endpoint.scheme.bootstrap.broker.template.mappings = 
	confluent.ppv2.endpoint.scheme.enable = false
	confluent.ppv2.endpoint.scheme.map.broker.zone.to.gateway.zone = false
	confluent.ppv2.endpoint.scheme.template.variable.cloud = 
	confluent.ppv2.endpoint.scheme.template.variable.domain = 
	confluent.ppv2.endpoint.scheme.template.variable.region = 
	confluent.ppv2.endpoint.scheme.template.variables = 
	confluent.ppv2.endpoint.scheme.templates = 
	confluent.prefer.tier.fetch.ms = -1
	confluent.produce.throttle.pre.check.enable = false
	confluent.produce.throttle.pre.check.for.new.connection.enable = false
	confluent.producer.id.cache.broker.hard.limit = -1
	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
	confluent.producer.id.cache.extra.eviction.percentage = 0
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.cache.partition.hard.limit = -1
	confluent.producer.id.cache.tenant.hard.limit = -1
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.quota.window.num = 11
	confluent.producer.id.quota.window.size.seconds = 1
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.mode.local.default = false
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.computing.usage.adjustment = 0.5
	confluent.quota.dynamic.adjustment.min.usage = 102400
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.internal.broker.max.consumer.rate = 9223372036854775807
	confluent.quota.tenant.internal.broker.max.producer.rate = 9223372036854775807
	confluent.quota.tenant.internal.throttling.enable = false
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.rack.id.mapping = null
	confluent.regional.metadata.client.class = null
	confluent.regional.resource.manager.client.scheduler.threads = 2
	confluent.regional.resource.manager.endpoint = null
	confluent.regional.resource.manager.watch.endpoint = null
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.replication.mode = PULL
	confluent.replication.push.feature.enable = false
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = true
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.calling.resource.identity = false
	confluent.require.compatible.keystore.updates = true
	confluent.require.confluent.issuer = false
	confluent.roll.check.interval.ms = 300000
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validation.context.name.enable = false
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.bc.approved.mode.enable = false
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.config = 
	confluent.security.revoked.certificate.ids = 
	confluent.segment.eager.roll.enable = false
	confluent.segment.speculative.prefetch.enable = false
	confluent.share.metadata.load.threads = 32
	confluent.spiffe.id.principal.extraction.rules = 
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.step.connection.rate.per.ip = -1.0
	confluent.step.connection.rate.per.tenant = -1.0
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.subdomain.prefix = null
	confluent.subdomain.separator.map = null
	confluent.subdomain.separator.variable = %sep
	confluent.system.time.roll.enable = false
	confluent.telemetry.enabled = false
	confluent.telemetry.external.client.metrics.delta.temporality = true
	confluent.telemetry.external.client.metrics.instance.cache.size = 16384
	confluent.telemetry.external.client.metrics.push.enabled = false
	confluent.telemetry.external.client.metrics.subscription.interval.ms.list = null
	confluent.telemetry.external.client.metrics.subscription.match.list = null
	confluent.telemetry.external.client.metrics.subscription.metrics.list = null
	confluent.tenant.latency.metric.enabled = false
	confluent.tenantaware.encryption.key.manager.enable = false
	confluent.tenantaware.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.tenantaware.encryption.key.manager.tenant.cache.eviction.time.sec = 172800
	confluent.tenantaware.encryption.key.manager.tenant.cache.size = 100
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.async.enable = false
	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 1
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.prefetch.cache.enable = false
	confluent.tier.prefetch.cache.entry.size.bytes = 1048576
	confluent.tier.prefetch.cache.range.bytes = 5242880
	confluent.tier.prefetch.cache.total.size.bytes = 209715200
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.ipv6.enabled = true
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.security.providers = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.provider = null
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.storage.class.override = 
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.s3.v2.enabled = false
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
	confluent.tier.topic.data.loss.validation.fencing.enable = false
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.head.data.loss.validation.enable = true
	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
	confluent.tier.topic.materialization.from.snapshot.enable = false
	confluent.tier.topic.producer.enable.idempotence = true
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.metadata.throttle.pre.check.partition.count.threshold = 1000
	confluent.topic.partition.default.placement = 2
	confluent.topic.policy.use.computed.assignments = false
	confluent.topic.replica.assignor.builder.class = 
	confluent.track.api.key.per.ip = false
	confluent.track.per.ip.max.size = 100000
	confluent.track.tenant.id.per.ip = false
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.traffic.network.type = 
	confluent.transaction.2pc.timeout.ms = -1
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.unique.deprecated.request.metrics.per.tenant = 1000
	confluent.valid.broker.rack.set = null
	confluent.valid.sni.hostnames = 
	confluent.valid.sni.hostnames.exclude.suffix = 
	confluent.verify.group.subscription.prefix = false
	confluent.virtual.topic.creation.enabled = false
	confluent.zone.tagged.request.metrics.enable = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	controlled.shutdown.enable = true
	controller.listener.names = CONTROLLER
	controller.performance.always.log.threshold.ms = 2000
	controller.performance.sample.period.ms = 60000
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.consumer.assignors = [uniform, range]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = bidirectional
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 5
	group.coordinator.new.enable = true
	group.coordinator.rebalance.protocols = [classic, consumer]
	group.coordinator.threads = 4
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	group.share.delivery.count.limit = 5
	group.share.enable = false
	group.share.heartbeat.interval.ms = 5000
	group.share.max.groups = 10
	group.share.max.heartbeat.interval.ms = 15000
	group.share.max.record.lock.duration.ms = 60000
	group.share.max.session.timeout.ms = 60000
	group.share.max.size = 200
	group.share.min.heartbeat.interval.ms = 5000
	group.share.min.record.lock.duration.ms = 15000
	group.share.min.session.timeout.ms = 45000
	group.share.partition.max.record.locks = 200
	group.share.persister.class.name = org.apache.kafka.server.share.persister.DefaultStatePersister
	group.share.record.lock.duration.ms = 30000
	group.share.session.timeout.ms = 45000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	k2.stack.builder.class.name = null
	k2.startup.timeout.ms = 60000
	k2.topic.metadata.refresh.ms = 10000
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.cleanup.policy.empty.validation = none
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.timestamp.after.max.ms = 3600000
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.connections.per.tenant = 0
	max.connections.protected.listeners = []
	max.connections.reap.amount = 0
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.authorizer.support.resource.ids = false
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 2
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	otel.exporter.otlp.custom.endpoint = default
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.fetch.max.wait.ms = 500
	remote.list.offsets.request.timeout.ms = 30000
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 2
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	share.coordinator.append.linger.ms = 5
	share.coordinator.load.buffer.size = 5242880
	share.coordinator.snapshot.update.records.per.snapshot = 500
	share.coordinator.state.topic.compression.codec = 0
	share.coordinator.state.topic.min.isr = 2
	share.coordinator.state.topic.num.partitions = 50
	share.coordinator.state.topic.prune.interval.ms = 300000
	share.coordinator.state.topic.replication.factor = 3
	share.coordinator.state.topic.segment.bytes = 104857600
	share.coordinator.threads = 1
	share.coordinator.write.timeout.ms = 5000
	share.fetch.max.fetch.records = 2147483647
	share.fetch.purgatory.purge.interval.requests = 1000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	throughput.quota.window.num = 11
	token.impersonation.validation = true
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.metadata.load.threads = 32
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unclean.leader.election.interval.ms = 300000
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,407] INFO Unexpected credentials store injected: null (io.confluent.kafkarest.servlet.KafkaRestApplicationProvider:%L)
[2025-06-22 05:26:39,407] INFO For rest-app with listener null, configuring custom request logging (io.confluent.kafkarest.KafkaRestApplication:%L)
[2025-06-22 05:26:39,410] INFO EventEmitterConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,410] INFO EventEmitterConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,410] INFO Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,411] INFO ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.cluster.id = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.external.client.metrics.exclude.labels = 
	confluent.telemetry.metrics.collector.include = .*io\.confluent\.system/(?:.*/)?(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO HttpClientConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = httpTelemetryClient
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client = _confluentClient
	client.attempts.max = null
	client.base.url = null
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	remote.configurable = true
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig:%L)
[2025-06-22 05:26:39,411] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,411] INFO PollingRemoteConfigurationConfig values: 
	enabled = true
	refresh.interval.ms = 60000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] WARN Ignoring redefinition of existing telemetry label kafka_rest.version (io.confluent.telemetry.ResourceBuilderFacade:%L)
[2025-06-22 05:26:39,411] INFO Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,411] INFO ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.cluster.id = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.external.client.metrics.exclude.labels = 
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io\.confluent\.system/(?:.*/)?(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO HttpClientConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = httpTelemetryClient
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client = _confluentClient
	client.attempts.max = null
	client.base.url = null
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	remote.configurable = true
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig:%L)
[2025-06-22 05:26:39,411] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,411] INFO PollingRemoteConfigurationConfig values: 
	enabled = true
	refresh.interval.ms = 60000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter:%L)
[2025-06-22 05:26:39,411] ERROR Unable to submit events without credentials (io.confluent.telemetry.events.exporter.http.HttpExporter:%L)
[2025-06-22 05:26:39,411] INFO EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.telemetry.events.exporter.http.EventHttpExporter
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = true
	events.enabled = true
	filtering.enabled = false
	filtering.routes.allowed = []
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,412] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = true
	events.enabled = true
	filtering.enabled = false
	filtering.routes.allowed = []
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,418] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter:%L)
[2025-06-22 05:26:39,419] ERROR Unable to submit events without credentials (io.confluent.telemetry.events.exporter.http.HttpExporter:%L)
[2025-06-22 05:26:39,419] INFO Application provider 'KafkaRestApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader:%L)
[2025-06-22 05:26:39,420] INFO Application provider 'MetadataApiApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader:%L)
[2025-06-22 05:26:39,420] INFO MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = null
	confluent.metadata.server.enable = false
	confluent.metadata.server.kraft.controller.enabled = false
	confluent.metadata.server.listeners = null
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,420] INFO Application provider 'RbacApplicationProvider' did not provide any instances. (io.confluent.http.server.KafkaHttpApplicationLoader:%L)
[2025-06-22 05:26:39,422] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer:%L)
[2025-06-22 05:26:39,448] INFO Adding listener with HTTP/2: NamedURI{uri=http://0.0.0.0:8090, name='null'} (io.confluent.rest.ApplicationServer:%L)
[2025-06-22 05:26:39,458] INFO DynamicMetricsReporters initiated successfully. (kafka.server.DynamicMetricsReportersScheduler:%L)
[2025-06-22 05:26:39,458] INFO Stopping DynamicMetricsReportersScheduler. (kafka.server.DynamicMetricsReportersScheduler:%L)
[2025-06-22 05:26:39,463] INFO Loaded KafkaHttpServer implementation class io.confluent.http.server.KafkaHttpServerImpl (io.confluent.kafka.http.server.KafkaHttpServerLoader:%L)
[2025-06-22 05:26:39,463] INFO KafkaHttpServer transitioned from NEW to STARTING.. (io.confluent.http.server.KafkaHttpServerImpl:%L)
[2025-06-22 05:26:39,468] INFO Registered CombinedNetworkTrafficListener to network connector null of listener: null (io.confluent.rest.ApplicationServer:%L)
[2025-06-22 05:26:39,474] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig:%L)
[2025-06-22 05:26:39,475] INFO SchemaRegistryConfig values: 
	auto.register.schemas = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.schema.id.deserializer = class io.confluent.kafka.serializers.schema.id.DualSchemaIdDeserializer
	key.schema.id.serializer = class io.confluent.kafka.serializers.schema.id.PrefixSchemaIdSerializer
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.retries = 3
	max.schemas.per.subject = 1000
	normalize.schemas = false
	propagate.schema.tags = false
	proxy.host = 
	proxy.port = -1
	retries.max.wait.ms = 20000
	retries.wait.ms = 1000
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	schema.registry.url.randomize = false
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.schema.id.deserializer = class io.confluent.kafka.serializers.schema.id.DualSchemaIdDeserializer
	value.schema.id.serializer = class io.confluent.kafka.serializers.schema.id.PrefixSchemaIdSerializer
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,479] INFO Binding EmbeddedKafkaRestApplication to all listeners. (io.confluent.rest.Application:%L)
[2025-06-22 05:26:39,479] INFO Registered CombinedNetworkTrafficListener to network connector null of listener: null (io.confluent.rest.ApplicationServer:%L)
[2025-06-22 05:26:39,480] INFO [ControllerServer id=1] Using the default placer, StripedReplicaPlacer, to make the assignment for topic _confluent-link-metadata. (kafka.assignor.ConfluentReplicaPlacer:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=1, assignments=[], configs=[CreatableTopicConfig(name='cleanup.policy', value='compact'), CreatableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-link-metadata with topic ID G-9S9N2rSd2_OyvjHChE3w. (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-link-metadata') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-link-metadata') which set configuration min.insync.replicas to 2 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-0 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-1 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-2 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO Binding MetadataApiApplication to all listeners. (io.confluent.rest.Application:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-3 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-4 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-5 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-6 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-7 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-8 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-9 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-10 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-11 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-12 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-13 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-14 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-15 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-16 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-17 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-18 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-19 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-20 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-21 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-22 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-23 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-24 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-25 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-26 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-27 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-28 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-29 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-30 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-31 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-32 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-33 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-34 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-35 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-36 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-37 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-38 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-39 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-40 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-41 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-42 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-43 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-44 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-45 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-46 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-47 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-48 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-49 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,514] INFO [Broker id=1] Transitioning 50 partition(s) to local leaders. (state.change.logger:%L)
[2025-06-22 05:26:39,514] INFO SBC Event SbcMetadataUpdateEvent-11 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-12]. Enqueuing... (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,514] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,514] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-link-metadata-43, _confluent-link-metadata-10, _confluent-link-metadata-39, _confluent-link-metadata-6, _confluent-link-metadata-18, _confluent-link-metadata-47, _confluent-link-metadata-14, _confluent-link-metadata-27, _confluent-link-metadata-23, _confluent-link-metadata-35, _confluent-link-metadata-2, _confluent-link-metadata-31, _confluent-link-metadata-42, _confluent-link-metadata-13, _confluent-link-metadata-38, _confluent-link-metadata-9, _confluent-link-metadata-21, _confluent-link-metadata-46, _confluent-link-metadata-17, _confluent-link-metadata-26, _confluent-link-metadata-22, _confluent-link-metadata-34, _confluent-link-metadata-5, _confluent-link-metadata-30, _confluent-link-metadata-1, _confluent-link-metadata-45, _confluent-link-metadata-12, _confluent-link-metadata-41, _confluent-link-metadata-8, _confluent-link-metadata-20, _confluent-link-metadata-49, _confluent-link-metadata-16, _confluent-link-metadata-29, _confluent-link-metadata-25, _confluent-link-metadata-37, _confluent-link-metadata-4, _confluent-link-metadata-33, _confluent-link-metadata-0, _confluent-link-metadata-11, _confluent-link-metadata-44, _confluent-link-metadata-7, _confluent-link-metadata-40, _confluent-link-metadata-19, _confluent-link-metadata-15, _confluent-link-metadata-48, _confluent-link-metadata-28, _confluent-link-metadata-24, _confluent-link-metadata-3, _confluent-link-metadata-36, _confluent-link-metadata-32) (kafka.server.ReplicaFetcherManager:%L)
[2025-06-22 05:26:39,514] INFO [Broker id=1] Creating new partition _confluent-link-metadata-43 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,514] INFO Balancer Status state for brokers [1] transitioned from BALANCER_EVENT_RECEIVED to DISABLED due to event BALANCER_DISABLED. (io.confluent.databalancer.operation.StateMachine:%L)
[2025-06-22 05:26:39,514] INFO DataBalancer: Skipping DataBalancer startup. BalancerEnabledConfig: BalancerEnabledConfig{isEnabled=false} (io.confluent.databalancer.KafkaDataBalanceManager:%L)
[2025-06-22 05:26:39,514] INFO Handling event SbcKraftBrokerAdditionEvent-9 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,514] INFO Processing SbcKraftBrokerAdditionEvent-9 event with data: empty_brokers: [], new_brokers: [1] (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,515] WARN Notified of broker additions (empty broker ids [], new brokers [1]) but DataBalancer is disabled -- ignoring for now (io.confluent.databalancer.KafkaDataBalanceManager:%L)
[2025-06-22 05:26:39,515] INFO Handling event SbcConfigUpdateEvent-12 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,515] INFO Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":3,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":1,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":29999,"validateOnly":false},"response":{"responseData":"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","errorCode":0},"connection":"127.0.0.1:29093-127.0.0.1:34176-2-0","clientAddress":"127.0.0.1","totalTimeMs":35.039,"requestQueueTimeMs":0.04,"localTimeMs":0.083,"remoteTimeMs":34.695,"throttleTimeMs":0,"responseQueueTimeMs":0.106,"sendTimeMs":0.114,"sendIoTimeMs":0.074,"responseSize":2544,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":175056999947900002} (kafka.request.logger:%L)
[2025-06-22 05:26:39,515] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-link-metadata')=ConfigurationDelta(changedKeys=[cleanup.policy, min.insync.replicas])}) (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,515] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,515] INFO Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":3,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":1,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":29999,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"G-9S9N2rSd2_OyvjHChE3w","errorCode":0,"errorMessage":null,"numPartitions":50,"replicationFactor":1,"configs":[{"configName":"cleanup.policy","value":"compact","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"compression.gzip.level","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.lz4.level","value":"9","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.type","value":"producer","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.zstd.level","value":"3","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.append.record.interceptor.classes","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.cluster.link.allow.legacy.message.format","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.compacted.topic.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.log.cleaner.timestamp.validation.enable","value":"true","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.max.segment.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.min.segment.ms","value":"1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.placement.constraints","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.schema.validation.context.name.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.segment.speculative.prefetch.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.delete.delay.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.max.deletions.per.run","value":"72","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.system.time.roll.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.min.efficiency","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.segment.min.bytes","value":"20971520","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.dual.compaction","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.min.cleanable.ratio","value":"0.75","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.bytes","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.segment.hotset.roll.min.bytes","value":"104857600","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.topic.type","value":"standard","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"delete.retention.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"file.delete.delay.ms","value":"60000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.messages","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"follower.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"index.interval.bytes","value":"4096","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"leader.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.bytes","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.ms","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.compaction.lag.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.message.bytes","value":"1048588","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.after.max.ms","value":"3600000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.before.max.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.type","value":"CreateTime","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.cleanable.dirty.ratio","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.compaction.lag.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.insync.replicas","value":"2","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"preallocate","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.copy.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.delete.on.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.storage.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"retention.bytes","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"retention.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.bytes","value":"1073741824","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.index.bytes","value":"10485760","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.jitter.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"unclean.leader.election.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false}]}]},"connection":"127.0.0.1:29092-127.0.0.1:53406-1-0","clientAddress":"127.0.0.1","totalTimeMs":37.78,"requestQueueTimeMs":0.03,"localTimeMs":0.801,"remoteTimeMs":36.686,"throttleTimeMs":0,"responseQueueTimeMs":0.088,"sendTimeMs":0.174,"sendIoTimeMs":0.142,"responseSize":2534,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"8.0.0-0-ce"},"isDisconnectedClient":false,"requestId":175056999947700101} (kafka.request.logger:%L)
[2025-06-22 05:26:39,515] INFO [Broker id=1] Creating new partition _confluent-link-metadata-10 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,515] INFO [Broker id=1] Creating new partition _confluent-link-metadata-39 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-6 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-18 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-47 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-14 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-27 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-23 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-35 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-2 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-31 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,517] INFO jetty-12.0.16; built: 2024-12-09T21:02:54.535Z; git: c3f88bafb4e393f23204dc14dc57b042e84debc7; jvm 21.0.2+13-jvmci-23.1-b30 (org.eclipse.jetty.server.Server:%L)
[2025-06-22 05:26:39,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-42 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-13 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,518] INFO [Broker id=1] Creating new partition _confluent-link-metadata-38 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,518] INFO [Broker id=1] Creating new partition _confluent-link-metadata-9 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,519] INFO [Broker id=1] Creating new partition _confluent-link-metadata-21 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,520] INFO [Broker id=1] Creating new partition _confluent-link-metadata-46 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,520] INFO [Broker id=1] Creating new partition _confluent-link-metadata-17 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,520] INFO [Broker id=1] Creating new partition _confluent-link-metadata-26 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,520] INFO [Broker id=1] Creating new partition _confluent-link-metadata-22 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,521] INFO [Broker id=1] Creating new partition _confluent-link-metadata-34 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,521] INFO [Broker id=1] Creating new partition _confluent-link-metadata-5 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,521] INFO [Broker id=1] Creating new partition _confluent-link-metadata-30 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,521] INFO [Broker id=1] Creating new partition _confluent-link-metadata-1 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,522] INFO [Broker id=1] Creating new partition _confluent-link-metadata-45 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,522] INFO [Broker id=1] Creating new partition _confluent-link-metadata-12 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,522] INFO [Broker id=1] Creating new partition _confluent-link-metadata-41 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-8 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-20 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-49 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-16 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-29 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-25 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-37 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-4 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-33 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-0 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-11 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,525] INFO Session workerName=node0 (org.eclipse.jetty.session.DefaultSessionIdManager:%L)
[2025-06-22 05:26:39,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-44 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-7 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-40 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-19 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,525] INFO Started oeje10s.ServletContextHandler@6d711502{/kafka,/kafka,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@2a49570f{STARTED}} (org.eclipse.jetty.server.handler.ContextHandler:%L)
[2025-06-22 05:26:39,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-15 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-48 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-28 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-24 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-3 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-36 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,527] INFO [Broker id=1] Creating new partition _confluent-link-metadata-32 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,527] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 50 partitions (state.change.logger:%L)
[2025-06-22 05:26:39,530] INFO [MergedLog partition=_confluent-link-metadata-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,531] INFO Created log for partition _confluent-link-metadata-25 in /var/lib/kafka/data/_confluent-link-metadata-25 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,531] INFO [Partition _confluent-link-metadata-25 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-25 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,531] INFO [Partition _confluent-link-metadata-25 broker=1] Log loaded for partition _confluent-link-metadata-25 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,531] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-25 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,531] INFO [MergedLog partition=_confluent-link-metadata-25, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,532] INFO [Broker id=1] Leader _confluent-link-metadata-25 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,532] INFO [MergedLog partition=_confluent-link-metadata-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,532] INFO Created log for partition _confluent-link-metadata-21 in /var/lib/kafka/data/_confluent-link-metadata-21 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,532] INFO [Partition _confluent-link-metadata-21 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-21 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,532] INFO [Partition _confluent-link-metadata-21 broker=1] Log loaded for partition _confluent-link-metadata-21 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,532] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-21 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,532] INFO [MergedLog partition=_confluent-link-metadata-21, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,532] INFO [Broker id=1] Leader _confluent-link-metadata-21 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,534] INFO [MergedLog partition=_confluent-link-metadata-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,534] INFO Created log for partition _confluent-link-metadata-24 in /var/lib/kafka/data/_confluent-link-metadata-24 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,534] INFO [Partition _confluent-link-metadata-24 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-24 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,534] INFO [Partition _confluent-link-metadata-24 broker=1] Log loaded for partition _confluent-link-metadata-24 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,534] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-24 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,534] INFO [MergedLog partition=_confluent-link-metadata-24, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,535] INFO [Broker id=1] Leader _confluent-link-metadata-24 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,535] INFO [MergedLog partition=_confluent-link-metadata-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,536] INFO Created log for partition _confluent-link-metadata-23 in /var/lib/kafka/data/_confluent-link-metadata-23 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,536] INFO [Partition _confluent-link-metadata-23 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-23 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,536] INFO [Partition _confluent-link-metadata-23 broker=1] Log loaded for partition _confluent-link-metadata-23 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,536] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-23 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,536] INFO [MergedLog partition=_confluent-link-metadata-23, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,536] INFO [Broker id=1] Leader _confluent-link-metadata-23 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,536] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig:%L)
[2025-06-22 05:26:39,536] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig:%L)
[2025-06-22 05:26:39,537] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig:%L)
[2025-06-22 05:26:39,538] INFO [MergedLog partition=_confluent-link-metadata-41, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,538] INFO Created log for partition _confluent-link-metadata-41 in /var/lib/kafka/data/_confluent-link-metadata-41 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,538] INFO [Partition _confluent-link-metadata-41 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-41 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,538] INFO [Partition _confluent-link-metadata-41 broker=1] Log loaded for partition _confluent-link-metadata-41 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,538] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-41 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,538] INFO [MergedLog partition=_confluent-link-metadata-41, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,538] INFO [Broker id=1] Leader _confluent-link-metadata-41 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,538] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig:%L)
[2025-06-22 05:26:39,540] INFO [MergedLog partition=_confluent-link-metadata-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,540] INFO Created log for partition _confluent-link-metadata-20 in /var/lib/kafka/data/_confluent-link-metadata-20 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,540] INFO [Partition _confluent-link-metadata-20 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-20 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,540] INFO [Partition _confluent-link-metadata-20 broker=1] Log loaded for partition _confluent-link-metadata-20 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,540] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-20 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,541] INFO [MergedLog partition=_confluent-link-metadata-20, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,541] INFO [Broker id=1] Leader _confluent-link-metadata-20 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,542] INFO [MergedLog partition=_confluent-link-metadata-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,543] INFO Created log for partition _confluent-link-metadata-7 in /var/lib/kafka/data/_confluent-link-metadata-7 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,543] INFO [Partition _confluent-link-metadata-7 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-7 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,543] INFO [Partition _confluent-link-metadata-7 broker=1] Log loaded for partition _confluent-link-metadata-7 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,543] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-7 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,543] INFO [MergedLog partition=_confluent-link-metadata-7, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,543] INFO [Broker id=1] Leader _confluent-link-metadata-7 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,544] INFO [MergedLog partition=_confluent-link-metadata-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,544] INFO Created log for partition _confluent-link-metadata-19 in /var/lib/kafka/data/_confluent-link-metadata-19 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,544] INFO [Partition _confluent-link-metadata-19 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-19 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,544] INFO [Partition _confluent-link-metadata-19 broker=1] Log loaded for partition _confluent-link-metadata-19 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,544] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-19 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,544] INFO [MergedLog partition=_confluent-link-metadata-19, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,544] INFO [Broker id=1] Leader _confluent-link-metadata-19 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,547] INFO [MergedLog partition=_confluent-link-metadata-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,548] INFO Created log for partition _confluent-link-metadata-45 in /var/lib/kafka/data/_confluent-link-metadata-45 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,548] INFO [Partition _confluent-link-metadata-45 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-45 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,548] INFO [Partition _confluent-link-metadata-45 broker=1] Log loaded for partition _confluent-link-metadata-45 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,548] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-45 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,548] INFO [MergedLog partition=_confluent-link-metadata-45, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,548] INFO [Broker id=1] Leader _confluent-link-metadata-45 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,549] INFO [MergedLog partition=_confluent-link-metadata-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,549] INFO Created log for partition _confluent-link-metadata-33 in /var/lib/kafka/data/_confluent-link-metadata-33 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,549] INFO [Partition _confluent-link-metadata-33 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-33 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,549] INFO [Partition _confluent-link-metadata-33 broker=1] Log loaded for partition _confluent-link-metadata-33 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,549] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-33 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,549] INFO [MergedLog partition=_confluent-link-metadata-33, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,549] INFO [Broker id=1] Leader _confluent-link-metadata-33 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,550] INFO [MergedLog partition=_confluent-link-metadata-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,551] INFO Created log for partition _confluent-link-metadata-29 in /var/lib/kafka/data/_confluent-link-metadata-29 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,551] INFO [Partition _confluent-link-metadata-29 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-29 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,551] INFO [Partition _confluent-link-metadata-29 broker=1] Log loaded for partition _confluent-link-metadata-29 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,551] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-29 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,551] INFO [MergedLog partition=_confluent-link-metadata-29, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,551] INFO [Broker id=1] Leader _confluent-link-metadata-29 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,552] INFO [MergedLog partition=_confluent-link-metadata-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,552] INFO Created log for partition _confluent-link-metadata-46 in /var/lib/kafka/data/_confluent-link-metadata-46 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,552] INFO [Partition _confluent-link-metadata-46 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-46 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,552] INFO [Partition _confluent-link-metadata-46 broker=1] Log loaded for partition _confluent-link-metadata-46 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,552] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-46 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,552] INFO [MergedLog partition=_confluent-link-metadata-46, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,552] INFO [Broker id=1] Leader _confluent-link-metadata-46 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,553] INFO [MergedLog partition=_confluent-link-metadata-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,553] INFO Created log for partition _confluent-link-metadata-28 in /var/lib/kafka/data/_confluent-link-metadata-28 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,553] INFO [Partition _confluent-link-metadata-28 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-28 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,553] INFO [Partition _confluent-link-metadata-28 broker=1] Log loaded for partition _confluent-link-metadata-28 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,553] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-28 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,553] INFO [MergedLog partition=_confluent-link-metadata-28, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,553] INFO [Broker id=1] Leader _confluent-link-metadata-28 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,554] INFO [MergedLog partition=_confluent-link-metadata-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,555] INFO Created log for partition _confluent-link-metadata-27 in /var/lib/kafka/data/_confluent-link-metadata-27 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,555] INFO [Partition _confluent-link-metadata-27 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-27 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,555] INFO [Partition _confluent-link-metadata-27 broker=1] Log loaded for partition _confluent-link-metadata-27 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,555] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-27 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,555] INFO [MergedLog partition=_confluent-link-metadata-27, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,555] INFO [Broker id=1] Leader _confluent-link-metadata-27 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,556] INFO [MergedLog partition=_confluent-link-metadata-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,556] INFO Created log for partition _confluent-link-metadata-8 in /var/lib/kafka/data/_confluent-link-metadata-8 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,556] INFO [Partition _confluent-link-metadata-8 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-8 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,556] INFO [Partition _confluent-link-metadata-8 broker=1] Log loaded for partition _confluent-link-metadata-8 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,556] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-8 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,556] INFO [MergedLog partition=_confluent-link-metadata-8, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,556] INFO [Broker id=1] Leader _confluent-link-metadata-8 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,557] INFO [MergedLog partition=_confluent-link-metadata-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,557] INFO Created log for partition _confluent-link-metadata-49 in /var/lib/kafka/data/_confluent-link-metadata-49 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,557] INFO [Partition _confluent-link-metadata-49 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-49 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,557] INFO [Partition _confluent-link-metadata-49 broker=1] Log loaded for partition _confluent-link-metadata-49 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,557] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-49 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,557] INFO [MergedLog partition=_confluent-link-metadata-49, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,557] INFO [Broker id=1] Leader _confluent-link-metadata-49 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,558] INFO [MergedLog partition=_confluent-link-metadata-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,558] INFO Created log for partition _confluent-link-metadata-40 in /var/lib/kafka/data/_confluent-link-metadata-40 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,558] INFO [Partition _confluent-link-metadata-40 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-40 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,558] INFO [Partition _confluent-link-metadata-40 broker=1] Log loaded for partition _confluent-link-metadata-40 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,558] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-40 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,558] INFO [MergedLog partition=_confluent-link-metadata-40, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,558] INFO [Broker id=1] Leader _confluent-link-metadata-40 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,559] INFO [MergedLog partition=_confluent-link-metadata-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,559] INFO Created log for partition _confluent-link-metadata-15 in /var/lib/kafka/data/_confluent-link-metadata-15 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,559] INFO [Partition _confluent-link-metadata-15 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-15 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,559] INFO [Partition _confluent-link-metadata-15 broker=1] Log loaded for partition _confluent-link-metadata-15 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,559] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-15 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,559] INFO [MergedLog partition=_confluent-link-metadata-15, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,559] INFO [Broker id=1] Leader _confluent-link-metadata-15 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,561] INFO [MergedLog partition=_confluent-link-metadata-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,561] INFO Created log for partition _confluent-link-metadata-12 in /var/lib/kafka/data/_confluent-link-metadata-12 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,561] INFO [Partition _confluent-link-metadata-12 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-12 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,561] INFO [Partition _confluent-link-metadata-12 broker=1] Log loaded for partition _confluent-link-metadata-12 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,561] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-12 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,561] INFO [MergedLog partition=_confluent-link-metadata-12, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,561] INFO [Broker id=1] Leader _confluent-link-metadata-12 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,563] INFO [MergedLog partition=_confluent-link-metadata-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,563] INFO Created log for partition _confluent-link-metadata-0 in /var/lib/kafka/data/_confluent-link-metadata-0 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,563] INFO [Partition _confluent-link-metadata-0 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,563] INFO [Partition _confluent-link-metadata-0 broker=1] Log loaded for partition _confluent-link-metadata-0 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,563] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-0 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,563] INFO [MergedLog partition=_confluent-link-metadata-0, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,563] INFO [Broker id=1] Leader _confluent-link-metadata-0 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,564] INFO [MergedLog partition=_confluent-link-metadata-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,564] INFO Created log for partition _confluent-link-metadata-37 in /var/lib/kafka/data/_confluent-link-metadata-37 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,565] INFO [Partition _confluent-link-metadata-37 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-37 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,565] INFO [Partition _confluent-link-metadata-37 broker=1] Log loaded for partition _confluent-link-metadata-37 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,565] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-37 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,565] INFO [MergedLog partition=_confluent-link-metadata-37, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,565] INFO [Broker id=1] Leader _confluent-link-metadata-37 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,567] INFO [MergedLog partition=_confluent-link-metadata-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,567] INFO Created log for partition _confluent-link-metadata-17 in /var/lib/kafka/data/_confluent-link-metadata-17 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,567] INFO [Partition _confluent-link-metadata-17 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-17 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,567] INFO [Partition _confluent-link-metadata-17 broker=1] Log loaded for partition _confluent-link-metadata-17 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,567] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-17 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,567] INFO [MergedLog partition=_confluent-link-metadata-17, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,568] INFO [Broker id=1] Leader _confluent-link-metadata-17 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,570] INFO [MergedLog partition=_confluent-link-metadata-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,570] INFO Created log for partition _confluent-link-metadata-32 in /var/lib/kafka/data/_confluent-link-metadata-32 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,570] INFO [Partition _confluent-link-metadata-32 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-32 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,570] INFO [Partition _confluent-link-metadata-32 broker=1] Log loaded for partition _confluent-link-metadata-32 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,570] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-32 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,570] INFO [MergedLog partition=_confluent-link-metadata-32, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,570] INFO [Broker id=1] Leader _confluent-link-metadata-32 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,574] INFO [MergedLog partition=_confluent-link-metadata-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,575] INFO Created log for partition _confluent-link-metadata-31 in /var/lib/kafka/data/_confluent-link-metadata-31 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,575] INFO [Partition _confluent-link-metadata-31 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-31 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,575] INFO [Partition _confluent-link-metadata-31 broker=1] Log loaded for partition _confluent-link-metadata-31 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,575] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-31 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,575] INFO [MergedLog partition=_confluent-link-metadata-31, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,575] INFO [Broker id=1] Leader _confluent-link-metadata-31 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,576] INFO [MergedLog partition=_confluent-link-metadata-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,576] INFO Created log for partition _confluent-link-metadata-3 in /var/lib/kafka/data/_confluent-link-metadata-3 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,576] INFO [Partition _confluent-link-metadata-3 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-3 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,576] INFO [Partition _confluent-link-metadata-3 broker=1] Log loaded for partition _confluent-link-metadata-3 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,576] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-3 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,576] INFO [MergedLog partition=_confluent-link-metadata-3, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,576] INFO [Broker id=1] Leader _confluent-link-metadata-3 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,578] INFO [MergedLog partition=_confluent-link-metadata-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,578] INFO Created log for partition _confluent-link-metadata-16 in /var/lib/kafka/data/_confluent-link-metadata-16 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,578] INFO [Partition _confluent-link-metadata-16 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-16 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,578] INFO [Partition _confluent-link-metadata-16 broker=1] Log loaded for partition _confluent-link-metadata-16 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,578] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-16 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,578] INFO [MergedLog partition=_confluent-link-metadata-16, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,578] INFO [Broker id=1] Leader _confluent-link-metadata-16 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,579] INFO [MergedLog partition=_confluent-link-metadata-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,579] INFO Created log for partition _confluent-link-metadata-11 in /var/lib/kafka/data/_confluent-link-metadata-11 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,579] INFO [Partition _confluent-link-metadata-11 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-11 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,579] INFO [Partition _confluent-link-metadata-11 broker=1] Log loaded for partition _confluent-link-metadata-11 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,579] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-11 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,579] INFO [MergedLog partition=_confluent-link-metadata-11, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,579] INFO [Broker id=1] Leader _confluent-link-metadata-11 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,720] INFO [MergedLog partition=_confluent-link-metadata-48, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,721] INFO Created log for partition _confluent-link-metadata-48 in /var/lib/kafka/data/_confluent-link-metadata-48 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,721] INFO [Partition _confluent-link-metadata-48 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-48 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,721] INFO [Partition _confluent-link-metadata-48 broker=1] Log loaded for partition _confluent-link-metadata-48 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,721] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-48 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,721] INFO [MergedLog partition=_confluent-link-metadata-48, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,721] INFO [Broker id=1] Leader _confluent-link-metadata-48 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,722] INFO [MergedLog partition=_confluent-link-metadata-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,722] INFO Created log for partition _confluent-link-metadata-18 in /var/lib/kafka/data/_confluent-link-metadata-18 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,722] INFO [Partition _confluent-link-metadata-18 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-18 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,722] INFO [Partition _confluent-link-metadata-18 broker=1] Log loaded for partition _confluent-link-metadata-18 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,722] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-18 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,722] INFO [MergedLog partition=_confluent-link-metadata-18, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,722] INFO [Broker id=1] Leader _confluent-link-metadata-18 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,724] INFO [MergedLog partition=_confluent-link-metadata-35, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,724] INFO Created log for partition _confluent-link-metadata-35 in /var/lib/kafka/data/_confluent-link-metadata-35 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,724] INFO [Partition _confluent-link-metadata-35 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-35 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,724] INFO [Partition _confluent-link-metadata-35 broker=1] Log loaded for partition _confluent-link-metadata-35 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,724] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-35 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,724] INFO [MergedLog partition=_confluent-link-metadata-35, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,724] INFO [Broker id=1] Leader _confluent-link-metadata-35 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,726] INFO [MergedLog partition=_confluent-link-metadata-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,726] INFO Created log for partition _confluent-link-metadata-4 in /var/lib/kafka/data/_confluent-link-metadata-4 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,726] INFO [Partition _confluent-link-metadata-4 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-4 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,726] INFO [Partition _confluent-link-metadata-4 broker=1] Log loaded for partition _confluent-link-metadata-4 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,726] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-4 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,726] INFO [MergedLog partition=_confluent-link-metadata-4, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,726] INFO [Broker id=1] Leader _confluent-link-metadata-4 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,727] INFO [MergedLog partition=_confluent-link-metadata-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,727] INFO Created log for partition _confluent-link-metadata-38 in /var/lib/kafka/data/_confluent-link-metadata-38 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,727] INFO [Partition _confluent-link-metadata-38 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-38 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,727] INFO [Partition _confluent-link-metadata-38 broker=1] Log loaded for partition _confluent-link-metadata-38 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,727] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-38 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,727] INFO [MergedLog partition=_confluent-link-metadata-38, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,728] INFO [Broker id=1] Leader _confluent-link-metadata-38 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,729] INFO [MergedLog partition=_confluent-link-metadata-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,729] INFO Created log for partition _confluent-link-metadata-39 in /var/lib/kafka/data/_confluent-link-metadata-39 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,729] INFO [Partition _confluent-link-metadata-39 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-39 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,729] INFO [Partition _confluent-link-metadata-39 broker=1] Log loaded for partition _confluent-link-metadata-39 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,729] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-39 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,729] INFO [MergedLog partition=_confluent-link-metadata-39, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,729] INFO [Broker id=1] Leader _confluent-link-metadata-39 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,730] INFO [MergedLog partition=_confluent-link-metadata-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,730] INFO Created log for partition _confluent-link-metadata-34 in /var/lib/kafka/data/_confluent-link-metadata-34 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,730] INFO [Partition _confluent-link-metadata-34 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-34 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,730] INFO [Partition _confluent-link-metadata-34 broker=1] Log loaded for partition _confluent-link-metadata-34 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,730] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-34 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,730] INFO [MergedLog partition=_confluent-link-metadata-34, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,730] INFO [Broker id=1] Leader _confluent-link-metadata-34 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,731] INFO [MergedLog partition=_confluent-link-metadata-36, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,731] INFO Created log for partition _confluent-link-metadata-36 in /var/lib/kafka/data/_confluent-link-metadata-36 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,731] INFO [Partition _confluent-link-metadata-36 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-36 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,731] INFO [Partition _confluent-link-metadata-36 broker=1] Log loaded for partition _confluent-link-metadata-36 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,731] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-36 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,731] INFO [MergedLog partition=_confluent-link-metadata-36, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,732] INFO [Broker id=1] Leader _confluent-link-metadata-36 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,734] INFO [MergedLog partition=_confluent-link-metadata-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,734] INFO Created log for partition _confluent-link-metadata-43 in /var/lib/kafka/data/_confluent-link-metadata-43 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,734] INFO [Partition _confluent-link-metadata-43 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-43 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,734] INFO [Partition _confluent-link-metadata-43 broker=1] Log loaded for partition _confluent-link-metadata-43 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,734] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-43 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,734] INFO [MergedLog partition=_confluent-link-metadata-43, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,734] INFO [Broker id=1] Leader _confluent-link-metadata-43 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,736] INFO [MergedLog partition=_confluent-link-metadata-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,736] INFO Created log for partition _confluent-link-metadata-44 in /var/lib/kafka/data/_confluent-link-metadata-44 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,736] INFO [Partition _confluent-link-metadata-44 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-44 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,736] INFO [Partition _confluent-link-metadata-44 broker=1] Log loaded for partition _confluent-link-metadata-44 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,736] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-44 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,736] INFO [MergedLog partition=_confluent-link-metadata-44, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,736] INFO [Broker id=1] Leader _confluent-link-metadata-44 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,738] INFO [MergedLog partition=_confluent-link-metadata-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,738] INFO Created log for partition _confluent-link-metadata-22 in /var/lib/kafka/data/_confluent-link-metadata-22 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,738] INFO [Partition _confluent-link-metadata-22 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-22 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,738] INFO [Partition _confluent-link-metadata-22 broker=1] Log loaded for partition _confluent-link-metadata-22 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,738] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-22 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,738] INFO [MergedLog partition=_confluent-link-metadata-22, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,738] INFO [Broker id=1] Leader _confluent-link-metadata-22 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,739] INFO HV000001: Hibernate Validator 8.0.1.Final (org.hibernate.validator.internal.util.Version:%L)
[2025-06-22 05:26:39,740] INFO [MergedLog partition=_confluent-link-metadata-47, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,740] INFO Created log for partition _confluent-link-metadata-47 in /var/lib/kafka/data/_confluent-link-metadata-47 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,740] INFO [Partition _confluent-link-metadata-47 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-47 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,740] INFO [Partition _confluent-link-metadata-47 broker=1] Log loaded for partition _confluent-link-metadata-47 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,740] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-47 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,740] INFO [MergedLog partition=_confluent-link-metadata-47, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,740] INFO [Broker id=1] Leader _confluent-link-metadata-47 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,742] INFO [MergedLog partition=_confluent-link-metadata-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,742] INFO Created log for partition _confluent-link-metadata-2 in /var/lib/kafka/data/_confluent-link-metadata-2 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,743] INFO [Partition _confluent-link-metadata-2 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-2 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,743] INFO [Partition _confluent-link-metadata-2 broker=1] Log loaded for partition _confluent-link-metadata-2 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,743] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-2 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,743] INFO [MergedLog partition=_confluent-link-metadata-2, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,743] INFO [Broker id=1] Leader _confluent-link-metadata-2 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,744] INFO [MergedLog partition=_confluent-link-metadata-42, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,744] INFO Created log for partition _confluent-link-metadata-42 in /var/lib/kafka/data/_confluent-link-metadata-42 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,744] INFO [Partition _confluent-link-metadata-42 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-42 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,744] INFO [Partition _confluent-link-metadata-42 broker=1] Log loaded for partition _confluent-link-metadata-42 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,744] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-42 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,744] INFO [MergedLog partition=_confluent-link-metadata-42, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,744] INFO [Broker id=1] Leader _confluent-link-metadata-42 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,745] INFO [MergedLog partition=_confluent-link-metadata-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,745] INFO Created log for partition _confluent-link-metadata-9 in /var/lib/kafka/data/_confluent-link-metadata-9 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,745] INFO [Partition _confluent-link-metadata-9 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-9 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,745] INFO [Partition _confluent-link-metadata-9 broker=1] Log loaded for partition _confluent-link-metadata-9 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,745] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-9 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,745] INFO [MergedLog partition=_confluent-link-metadata-9, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,745] INFO [Broker id=1] Leader _confluent-link-metadata-9 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,747] INFO [MergedLog partition=_confluent-link-metadata-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,747] INFO Created log for partition _confluent-link-metadata-6 in /var/lib/kafka/data/_confluent-link-metadata-6 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,747] INFO [Partition _confluent-link-metadata-6 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-6 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,747] INFO [Partition _confluent-link-metadata-6 broker=1] Log loaded for partition _confluent-link-metadata-6 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,747] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-6 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,747] INFO [MergedLog partition=_confluent-link-metadata-6, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,747] INFO [Broker id=1] Leader _confluent-link-metadata-6 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,748] INFO [MergedLog partition=_confluent-link-metadata-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,748] INFO Created log for partition _confluent-link-metadata-5 in /var/lib/kafka/data/_confluent-link-metadata-5 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,748] INFO [Partition _confluent-link-metadata-5 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-5 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,748] INFO [Partition _confluent-link-metadata-5 broker=1] Log loaded for partition _confluent-link-metadata-5 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,748] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-5 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,748] INFO [MergedLog partition=_confluent-link-metadata-5, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,748] INFO [Broker id=1] Leader _confluent-link-metadata-5 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,749] INFO [MergedLog partition=_confluent-link-metadata-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,749] INFO Created log for partition _confluent-link-metadata-26 in /var/lib/kafka/data/_confluent-link-metadata-26 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,749] INFO [Partition _confluent-link-metadata-26 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-26 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,749] INFO [Partition _confluent-link-metadata-26 broker=1] Log loaded for partition _confluent-link-metadata-26 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,749] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-26 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,749] INFO [MergedLog partition=_confluent-link-metadata-26, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,749] INFO [Broker id=1] Leader _confluent-link-metadata-26 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,750] INFO [MergedLog partition=_confluent-link-metadata-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,750] INFO Created log for partition _confluent-link-metadata-10 in /var/lib/kafka/data/_confluent-link-metadata-10 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,750] INFO [Partition _confluent-link-metadata-10 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-10 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,750] INFO [Partition _confluent-link-metadata-10 broker=1] Log loaded for partition _confluent-link-metadata-10 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,750] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-10 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,750] INFO [MergedLog partition=_confluent-link-metadata-10, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,750] INFO [Broker id=1] Leader _confluent-link-metadata-10 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,751] INFO [MergedLog partition=_confluent-link-metadata-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,751] INFO Created log for partition _confluent-link-metadata-30 in /var/lib/kafka/data/_confluent-link-metadata-30 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,751] INFO [Partition _confluent-link-metadata-30 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-30 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,751] INFO [Partition _confluent-link-metadata-30 broker=1] Log loaded for partition _confluent-link-metadata-30 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,751] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-30 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,751] INFO [MergedLog partition=_confluent-link-metadata-30, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,751] INFO [Broker id=1] Leader _confluent-link-metadata-30 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,752] INFO [MergedLog partition=_confluent-link-metadata-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,753] INFO Created log for partition _confluent-link-metadata-14 in /var/lib/kafka/data/_confluent-link-metadata-14 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,753] INFO [Partition _confluent-link-metadata-14 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-14 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,753] INFO [Partition _confluent-link-metadata-14 broker=1] Log loaded for partition _confluent-link-metadata-14 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,753] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-14 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,753] INFO [MergedLog partition=_confluent-link-metadata-14, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,753] INFO [Broker id=1] Leader _confluent-link-metadata-14 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,754] INFO [MergedLog partition=_confluent-link-metadata-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,754] INFO Created log for partition _confluent-link-metadata-13 in /var/lib/kafka/data/_confluent-link-metadata-13 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,754] INFO [Partition _confluent-link-metadata-13 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-13 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,754] INFO [Partition _confluent-link-metadata-13 broker=1] Log loaded for partition _confluent-link-metadata-13 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,754] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-13 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,754] INFO [MergedLog partition=_confluent-link-metadata-13, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,754] INFO [Broker id=1] Leader _confluent-link-metadata-13 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,755] INFO [MergedLog partition=_confluent-link-metadata-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,755] INFO Created log for partition _confluent-link-metadata-1 in /var/lib/kafka/data/_confluent-link-metadata-1 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,755] INFO [Partition _confluent-link-metadata-1 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-1 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,755] INFO [Partition _confluent-link-metadata-1 broker=1] Log loaded for partition _confluent-link-metadata-1 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,755] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-1 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,755] INFO [MergedLog partition=_confluent-link-metadata-1, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,755] INFO [Broker id=1] Leader _confluent-link-metadata-1 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,756] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-link-metadata with new configuration : cleanup.policy -> compact,min.insync.replicas -> 2 (kafka.server.metadata.DynamicConfigPublisher:%L)
[2025-06-22 05:26:39,889] INFO Started oeje10s.ServletContextHandler@6d711502{/kafka,/kafka,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@2a49570f{STARTED}} (org.eclipse.jetty.ee10.servlet.ServletContextHandler:%L)
[2025-06-22 05:26:39,889] INFO Started oeje10s.ServletContextHandler@69b78c1e{/v1/metadata,/v1/metadata,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@2d6bbdbe{STARTED}} (org.eclipse.jetty.server.handler.ContextHandler:%L)
Jun 22, 2025 5:26:39 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider io.confluent.metadataapi.resources.MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.metadataapi.resources.MetadataResource will be ignored. 
[2025-06-22 05:26:39,896] INFO Started oeje10s.ServletContextHandler@69b78c1e{/v1/metadata,/v1/metadata,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@2d6bbdbe{STARTED}} (org.eclipse.jetty.ee10.servlet.ServletContextHandler:%L)
[2025-06-22 05:26:39,898] INFO Started oeje10s.ServletContextHandler@1f4dfaef{/ws,/ws,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@1058b396{STARTED}} (org.eclipse.jetty.server.handler.ContextHandler:%L)
[2025-06-22 05:26:39,898] INFO Started oeje10s.ServletContextHandler@1f4dfaef{/ws,/ws,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@1058b396{STARTED}} (org.eclipse.jetty.ee10.servlet.ServletContextHandler:%L)
[2025-06-22 05:26:39,898] INFO Started oeje10s.ServletContextHandler@7418981d{/ws,/ws,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@4b972d75{STARTED}} (org.eclipse.jetty.server.handler.ContextHandler:%L)
[2025-06-22 05:26:39,898] INFO Started oeje10s.ServletContextHandler@7418981d{/ws,/ws,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@4b972d75{STARTED}} (org.eclipse.jetty.ee10.servlet.ServletContextHandler:%L)
[2025-06-22 05:26:39,899] INFO Getter/setter type mismatch for mbean attribute formEncodedMethods in class org.eclipse.jetty.server.HttpConfiguration, attribute will be read-only (org.eclipse.jetty.jmx.MetaData:%L)
[2025-06-22 05:26:39,900] INFO Started NetworkTrafficServerConnector@336a1881{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8090} (org.eclipse.jetty.server.AbstractConnector:%L)
[2025-06-22 05:26:39,900] INFO Started icr.ApplicationServer@62526b55{STARTING}[12.0.16,sto=5000] @1306ms (org.eclipse.jetty.server.Server:%L)
[2025-06-22 05:26:39,900] INFO KafkaHttpServer transitioned from STARTING to RUNNING.. (io.confluent.http.server.KafkaHttpServerImpl:%L)
[2025-06-22 05:26:39,900] INFO LicenseConfig values: 
	confluent.license = [hidden]
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 1
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,900] INFO LicenseConfig values: 
	confluent.license = [hidden]
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 1
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,901] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-1
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,901] INFO These configurations '[replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, min.insync.replicas, cluster.link.metadata.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,901] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,901] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,901] INFO Kafka startTimeMs: 1750569999901 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,916] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,917] INFO Starting License Store (io.confluent.license.LicenseStore:%L)
[2025-06-22 05:26:39,917] INFO Starting KafkaBasedLog with topic _confluent-command reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:%L)
[2025-06-22 05:26:39,917] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-1
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,917] INFO These configurations '[replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, min.insync.replicas, cluster.link.metadata.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,917] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,917] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,917] INFO Kafka startTimeMs: 1750569999917 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] Using the default placer, StripedReplicaPlacer, to make the assignment for topic _confluent-command. (kafka.assignor.ConfluentReplicaPlacer:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreatableTopicConfig(name='min.insync.replicas', value='1'), CreatableTopicConfig(name='cleanup.policy', value='compact')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-command with topic ID SxP9Ygb5TjS9pL_-neZ5jA. (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-command') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-command') which set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-command-0 with topic ID SxP9Ygb5TjS9pL_-neZ5jA and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,946] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger:%L)
[2025-06-22 05:26:39,946] INFO SBC Event SbcMetadataUpdateEvent-13 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-14]. Enqueuing... (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,946] INFO Handling event SbcConfigUpdateEvent-14 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,946] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-command')=ConfigurationDelta(changedKeys=[cleanup.policy, min.insync.replicas])}) (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,946] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager:%L)
[2025-06-22 05:26:39,946] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,946] INFO [Broker id=1] Creating new partition _confluent-command-0 with topic id SxP9Ygb5TjS9pL_-neZ5jA. (state.change.logger:%L)
[2025-06-22 05:26:39,946] INFO Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":1,"assignments":[],"configs":[{"name":"min.insync.replicas","value":"1"},{"name":"cleanup.policy","value":"compact"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"responseData":"AAAABQAAAAAAAhNfY29uZmx1ZW50LWNvbW1hbmRLE/1iBvlONL2kv/6d5nmMAAAAAAAAAQABPQ9jbGVhbnVwLnBvbGljeQhjb21wYWN0AAEAABdjb21wcmVzc2lvbi5nemlwLmxldmVsAy0xAAUAABZjb21wcmVzc2lvbi5sejQubGV2ZWwCOQAFAAARY29tcHJlc3Npb24udHlwZQlwcm9kdWNlcgAFAAAXY29tcHJlc3Npb24uenN0ZC5sZXZlbAIzAAUAACxjb25mbHVlbnQuYXBwZW5kLnJlY29yZC5pbnRlcmNlcHRvci5jbGFzc2VzAQAFAAAzY29uZmx1ZW50LmNsdXN0ZXIubGluay5hbGxvdy5sZWdhY3kubWVzc2FnZS5mb3JtYXQGZmFsc2UABQAAL2NvbmZsdWVudC5jb21wYWN0ZWQudG9waWMucHJlZmVyLnRpZXIuZmV0Y2gubXMDLTEABQAAIGNvbmZsdWVudC5rZXkuc2NoZW1hLnZhbGlkYXRpb24GZmFsc2UABQAAJGNvbmZsdWVudC5rZXkuc3ViamVjdC5uYW1lLnN0cmF0ZWd5OWlvLmNvbmZsdWVudC5rYWZrYS5zZXJpYWxpemVycy5zdWJqZWN0LlRvcGljTmFtZVN0cmF0ZWd5AAUAADJjb25mbHVlbnQubG9nLmNsZWFuZXIudGltZXN0YW1wLnZhbGlkYXRpb24uZW5hYmxlBXRydWUABQAAGWNvbmZsdWVudC5tYXguc2VnbWVudC5tcxQ5MjIzMzcyMDM2ODU0Nzc1ODA3AAUAABljb25mbHVlbnQubWluLnNlZ21lbnQubXMCMQAFAAAgY29uZmx1ZW50LnBsYWNlbWVudC5jb25zdHJhaW50cwEABQAAH2NvbmZsdWVudC5wcmVmZXIudGllci5mZXRjaC5tcwMtMQAFAAAwY29uZmx1ZW50LnNjaGVtYS52YWxpZGF0aW9uLmNvbnRleHQubmFtZS5lbmFibGUGZmFsc2UABQAALmNvbmZsdWVudC5zZWdtZW50LnNwZWN1bGF0aXZlLnByZWZldGNoLmVuYWJsZQZmYWxzZQAFAAAkY29uZmx1ZW50LnN0cmF5LmxvZy5kZWxldGUuZGVsYXkubXMKNjA0ODAwMDAwAAUAACpjb25mbHVlbnQuc3RyYXkubG9nLm1heC5kZWxldGlvbnMucGVyLnJ1bgM3MgAFAAAiY29uZmx1ZW50LnN5c3RlbS50aW1lLnJvbGwuZW5hYmxlBmZhbHNlAAUAAC5jb25mbHVlbnQudGllci5jbGVhbmVyLmNvbXBhY3QubWluLmVmZmljaWVuY3kEMC41AAUAADFjb25mbHVlbnQudGllci5jbGVhbmVyLmNvbXBhY3Quc2VnbWVudC5taW4uYnl0ZXMJMjA5NzE1MjAABQAAJ2NvbmZsdWVudC50aWVyLmNsZWFuZXIuZHVhbC5jb21wYWN0aW9uBmZhbHNlAAUAAB5jb25mbHVlbnQudGllci5jbGVhbmVyLmVuYWJsZQZmYWxzZQAFAAArY29uZmx1ZW50LnRpZXIuY2xlYW5lci5taW4uY2xlYW5hYmxlLnJhdGlvBTAuNzUABQAAFmNvbmZsdWVudC50aWVyLmVuYWJsZQZmYWxzZQAFAAAiY29uZmx1ZW50LnRpZXIubG9jYWwuaG90c2V0LmJ5dGVzAy0xAAUAAB9jb25mbHVlbnQudGllci5sb2NhbC5ob3RzZXQubXMJODY0MDAwMDAABQAALWNvbmZsdWVudC50aWVyLnNlZ21lbnQuaG90c2V0LnJvbGwubWluLmJ5dGVzCjEwNDg1NzYwMAAFAAAVY29uZmx1ZW50LnRvcGljLnR5cGUJc3RhbmRhcmQABQAAImNvbmZsdWVudC52YWx1ZS5zY2hlbWEudmFsaWRhdGlvbgZmYWxzZQAFAAAmY29uZmx1ZW50LnZhbHVlLnN1YmplY3QubmFtZS5zdHJhdGVneTlpby5jb25mbHVlbnQua2Fma2Euc2VyaWFsaXplcnMuc3ViamVjdC5Ub3BpY05hbWVTdHJhdGVneQAFAAAUZGVsZXRlLnJldGVudGlvbi5tcwk4NjQwMDAwMAAFAAAVZmlsZS5kZWxldGUuZGVsYXkubXMGNjAwMDAABQAAD2ZsdXNoLm1lc3NhZ2VzFDkyMjMzNzIwMzY4NTQ3NzU4MDcABQAACWZsdXNoLm1zFDkyMjMzNzIwMzY4NTQ3NzU4MDcABQAAKGZvbGxvd2VyLnJlcGxpY2F0aW9uLnRocm90dGxlZC5yZXBsaWNhcwEABQAAFWluZGV4LmludGVydmFsLmJ5dGVzBTQwOTYABQAAJmxlYWRlci5yZXBsaWNhdGlvbi50aHJvdHRsZWQucmVwbGljYXMBAAUAABZsb2NhbC5yZXRlbnRpb24uYnl0ZXMDLTIABQAAE2xvY2FsLnJldGVudGlvbi5tcwMtMgAFAAAWbWF4LmNvbXBhY3Rpb24ubGFnLm1zFDkyMjMzNzIwMzY4NTQ3NzU4MDcABQAAEm1heC5tZXNzYWdlLmJ5dGVzCDEwNDg1ODgABQAAH21lc3NhZ2UudGltZXN0YW1wLmFmdGVyLm1heC5tcwgzNjAwMDAwAAUAACBtZXNzYWdlLnRpbWVzdGFtcC5iZWZvcmUubWF4Lm1zFDkyMjMzNzIwMzY4NTQ3NzU4MDcABQAAF21lc3NhZ2UudGltZXN0YW1wLnR5cGULQ3JlYXRlVGltZQAFAAAabWluLmNsZWFuYWJsZS5kaXJ0eS5yYXRpbwQwLjUABQAAFm1pbi5jb21wYWN0aW9uLmxhZy5tcwIwAAUAABRtaW4uaW5zeW5jLnJlcGxpY2FzAjEAAQAADHByZWFsbG9jYXRlBmZhbHNlAAUAABhyZW1vdGUubG9nLmNvcHkuZGlzYWJsZQZmYWxzZQAFAAAdcmVtb3RlLmxvZy5kZWxldGUub24uZGlzYWJsZQZmYWxzZQAFAAAWcmVtb3RlLnN0b3JhZ2UuZW5hYmxlBmZhbHNlAAUAABByZXRlbnRpb24uYnl0ZXMDLTEABQAADXJldGVudGlvbi5tcwo2MDQ4MDAwMDAABQAADnNlZ21lbnQuYnl0ZXMLMTA3Mzc0MTgyNAAFAAAUc2VnbWVudC5pbmRleC5ieXRlcwkxMDQ4NTc2MAAFAAASc2VnbWVudC5qaXR0ZXIubXMCMAAFAAALc2VnbWVudC5tcwo2MDQ4MDAwMDAABQAAH3VuY2xlYW4ubGVhZGVyLmVsZWN0aW9uLmVuYWJsZQZmYWxzZQAFAAAAAA==","errorCode":0},"connection":"127.0.0.1:29093-127.0.0.1:34176-2-0","clientAddress":"127.0.0.1","totalTimeMs":26.306,"requestQueueTimeMs":0.033,"localTimeMs":0.072,"remoteTimeMs":26.014,"throttleTimeMs":0,"responseQueueTimeMs":0.117,"sendTimeMs":0.069,"sendIoTimeMs":0.053,"responseSize":2538,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":175056999991900002} (kafka.request.logger:%L)
[2025-06-22 05:26:39,946] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 1 partitions (state.change.logger:%L)
[2025-06-22 05:26:39,946] INFO Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":1,"assignments":[],"configs":[{"name":"min.insync.replicas","value":"1"},{"name":"cleanup.policy","value":"compact"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-command","topicId":"SxP9Ygb5TjS9pL_-neZ5jA","errorCode":0,"errorMessage":null,"numPartitions":1,"replicationFactor":1,"configs":[{"configName":"cleanup.policy","value":"compact","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"compression.gzip.level","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.lz4.level","value":"9","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.type","value":"producer","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.zstd.level","value":"3","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.append.record.interceptor.classes","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.cluster.link.allow.legacy.message.format","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.compacted.topic.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.log.cleaner.timestamp.validation.enable","value":"true","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.max.segment.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.min.segment.ms","value":"1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.placement.constraints","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.schema.validation.context.name.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.segment.speculative.prefetch.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.delete.delay.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.max.deletions.per.run","value":"72","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.system.time.roll.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.min.efficiency","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.segment.min.bytes","value":"20971520","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.dual.compaction","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.min.cleanable.ratio","value":"0.75","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.bytes","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.segment.hotset.roll.min.bytes","value":"104857600","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.topic.type","value":"standard","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"delete.retention.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"file.delete.delay.ms","value":"60000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.messages","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"follower.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"index.interval.bytes","value":"4096","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"leader.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.bytes","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.ms","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.compaction.lag.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.message.bytes","value":"1048588","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.after.max.ms","value":"3600000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.before.max.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.type","value":"CreateTime","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.cleanable.dirty.ratio","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.compaction.lag.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.insync.replicas","value":"1","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"preallocate","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.copy.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.delete.on.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.storage.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"retention.bytes","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"retention.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.bytes","value":"1073741824","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.index.bytes","value":"10485760","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.jitter.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"unclean.leader.election.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false}]}]},"connection":"127.0.0.1:29092-127.0.0.1:53444-2-1","clientAddress":"127.0.0.1","totalTimeMs":26.746,"requestQueueTimeMs":0.036,"localTimeMs":0.047,"remoteTimeMs":26.573,"throttleTimeMs":0,"responseQueueTimeMs":0.042,"sendTimeMs":0.047,"sendIoTimeMs":0.035,"responseSize":2528,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"8.0.0-0-ce"},"isDisconnectedClient":false,"requestId":175056999991900202} (kafka.request.logger:%L)
[2025-06-22 05:26:39,946] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,947] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-consumer-1
	client.rack = 
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,947] INFO [MergedLog partition=_confluent-command-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,947] INFO Created log for partition _confluent-command-0 in /var/lib/kafka/data/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=1} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,947] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,947] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,948] INFO Setting topicIdPartition SxP9Ygb5TjS9pL_-neZ5jA:_confluent-command-0 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,948] INFO [MergedLog partition=_confluent-command-0, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-command-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,948] INFO [Broker id=1] Leader _confluent-command-0 with topic id Some(SxP9Ygb5TjS9pL_-neZ5jA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,948] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-command with new configuration : cleanup.policy -> compact,min.insync.replicas -> 1 (kafka.server.metadata.DynamicConfigPublisher:%L)
[2025-06-22 05:26:39,950] INFO These configurations '[confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, cluster.link.metadata.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,950] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,950] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,950] INFO Kafka startTimeMs: 1750569999950 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,951] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata:%L)
[2025-06-22 05:26:39,952] INFO App info kafka.consumer for _confluent-license-consumer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,952] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:29092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = false
	interceptor.classes = []
	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	linger.ms = 5
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,953] INFO These configurations '[cluster.link.metadata.topic.replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,953] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,953] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,953] INFO Kafka startTimeMs: 1750569999953 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,953] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-consumer-1
	client.rack = 
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,953] INFO These configurations '[cluster.link.metadata.topic.replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,953] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,953] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,953] INFO Kafka startTimeMs: 1750569999953 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,954] INFO [Producer clientId=_confluent-license-producer-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata:%L)
[2025-06-22 05:26:39,954] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata:%L)
[2025-06-22 05:26:39,954] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Assigned to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:%L)
[2025-06-22 05:26:39,954] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Seeking to AutoOffsetResetStrategy{type=earliest} offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:%L)
[2025-06-22 05:26:39,956] INFO Finished reading KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog:%L)
[2025-06-22 05:26:39,956] INFO Started KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog:%L)
[2025-06-22 05:26:39,956] INFO Started License Store (io.confluent.license.LicenseStore:%L)
[2025-06-22 05:26:39,995] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata:%L)
[2025-06-22 05:26:40,460] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-1
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:40,461] INFO These configurations '[replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, min.insync.replicas, cluster.link.metadata.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:40,461] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,461] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,461] INFO Kafka startTimeMs: 1750570000461 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,464] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,476] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-1
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:40,476] INFO These configurations '[replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, min.insync.replicas, cluster.link.metadata.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:40,476] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,476] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,476] INFO Kafka startTimeMs: 1750570000476 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,479] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,479] INFO License for single cluster, single node (io.confluent.license.LicenseManager:%L)
[2025-06-22 05:26:40,480] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:40,480] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,480] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,480] INFO Kafka startTimeMs: 1750570000480 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,480] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer:%L)
[2025-06-22 05:27:09,111] INFO Beginning log roller... (kafka.log.LogManager:%L)
[2025-06-22 05:27:09,112] INFO Log roller completed in 0 seconds (kafka.log.LogManager:%L)
[2025-06-22 05:27:38,954] INFO [ControllerServer id=1] In the last 60000 ms period, 338 controller events were completed, which took an average of 10.81 ms each. The slowest event was writeNoOpRecord(1648315861), which took 42.47 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:27:38,959] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:27:39,423] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:27:39,428] INFO These configurations '[compression.type, enable.idempotence, acks, key.serializer, max.request.size, value.serializer, partitioner.class, interceptor.classes, max.in.flight.requests.per.connection, linger.ms]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:27:39,428] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:27:39,428] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:27:39,428] INFO Kafka startTimeMs: 1750570059428 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:27:39,475] INFO [ControllerServer id=1] Using the default placer, StripedReplicaPlacer, to make the assignment for topic _confluent-telemetry-metrics. (kafka.assignor.ConfluentReplicaPlacer:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=1, assignments=[], configs=[CreatableTopicConfig(name='max.message.bytes', value='10485760'), CreatableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreatableTopicConfig(name='min.insync.replicas', value='1'), CreatableTopicConfig(name='retention.ms', value='259200000'), CreatableTopicConfig(name='segment.ms', value='14400000'), CreatableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-telemetry-metrics with topic ID WJs2_1GlS2iUCcZR9G1uRw. (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration max.message.bytes to 10485760 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration segment.ms to 14400000 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-0 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-1 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-2 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-3 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-4 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-5 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-6 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-7 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-8 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-9 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-10 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-11 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,505] INFO SBC Event SbcMetadataUpdateEvent-134 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-135]. Enqueuing... (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:27:39,505] INFO [Broker id=1] Transitioning 12 partition(s) to local leaders. (state.change.logger:%L)
[2025-06-22 05:27:39,505] INFO Handling event SbcConfigUpdateEvent-135 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:27:39,505] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11, _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2) (kafka.server.ReplicaFetcherManager:%L)
[2025-06-22 05:27:39,505] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-3 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,505] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics')=ConfigurationDelta(changedKeys=[max.message.bytes, message.timestamp.type, min.insync.replicas, retention.ms, segment.ms, retention.bytes])}) (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:27:39,505] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:27:39,506] INFO Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"confluent-telemetry-reporter-local-producer","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-telemetry-metrics","numPartitions":12,"replicationFactor":1,"assignments":[],"configs":[{"name":"max.message.bytes","value":"10485760"},{"name":"message.timestamp.type","value":"CreateTime"},{"name":"min.insync.replicas","value":"1"},{"name":"retention.ms","value":"259200000"},{"name":"segment.ms","value":"14400000"},{"name":"retention.bytes","value":"-1"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"responseData":"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","errorCode":0},"connection":"127.0.0.1:29093-127.0.0.1:34176-2-0","clientAddress":"127.0.0.1","totalTimeMs":31.856,"requestQueueTimeMs":0.077,"localTimeMs":0.199,"remoteTimeMs":31.216,"throttleTimeMs":0,"responseQueueTimeMs":0.147,"sendTimeMs":0.216,"sendIoTimeMs":0.169,"responseSize":2547,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":175057005947300002} (kafka.request.logger:%L)
[2025-06-22 05:27:39,506] INFO Created telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter:%L)
[2025-06-22 05:27:39,506] INFO Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"confluent-telemetry-reporter-local-producer","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-telemetry-metrics","numPartitions":12,"replicationFactor":1,"assignments":[],"configs":[{"name":"max.message.bytes","value":"10485760"},{"name":"message.timestamp.type","value":"CreateTime"},{"name":"min.insync.replicas","value":"1"},{"name":"retention.ms","value":"259200000"},{"name":"segment.ms","value":"14400000"},{"name":"retention.bytes","value":"-1"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-telemetry-metrics","topicId":"WJs2_1GlS2iUCcZR9G1uRw","errorCode":0,"errorMessage":null,"numPartitions":12,"replicationFactor":1,"configs":[{"configName":"cleanup.policy","value":"delete","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.gzip.level","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.lz4.level","value":"9","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.type","value":"producer","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.zstd.level","value":"3","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.append.record.interceptor.classes","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.cluster.link.allow.legacy.message.format","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.compacted.topic.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.log.cleaner.timestamp.validation.enable","value":"true","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.max.segment.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.min.segment.ms","value":"1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.placement.constraints","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.schema.validation.context.name.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.segment.speculative.prefetch.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.delete.delay.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.max.deletions.per.run","value":"72","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.system.time.roll.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.min.efficiency","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.segment.min.bytes","value":"20971520","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.dual.compaction","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.min.cleanable.ratio","value":"0.75","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.bytes","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.segment.hotset.roll.min.bytes","value":"104857600","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.topic.type","value":"standard","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"delete.retention.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"file.delete.delay.ms","value":"60000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.messages","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"follower.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"index.interval.bytes","value":"4096","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"leader.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.bytes","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.ms","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.compaction.lag.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.message.bytes","value":"10485760","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"message.timestamp.after.max.ms","value":"3600000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.before.max.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.type","value":"CreateTime","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"min.cleanable.dirty.ratio","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.compaction.lag.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.insync.replicas","value":"1","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"preallocate","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.copy.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.delete.on.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.storage.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"retention.bytes","value":"-1","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"retention.ms","value":"259200000","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"segment.bytes","value":"1073741824","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.index.bytes","value":"10485760","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.jitter.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.ms","value":"14400000","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"unclean.leader.election.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false}]}]},"connection":"127.0.0.1:29092-127.0.0.1:60732-1-5","clientAddress":"127.0.0.1","totalTimeMs":33.626,"requestQueueTimeMs":0.091,"localTimeMs":0.503,"remoteTimeMs":32.793,"throttleTimeMs":0,"responseQueueTimeMs":0.088,"sendTimeMs":0.148,"sendIoTimeMs":0.108,"responseSize":2537,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"8.0.0-0-ce"},"isDisconnectedClient":false,"requestId":175057005947200001} (kafka.request.logger:%L)
[2025-06-22 05:27:39,506] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:27:39,507] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-4 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,508] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-5 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,509] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-6 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,510] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-7 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,510] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-8 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,511] INFO Partitioner has null list of partitions to produce to. Calculating partitions to produce to (io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner:%L)
[2025-06-22 05:27:39,511] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-9 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,511] INFO Kafka Producer producing to the following subset partitions: {_confluent-telemetry-metrics=[5, 11]} (io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner:%L)
[2025-06-22 05:27:39,512] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-10 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,512] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-11 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,513] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-0 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,514] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-1 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,515] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-2 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,516] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 12 partitions (state.change.logger:%L)
[2025-06-22 05:27:39,519] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,520] INFO Created log for partition _confluent-telemetry-metrics-7 in /var/lib/kafka/data/_confluent-telemetry-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,521] INFO [Partition _confluent-telemetry-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-7 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,521] INFO [Partition _confluent-telemetry-metrics-7 broker=1] Log loaded for partition _confluent-telemetry-metrics-7 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,521] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-7 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,521] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,521] INFO [Broker id=1] Leader _confluent-telemetry-metrics-7 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,524] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,524] INFO Created log for partition _confluent-telemetry-metrics-0 in /var/lib/kafka/data/_confluent-telemetry-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,524] INFO [Partition _confluent-telemetry-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,524] INFO [Partition _confluent-telemetry-metrics-0 broker=1] Log loaded for partition _confluent-telemetry-metrics-0 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,524] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-0 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,524] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,525] INFO [Broker id=1] Leader _confluent-telemetry-metrics-0 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,527] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,528] INFO Created log for partition _confluent-telemetry-metrics-1 in /var/lib/kafka/data/_confluent-telemetry-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,528] INFO [Partition _confluent-telemetry-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-1 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,528] INFO [Partition _confluent-telemetry-metrics-1 broker=1] Log loaded for partition _confluent-telemetry-metrics-1 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,528] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-1 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,528] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,528] INFO [Broker id=1] Leader _confluent-telemetry-metrics-1 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,530] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,531] INFO Created log for partition _confluent-telemetry-metrics-5 in /var/lib/kafka/data/_confluent-telemetry-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,531] INFO [Partition _confluent-telemetry-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-5 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,531] INFO [Partition _confluent-telemetry-metrics-5 broker=1] Log loaded for partition _confluent-telemetry-metrics-5 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,531] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-5 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,531] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,531] INFO [Broker id=1] Leader _confluent-telemetry-metrics-5 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,533] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,534] INFO Created log for partition _confluent-telemetry-metrics-11 in /var/lib/kafka/data/_confluent-telemetry-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,534] INFO [Partition _confluent-telemetry-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-11 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,534] INFO [Partition _confluent-telemetry-metrics-11 broker=1] Log loaded for partition _confluent-telemetry-metrics-11 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,534] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-11 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,534] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,534] INFO [Broker id=1] Leader _confluent-telemetry-metrics-11 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,536] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,537] INFO Created log for partition _confluent-telemetry-metrics-9 in /var/lib/kafka/data/_confluent-telemetry-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,537] INFO [Partition _confluent-telemetry-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-9 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,537] INFO [Partition _confluent-telemetry-metrics-9 broker=1] Log loaded for partition _confluent-telemetry-metrics-9 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,537] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-9 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,537] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,537] INFO [Broker id=1] Leader _confluent-telemetry-metrics-9 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,539] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,539] INFO Created log for partition _confluent-telemetry-metrics-3 in /var/lib/kafka/data/_confluent-telemetry-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,539] INFO [Partition _confluent-telemetry-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-3 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,539] INFO [Partition _confluent-telemetry-metrics-3 broker=1] Log loaded for partition _confluent-telemetry-metrics-3 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,539] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-3 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,539] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,539] INFO [Broker id=1] Leader _confluent-telemetry-metrics-3 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,541] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,541] INFO Created log for partition _confluent-telemetry-metrics-8 in /var/lib/kafka/data/_confluent-telemetry-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,542] INFO [Partition _confluent-telemetry-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-8 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,542] INFO [Partition _confluent-telemetry-metrics-8 broker=1] Log loaded for partition _confluent-telemetry-metrics-8 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,542] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-8 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,542] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,542] INFO [Broker id=1] Leader _confluent-telemetry-metrics-8 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,544] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,545] INFO Created log for partition _confluent-telemetry-metrics-2 in /var/lib/kafka/data/_confluent-telemetry-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,545] INFO [Partition _confluent-telemetry-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-2 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,545] INFO [Partition _confluent-telemetry-metrics-2 broker=1] Log loaded for partition _confluent-telemetry-metrics-2 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,545] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-2 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,545] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,545] INFO [Broker id=1] Leader _confluent-telemetry-metrics-2 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,547] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,548] INFO Created log for partition _confluent-telemetry-metrics-6 in /var/lib/kafka/data/_confluent-telemetry-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,548] INFO [Partition _confluent-telemetry-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-6 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,548] INFO [Partition _confluent-telemetry-metrics-6 broker=1] Log loaded for partition _confluent-telemetry-metrics-6 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,548] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-6 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,548] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,548] INFO [Broker id=1] Leader _confluent-telemetry-metrics-6 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,549] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,550] INFO Created log for partition _confluent-telemetry-metrics-10 in /var/lib/kafka/data/_confluent-telemetry-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,550] INFO [Partition _confluent-telemetry-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-10 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,550] INFO [Partition _confluent-telemetry-metrics-10 broker=1] Log loaded for partition _confluent-telemetry-metrics-10 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,550] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-10 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,550] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,550] INFO [Broker id=1] Leader _confluent-telemetry-metrics-10 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,552] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,552] INFO Created log for partition _confluent-telemetry-metrics-4 in /var/lib/kafka/data/_confluent-telemetry-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,552] INFO [Partition _confluent-telemetry-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-4 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,552] INFO [Partition _confluent-telemetry-metrics-4 broker=1] Log loaded for partition _confluent-telemetry-metrics-4 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,552] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-4 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,553] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,553] INFO [Broker id=1] Leader _confluent-telemetry-metrics-4 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,554] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-telemetry-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher:%L)
[2025-06-22 05:27:39,574] INFO [Partition _confluent-telemetry-metrics-5 broker=1] roll: _confluent-telemetry-metrics-5: first produce received, lastOffset: 24, leaderEpoch: 0, numMessages:25, time diff: 57 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,574] INFO [Partition _confluent-telemetry-metrics-5 broker=1] roll: _confluent-telemetry-metrics-5: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 25, become leader time: 1750570059516 ms, time diff: 58 ms (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,574] INFO [Partition _confluent-telemetry-metrics-11 broker=1] roll: _confluent-telemetry-metrics-11: first produce received, lastOffset: 19, leaderEpoch: 0, numMessages:20, time diff: 58 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,574] INFO [Partition _confluent-telemetry-metrics-11 broker=1] roll: _confluent-telemetry-metrics-11: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 20, become leader time: 1750570059516 ms, time diff: 58 ms (kafka.cluster.Partition:%L)
[2025-06-22 05:28:38,955] INFO [ControllerServer id=1] In the last 60000 ms period, 326 controller events were completed, which took an average of 10.68 ms each. The slowest event was writeNoOpRecord(1613906694), which took 42.39 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:28:38,959] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:29:38,961] INFO [ControllerServer id=1] In the last 60000 ms period, 322 controller events were completed, which took an average of 10.77 ms each. The slowest event was writeNoOpRecord(475995535), which took 37.67 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:29:38,961] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
➜  confluent-server-oauth git:(cp-server-native) ✗ docker ps          
CONTAINER ID   IMAGE                                     COMMAND                  CREATED         STATUS                     PORTS                                                                                            NAMES
8b4893672d2f   cp-server-native-local-80x-oauth:latest   "bash -c 'if [ ! -f …"   7 minutes ago   Up 6 minutes (unhealthy)   0.0.0.0:8091->8091/tcp, 0.0.0.0:9092->9092/tcp, 0.0.0.0:9095->9095/tcp, 0.0.0.0:9101->9101/tcp   broker1
aff851a23883   quay.io/keycloak/keycloak:23.0.5          "/opt/keycloak/bin/k…"   7 minutes ago   Up 7 minutes (healthy)     0.0.0.0:8080->8080/tcp, 8443/tcp                                                                 keycloak
➜  confluent-server-oauth git:(cp-server-native) ✗ docker logs broker1
Generate keys and certificates used for MDS
writing RSA key
===> User
/etc/confluent/docker/run: line 17: /etc/confluent/docker/bash-config: No such file or directory
uid=0(root) gid=0(root) groups=0(root)
===> Configuring ...
/etc/confluent/docker/configure: line 17: /etc/confluent/docker/bash-config: No such file or directory
Running in KRaft mode...
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
####################################
kafka.properties
log4j2.yaml
secrets
server.properties
tools-log4j2.yaml
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
===> Running preflight checks ... 
/etc/confluent/docker/ensure: line 17: /etc/confluent/docker/bash-config: No such file or directory
===> Check if /var/lib/kafka/data is writable ...
/usr/bin/python3: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
===> Running in KRaft mode, skipping Zookeeper health check...
===> Using provided cluster id vHCgQyIrRHG8Jv27qI2h3Q ...
grep: error while loading shared libraries: libpcre.so.1: cannot open shared object file: No such file or directory
/usr/bin/kafka-run-class: line 450: exec: java: not found
===> Done kafka-storage
===> Launching ... 
===> Launching kafka... 
===> Kafka start...
2025-06-22T05:26:38.686644Z main ERROR Class class org.apache.logging.log4j.core.pattern.LineLocationPatternConverter does not contain a static newInstance method
2025-06-22T05:26:38.687549Z main ERROR Unrecognized conversion specifier [L] starting at position 17 in conversion pattern.
[2025-06-22 05:26:38,699] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$:%L)
[2025-06-22 05:26:38,751] INFO KafkaConfig values: 
	add.partitions.to.txn.retry.backoff.max.ms = 100
	add.partitions.to.txn.retry.backoff.ms = 20
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = f4iDkhvfTGqMLnNPxFcjWw
	client.quota.callback.class = null
	client.quota.max.throttle.time.in.response.ms = 60000
	client.quota.max.throttle.time.ms = 5000
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	confluent.accp.enabled = false
	confluent.acks.equal.to.one.request.replication.lag.threshold.ms = -1
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.api.visibility = DEFAULT
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
	confluent.balancer.capacity.threshold.upper.limit = 0.95
	confluent.balancer.cell.load.upper.bound = 0.7
	confluent.balancer.cell.overload.detection.interval.ms = 3600000
	confluent.balancer.cell.overload.duration.ms = 86400000
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.cpu.balance.threshold = 1.1
	confluent.balancer.cpu.goal.act.as.capacity.goal = false
	confluent.balancer.cpu.low.utilization.threshold = 0.2
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.enable.network.capacity.metric.ingestion = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.flex.fanout.network.capacity.metrics.avg.period.ms = 1800000
	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.min.valid.windows = 5
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.inter.cell.balancing.enabled = false
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.minimum.reported.brokers.with.network.capacity.metrics.percentage = 0.8
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.num.concurrent.replica.movements.as.destination.per.broker = 18
	confluent.balancer.num.concurrent.replica.movements.as.source.per.broker = 12
	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.sbc.metrics.parser.enabled = false
	confluent.balancer.self.healing.maximum.rounds = 1
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.tenant.maximum.movements = 0
	confluent.balancer.tenant.suspension.ms = 86400000
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.balancing.itrdg.with.hard.goals.enabled = false
	confluent.balancer.topic.partition.maximum.movements = 3
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.movements.history.limit = 900
	confluent.balancer.topic.partition.suspension.ms = 3600000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.balancer.v2.addition.enabled = false
	confluent.balancer.v2.addition.reassignment.cancellations.enabled = false
	confluent.balancer.v2.executor.enabled = false
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.assertion.claim.aud = null
	confluent.bearer.assertion.claim.exp.minutes = null
	confluent.bearer.assertion.claim.iss = null
	confluent.bearer.assertion.claim.jti.include = null
	confluent.bearer.assertion.claim.nbf.include = null
	confluent.bearer.assertion.claim.sub = null
	confluent.bearer.assertion.file = null
	confluent.bearer.assertion.private.key.file = null
	confluent.bearer.assertion.private.key.passphrase = null
	confluent.bearer.assertion.template.file = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.advertised.limit.load = 0.8
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.tenant.metric.enable = false
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.calling.resource.identity.type.map = 
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.full.configs.enable = false
	confluent.catalog.collector.max.bytes.per.snapshot = 850000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.multitenant.topics.enable = true
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cell.metrics.refresh.period.ms = 60000
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.k2.base.broker.index = -1
	confluent.cells.load.refresher.enable = true
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.client.topic.max.metrics.count = 1000
	confluent.client.topic.metrics.expiry.sec = 3600
	confluent.client.topic.metrics.manager = class org.apache.kafka.server.metrics.ClientTopicMetricsManager$NoOpClientTopicMetricsManager
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.list.object.thread_pool.size = 1
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.admin.max.in.flight.requests = 1000
	confluent.cluster.link.admin.request.batch.size = 1
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.allow.legacy.message.format = false
	confluent.cluster.link.allow.truncation.below.hwm = false
	confluent.cluster.link.availability.check.mode = ALL
	confluent.cluster.link.background.thread.affinity = LINK
	confluent.cluster.link.bootstrap.translation.feature.enable = true
	confluent.cluster.link.clients.max.idle.ms = 3153600000000
	confluent.cluster.link.enable = true
	confluent.cluster.link.enable.local.admin = false
	confluent.cluster.link.enable.metrics.reduction = false
	confluent.cluster.link.enable.metrics.reduction.advanced = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.fetcher.auto.tune.enable = false
	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
	confluent.cluster.link.insync.fetch.response.min.bytes = 1
	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.intranet.connectivity.denied.org.ids = []
	confluent.cluster.link.intranet.connectivity.enable = false
	confluent.cluster.link.intranet.connectivity.migration.enable = false
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.local.admin.multitenant.enable = false
	confluent.cluster.link.local.reverse.connection.listener.map = null
	confluent.cluster.link.max.client.connections = 2147483647
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.mirror.transition.batch.size = 10
	confluent.cluster.link.num.background.threads = 1
	confluent.cluster.link.periodic.task.batch.size = 2147483647
	confluent.cluster.link.periodic.task.min.interval.ms = 1000
	confluent.cluster.link.persistent.connection.backoff.max.ms = 0
	confluent.cluster.link.replica.fetch.connections.mode = combined
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.cluster.link.request.quota.capacity = 400
	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
	confluent.cluster.link.switchover.disabled.principals = []
	confluent.cluster.link.switchover.enable = false
	confluent.cluster.link.switchover.listeners = []
	confluent.cluster.link.switchover.server.states = []
	confluent.cluster.link.tenant.replication.quota.enable = false
	confluent.cluster.link.tenant.request.quota.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
	confluent.cluster.metadata.snapshot.tier.upload.enable = false
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
	confluent.consumer.fetch.partition.pruning.enable = true
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.dataflow.policy.watch.monitor.ms = 300000
	confluent.default.data.policy.enforcement = true
	confluent.defer.isr.shrink.enable = false
	confluent.describe.topic.partitions.enabled = true
	confluent.disk.io.manager.enable = false
	confluent.disk.throughput.headroom = 10485760
	confluent.disk.throughput.limit = 10485760000
	confluent.disk.throughput.quota.tier.archive = 1048576000
	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.log.ignored.event.types = 
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 1
	confluent.e2e_checksum.protection.enabled = false
	confluent.e2e_checksum.protection.files = [none]
	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
	confluent.elastic.cku.enabled = false
	confluent.elastic.cku.scaletozero.enabled = false
	confluent.eligible.controllers = []
	confluent.enable.broker.reporting.min.usage.mode = true
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fail.unsatisfied.placement.constraints = false
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.flexible.fanout.broker.max.fetch.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.max.produce.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.min.producer.percentage = 10.0
	confluent.flexible.fanout.broker.network.out.bytes.per.second = 6200000
	confluent.flexible.fanout.broker.recompute.interval.ms = 30000
	confluent.flexible.fanout.broker.storage.bytes.per.second = 512000000
	confluent.flexible.fanout.enabled = false
	confluent.flexible.fanout.lazy.evaluation.threshold = 0.5
	confluent.flexible.fanout.mode = TENANT_QUOTA
	confluent.floor.connection.rate.per.ip = -1.0
	confluent.floor.connection.rate.per.tenant = -1.0
	confluent.group.coordinator.dynamic.append.linger.enable = false
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.coordinator.txn.offset.validation.enable = false
	confluent.group.highest.offset.commit.rates.log.count = 10
	confluent.group.highest.offset.commit.rates.log.enable = false
	confluent.group.highest.offset.commit.rates.log.interval.ms = 300000
	confluent.group.metadata.load.threads = 32
	confluent.group.subscription.pattern.log.interval.ms = -1
	confluent.heap.tenured.notify.bytes = 0
	confluent.heap.tenured.notify.enabled = false
	confluent.hot.partition.ratio = 0.8
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.internal.rest.server.ssl.enable = false
	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
	confluent.leader.epoch.checkpoint.checksum.enabled = false
	confluent.listener.protocol = TCP
	confluent.log.cleaner.timestamp.validation.enable = true
	confluent.log.placement.constraints = 
	confluent.max.broker.load = 1.0
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
	confluent.max.connection.rate.per.ip = -1.0
	confluent.max.connection.rate.per.tenant = -1.0
	confluent.max.connection.throttle.ms = null
	confluent.max.segment.ms = 9223372036854775807
	confluent.metadata.active.encryptor = null
	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.required = false
	confluent.metadata.encryptor.secret.file = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.jvm.warmup.ms = 60000
	confluent.metadata.leader.balance.slice.delay.ms = 100
	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
	confluent.metadata.max.leader.balance.changes.per.slice = 1000
	confluent.metadata.reject.when.throttled.enable = false
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.connection.throttle.ms = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 20000
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.mtls.build.client.cert.chain.enable = false
	confluent.mtls.enable = false
	confluent.mtls.listener.name = EXTERNAL
	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
	confluent.mtls.truststore.alter.configs.timeout.ms = 300000
	confluent.mtls.truststore.manager.class.name = null
	confluent.multitenant.authorizer.enable.acl.state = false
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.lkc.id.enable = false
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.external.listener.name = EXTERNAL
	confluent.network.health.manager.externalconnectivitystartup.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.oauth.flat.networking.verification.enable = false
	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
	confluent.offsets.topic.placement.constraints = 
	confluent.omit.network.processor.metric.tag = false
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.ppv2.endpoint.scheme.bootstrap.broker.template.mappings = 
	confluent.ppv2.endpoint.scheme.enable = false
	confluent.ppv2.endpoint.scheme.map.broker.zone.to.gateway.zone = false
	confluent.ppv2.endpoint.scheme.template.variable.cloud = 
	confluent.ppv2.endpoint.scheme.template.variable.domain = 
	confluent.ppv2.endpoint.scheme.template.variable.region = 
	confluent.ppv2.endpoint.scheme.template.variables = 
	confluent.ppv2.endpoint.scheme.templates = 
	confluent.prefer.tier.fetch.ms = -1
	confluent.produce.throttle.pre.check.enable = false
	confluent.produce.throttle.pre.check.for.new.connection.enable = false
	confluent.producer.id.cache.broker.hard.limit = -1
	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
	confluent.producer.id.cache.extra.eviction.percentage = 0
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.cache.partition.hard.limit = -1
	confluent.producer.id.cache.tenant.hard.limit = -1
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.quota.window.num = 11
	confluent.producer.id.quota.window.size.seconds = 1
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.mode.local.default = false
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.computing.usage.adjustment = 0.5
	confluent.quota.dynamic.adjustment.min.usage = 102400
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.internal.broker.max.consumer.rate = 9223372036854775807
	confluent.quota.tenant.internal.broker.max.producer.rate = 9223372036854775807
	confluent.quota.tenant.internal.throttling.enable = false
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.rack.id.mapping = null
	confluent.regional.metadata.client.class = null
	confluent.regional.resource.manager.client.scheduler.threads = 2
	confluent.regional.resource.manager.endpoint = null
	confluent.regional.resource.manager.watch.endpoint = null
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.replication.mode = PULL
	confluent.replication.push.feature.enable = false
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = true
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.calling.resource.identity = false
	confluent.require.compatible.keystore.updates = true
	confluent.require.confluent.issuer = false
	confluent.roll.check.interval.ms = 300000
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validation.context.name.enable = false
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.bc.approved.mode.enable = false
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.config = 
	confluent.security.revoked.certificate.ids = 
	confluent.segment.eager.roll.enable = false
	confluent.segment.speculative.prefetch.enable = false
	confluent.share.metadata.load.threads = 32
	confluent.spiffe.id.principal.extraction.rules = 
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.step.connection.rate.per.ip = -1.0
	confluent.step.connection.rate.per.tenant = -1.0
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.subdomain.prefix = null
	confluent.subdomain.separator.map = null
	confluent.subdomain.separator.variable = %sep
	confluent.system.time.roll.enable = false
	confluent.telemetry.enabled = false
	confluent.telemetry.external.client.metrics.delta.temporality = true
	confluent.telemetry.external.client.metrics.instance.cache.size = 16384
	confluent.telemetry.external.client.metrics.push.enabled = false
	confluent.telemetry.external.client.metrics.subscription.interval.ms.list = null
	confluent.telemetry.external.client.metrics.subscription.match.list = null
	confluent.telemetry.external.client.metrics.subscription.metrics.list = null
	confluent.tenant.latency.metric.enabled = false
	confluent.tenantaware.encryption.key.manager.enable = false
	confluent.tenantaware.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.tenantaware.encryption.key.manager.tenant.cache.eviction.time.sec = 172800
	confluent.tenantaware.encryption.key.manager.tenant.cache.size = 100
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.async.enable = false
	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 1
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.prefetch.cache.enable = false
	confluent.tier.prefetch.cache.entry.size.bytes = 1048576
	confluent.tier.prefetch.cache.range.bytes = 5242880
	confluent.tier.prefetch.cache.total.size.bytes = 209715200
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.ipv6.enabled = true
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.security.providers = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.provider = null
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.storage.class.override = 
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.s3.v2.enabled = false
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
	confluent.tier.topic.data.loss.validation.fencing.enable = false
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.head.data.loss.validation.enable = true
	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
	confluent.tier.topic.materialization.from.snapshot.enable = false
	confluent.tier.topic.producer.enable.idempotence = true
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.metadata.throttle.pre.check.partition.count.threshold = 1000
	confluent.topic.partition.default.placement = 2
	confluent.topic.policy.use.computed.assignments = false
	confluent.topic.replica.assignor.builder.class = 
	confluent.track.api.key.per.ip = false
	confluent.track.per.ip.max.size = 100000
	confluent.track.tenant.id.per.ip = false
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.traffic.network.type = 
	confluent.transaction.2pc.timeout.ms = -1
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.unique.deprecated.request.metrics.per.tenant = 1000
	confluent.valid.broker.rack.set = null
	confluent.valid.sni.hostnames = 
	confluent.valid.sni.hostnames.exclude.suffix = 
	confluent.verify.group.subscription.prefix = false
	confluent.virtual.topic.creation.enabled = false
	confluent.zone.tagged.request.metrics.enable = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	controlled.shutdown.enable = true
	controller.listener.names = CONTROLLER
	controller.performance.always.log.threshold.ms = 2000
	controller.performance.sample.period.ms = 60000
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.consumer.assignors = [uniform, range]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = bidirectional
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 5
	group.coordinator.new.enable = true
	group.coordinator.rebalance.protocols = [classic, consumer]
	group.coordinator.threads = 4
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	group.share.delivery.count.limit = 5
	group.share.enable = false
	group.share.heartbeat.interval.ms = 5000
	group.share.max.groups = 10
	group.share.max.heartbeat.interval.ms = 15000
	group.share.max.record.lock.duration.ms = 60000
	group.share.max.session.timeout.ms = 60000
	group.share.max.size = 200
	group.share.min.heartbeat.interval.ms = 5000
	group.share.min.record.lock.duration.ms = 15000
	group.share.min.session.timeout.ms = 45000
	group.share.partition.max.record.locks = 200
	group.share.persister.class.name = org.apache.kafka.server.share.persister.DefaultStatePersister
	group.share.record.lock.duration.ms = 30000
	group.share.session.timeout.ms = 45000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	k2.stack.builder.class.name = null
	k2.startup.timeout.ms = 60000
	k2.topic.metadata.refresh.ms = 10000
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.cleanup.policy.empty.validation = none
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.timestamp.after.max.ms = 3600000
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.connections.per.tenant = 0
	max.connections.protected.listeners = []
	max.connections.reap.amount = 0
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.authorizer.support.resource.ids = false
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 2
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	otel.exporter.otlp.custom.endpoint = default
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.fetch.max.wait.ms = 500
	remote.list.offsets.request.timeout.ms = 30000
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 2
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	share.coordinator.append.linger.ms = 5
	share.coordinator.load.buffer.size = 5242880
	share.coordinator.snapshot.update.records.per.snapshot = 500
	share.coordinator.state.topic.compression.codec = 0
	share.coordinator.state.topic.min.isr = 2
	share.coordinator.state.topic.num.partitions = 50
	share.coordinator.state.topic.prune.interval.ms = 300000
	share.coordinator.state.topic.replication.factor = 3
	share.coordinator.state.topic.segment.bytes = 104857600
	share.coordinator.threads = 1
	share.coordinator.write.timeout.ms = 5000
	share.fetch.max.fetch.records = 2147483647
	share.fetch.purgatory.purge.interval.requests = 1000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	throughput.quota.window.num = 11
	token.impersonation.validation = true
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.metadata.load.threads = 32
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unclean.leader.election.interval.ms = 300000
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,758] INFO KafkaConfig values: 
	add.partitions.to.txn.retry.backoff.max.ms = 100
	add.partitions.to.txn.retry.backoff.ms = 20
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = f4iDkhvfTGqMLnNPxFcjWw
	client.quota.callback.class = null
	client.quota.max.throttle.time.in.response.ms = 60000
	client.quota.max.throttle.time.ms = 5000
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	confluent.accp.enabled = false
	confluent.acks.equal.to.one.request.replication.lag.threshold.ms = -1
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.api.visibility = DEFAULT
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
	confluent.balancer.capacity.threshold.upper.limit = 0.95
	confluent.balancer.cell.load.upper.bound = 0.7
	confluent.balancer.cell.overload.detection.interval.ms = 3600000
	confluent.balancer.cell.overload.duration.ms = 86400000
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.cpu.balance.threshold = 1.1
	confluent.balancer.cpu.goal.act.as.capacity.goal = false
	confluent.balancer.cpu.low.utilization.threshold = 0.2
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.enable.network.capacity.metric.ingestion = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.flex.fanout.network.capacity.metrics.avg.period.ms = 1800000
	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.min.valid.windows = 5
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.inter.cell.balancing.enabled = false
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.minimum.reported.brokers.with.network.capacity.metrics.percentage = 0.8
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.num.concurrent.replica.movements.as.destination.per.broker = 18
	confluent.balancer.num.concurrent.replica.movements.as.source.per.broker = 12
	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.sbc.metrics.parser.enabled = false
	confluent.balancer.self.healing.maximum.rounds = 1
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.tenant.maximum.movements = 0
	confluent.balancer.tenant.suspension.ms = 86400000
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.balancing.itrdg.with.hard.goals.enabled = false
	confluent.balancer.topic.partition.maximum.movements = 3
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.movements.history.limit = 900
	confluent.balancer.topic.partition.suspension.ms = 3600000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.balancer.v2.addition.enabled = false
	confluent.balancer.v2.addition.reassignment.cancellations.enabled = false
	confluent.balancer.v2.executor.enabled = false
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.assertion.claim.aud = null
	confluent.bearer.assertion.claim.exp.minutes = null
	confluent.bearer.assertion.claim.iss = null
	confluent.bearer.assertion.claim.jti.include = null
	confluent.bearer.assertion.claim.nbf.include = null
	confluent.bearer.assertion.claim.sub = null
	confluent.bearer.assertion.file = null
	confluent.bearer.assertion.private.key.file = null
	confluent.bearer.assertion.private.key.passphrase = null
	confluent.bearer.assertion.template.file = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.advertised.limit.load = 0.8
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.tenant.metric.enable = false
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.calling.resource.identity.type.map = 
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.full.configs.enable = false
	confluent.catalog.collector.max.bytes.per.snapshot = 850000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.multitenant.topics.enable = true
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cell.metrics.refresh.period.ms = 60000
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.k2.base.broker.index = -1
	confluent.cells.load.refresher.enable = true
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.client.topic.max.metrics.count = 1000
	confluent.client.topic.metrics.expiry.sec = 3600
	confluent.client.topic.metrics.manager = class org.apache.kafka.server.metrics.ClientTopicMetricsManager$NoOpClientTopicMetricsManager
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.list.object.thread_pool.size = 1
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.admin.max.in.flight.requests = 1000
	confluent.cluster.link.admin.request.batch.size = 1
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.allow.legacy.message.format = false
	confluent.cluster.link.allow.truncation.below.hwm = false
	confluent.cluster.link.availability.check.mode = ALL
	confluent.cluster.link.background.thread.affinity = LINK
	confluent.cluster.link.bootstrap.translation.feature.enable = true
	confluent.cluster.link.clients.max.idle.ms = 3153600000000
	confluent.cluster.link.enable = true
	confluent.cluster.link.enable.local.admin = false
	confluent.cluster.link.enable.metrics.reduction = false
	confluent.cluster.link.enable.metrics.reduction.advanced = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.fetcher.auto.tune.enable = false
	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
	confluent.cluster.link.insync.fetch.response.min.bytes = 1
	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.intranet.connectivity.denied.org.ids = []
	confluent.cluster.link.intranet.connectivity.enable = false
	confluent.cluster.link.intranet.connectivity.migration.enable = false
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.local.admin.multitenant.enable = false
	confluent.cluster.link.local.reverse.connection.listener.map = null
	confluent.cluster.link.max.client.connections = 2147483647
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.mirror.transition.batch.size = 10
	confluent.cluster.link.num.background.threads = 1
	confluent.cluster.link.periodic.task.batch.size = 2147483647
	confluent.cluster.link.periodic.task.min.interval.ms = 1000
	confluent.cluster.link.persistent.connection.backoff.max.ms = 0
	confluent.cluster.link.replica.fetch.connections.mode = combined
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.cluster.link.request.quota.capacity = 400
	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
	confluent.cluster.link.switchover.disabled.principals = []
	confluent.cluster.link.switchover.enable = false
	confluent.cluster.link.switchover.listeners = []
	confluent.cluster.link.switchover.server.states = []
	confluent.cluster.link.tenant.replication.quota.enable = false
	confluent.cluster.link.tenant.request.quota.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
	confluent.cluster.metadata.snapshot.tier.upload.enable = false
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
	confluent.consumer.fetch.partition.pruning.enable = true
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.dataflow.policy.watch.monitor.ms = 300000
	confluent.default.data.policy.enforcement = true
	confluent.defer.isr.shrink.enable = false
	confluent.describe.topic.partitions.enabled = true
	confluent.disk.io.manager.enable = false
	confluent.disk.throughput.headroom = 10485760
	confluent.disk.throughput.limit = 10485760000
	confluent.disk.throughput.quota.tier.archive = 1048576000
	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.log.ignored.event.types = 
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 1
	confluent.e2e_checksum.protection.enabled = false
	confluent.e2e_checksum.protection.files = [none]
	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
	confluent.elastic.cku.enabled = false
	confluent.elastic.cku.scaletozero.enabled = false
	confluent.eligible.controllers = []
	confluent.enable.broker.reporting.min.usage.mode = true
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fail.unsatisfied.placement.constraints = false
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.flexible.fanout.broker.max.fetch.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.max.produce.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.min.producer.percentage = 10.0
	confluent.flexible.fanout.broker.network.out.bytes.per.second = 6200000
	confluent.flexible.fanout.broker.recompute.interval.ms = 30000
	confluent.flexible.fanout.broker.storage.bytes.per.second = 512000000
	confluent.flexible.fanout.enabled = false
	confluent.flexible.fanout.lazy.evaluation.threshold = 0.5
	confluent.flexible.fanout.mode = TENANT_QUOTA
	confluent.floor.connection.rate.per.ip = -1.0
	confluent.floor.connection.rate.per.tenant = -1.0
	confluent.group.coordinator.dynamic.append.linger.enable = false
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.coordinator.txn.offset.validation.enable = false
	confluent.group.highest.offset.commit.rates.log.count = 10
	confluent.group.highest.offset.commit.rates.log.enable = false
	confluent.group.highest.offset.commit.rates.log.interval.ms = 300000
	confluent.group.metadata.load.threads = 32
	confluent.group.subscription.pattern.log.interval.ms = -1
	confluent.heap.tenured.notify.bytes = 0
	confluent.heap.tenured.notify.enabled = false
	confluent.hot.partition.ratio = 0.8
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.internal.rest.server.ssl.enable = false
	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
	confluent.leader.epoch.checkpoint.checksum.enabled = false
	confluent.listener.protocol = TCP
	confluent.log.cleaner.timestamp.validation.enable = true
	confluent.log.placement.constraints = 
	confluent.max.broker.load = 1.0
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
	confluent.max.connection.rate.per.ip = -1.0
	confluent.max.connection.rate.per.tenant = -1.0
	confluent.max.connection.throttle.ms = null
	confluent.max.segment.ms = 9223372036854775807
	confluent.metadata.active.encryptor = null
	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.required = false
	confluent.metadata.encryptor.secret.file = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.jvm.warmup.ms = 60000
	confluent.metadata.leader.balance.slice.delay.ms = 100
	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
	confluent.metadata.max.leader.balance.changes.per.slice = 1000
	confluent.metadata.reject.when.throttled.enable = false
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.connection.throttle.ms = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 20000
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.mtls.build.client.cert.chain.enable = false
	confluent.mtls.enable = false
	confluent.mtls.listener.name = EXTERNAL
	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
	confluent.mtls.truststore.alter.configs.timeout.ms = 300000
	confluent.mtls.truststore.manager.class.name = null
	confluent.multitenant.authorizer.enable.acl.state = false
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.lkc.id.enable = false
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.external.listener.name = EXTERNAL
	confluent.network.health.manager.externalconnectivitystartup.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.oauth.flat.networking.verification.enable = false
	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
	confluent.offsets.topic.placement.constraints = 
	confluent.omit.network.processor.metric.tag = false
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.ppv2.endpoint.scheme.bootstrap.broker.template.mappings = 
	confluent.ppv2.endpoint.scheme.enable = false
	confluent.ppv2.endpoint.scheme.map.broker.zone.to.gateway.zone = false
	confluent.ppv2.endpoint.scheme.template.variable.cloud = 
	confluent.ppv2.endpoint.scheme.template.variable.domain = 
	confluent.ppv2.endpoint.scheme.template.variable.region = 
	confluent.ppv2.endpoint.scheme.template.variables = 
	confluent.ppv2.endpoint.scheme.templates = 
	confluent.prefer.tier.fetch.ms = -1
	confluent.produce.throttle.pre.check.enable = false
	confluent.produce.throttle.pre.check.for.new.connection.enable = false
	confluent.producer.id.cache.broker.hard.limit = -1
	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
	confluent.producer.id.cache.extra.eviction.percentage = 0
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.cache.partition.hard.limit = -1
	confluent.producer.id.cache.tenant.hard.limit = -1
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.quota.window.num = 11
	confluent.producer.id.quota.window.size.seconds = 1
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.mode.local.default = false
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.computing.usage.adjustment = 0.5
	confluent.quota.dynamic.adjustment.min.usage = 102400
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.internal.broker.max.consumer.rate = 9223372036854775807
	confluent.quota.tenant.internal.broker.max.producer.rate = 9223372036854775807
	confluent.quota.tenant.internal.throttling.enable = false
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.rack.id.mapping = null
	confluent.regional.metadata.client.class = null
	confluent.regional.resource.manager.client.scheduler.threads = 2
	confluent.regional.resource.manager.endpoint = null
	confluent.regional.resource.manager.watch.endpoint = null
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.replication.mode = PULL
	confluent.replication.push.feature.enable = false
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = true
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.calling.resource.identity = false
	confluent.require.compatible.keystore.updates = true
	confluent.require.confluent.issuer = false
	confluent.roll.check.interval.ms = 300000
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validation.context.name.enable = false
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.bc.approved.mode.enable = false
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.config = 
	confluent.security.revoked.certificate.ids = 
	confluent.segment.eager.roll.enable = false
	confluent.segment.speculative.prefetch.enable = false
	confluent.share.metadata.load.threads = 32
	confluent.spiffe.id.principal.extraction.rules = 
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.step.connection.rate.per.ip = -1.0
	confluent.step.connection.rate.per.tenant = -1.0
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.subdomain.prefix = null
	confluent.subdomain.separator.map = null
	confluent.subdomain.separator.variable = %sep
	confluent.system.time.roll.enable = false
	confluent.telemetry.enabled = false
	confluent.telemetry.external.client.metrics.delta.temporality = true
	confluent.telemetry.external.client.metrics.instance.cache.size = 16384
	confluent.telemetry.external.client.metrics.push.enabled = false
	confluent.telemetry.external.client.metrics.subscription.interval.ms.list = null
	confluent.telemetry.external.client.metrics.subscription.match.list = null
	confluent.telemetry.external.client.metrics.subscription.metrics.list = null
	confluent.tenant.latency.metric.enabled = false
	confluent.tenantaware.encryption.key.manager.enable = false
	confluent.tenantaware.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.tenantaware.encryption.key.manager.tenant.cache.eviction.time.sec = 172800
	confluent.tenantaware.encryption.key.manager.tenant.cache.size = 100
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.async.enable = false
	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 1
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.prefetch.cache.enable = false
	confluent.tier.prefetch.cache.entry.size.bytes = 1048576
	confluent.tier.prefetch.cache.range.bytes = 5242880
	confluent.tier.prefetch.cache.total.size.bytes = 209715200
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.ipv6.enabled = true
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.security.providers = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.provider = null
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.storage.class.override = 
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.s3.v2.enabled = false
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
	confluent.tier.topic.data.loss.validation.fencing.enable = false
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.head.data.loss.validation.enable = true
	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
	confluent.tier.topic.materialization.from.snapshot.enable = false
	confluent.tier.topic.producer.enable.idempotence = true
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.metadata.throttle.pre.check.partition.count.threshold = 1000
	confluent.topic.partition.default.placement = 2
	confluent.topic.policy.use.computed.assignments = false
	confluent.topic.replica.assignor.builder.class = 
	confluent.track.api.key.per.ip = false
	confluent.track.per.ip.max.size = 100000
	confluent.track.tenant.id.per.ip = false
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.traffic.network.type = 
	confluent.transaction.2pc.timeout.ms = -1
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.unique.deprecated.request.metrics.per.tenant = 1000
	confluent.valid.broker.rack.set = null
	confluent.valid.sni.hostnames = 
	confluent.valid.sni.hostnames.exclude.suffix = 
	confluent.verify.group.subscription.prefix = false
	confluent.virtual.topic.creation.enabled = false
	confluent.zone.tagged.request.metrics.enable = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	controlled.shutdown.enable = true
	controller.listener.names = CONTROLLER
	controller.performance.always.log.threshold.ms = 2000
	controller.performance.sample.period.ms = 60000
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.consumer.assignors = [uniform, range]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = bidirectional
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 5
	group.coordinator.new.enable = true
	group.coordinator.rebalance.protocols = [classic, consumer]
	group.coordinator.threads = 4
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	group.share.delivery.count.limit = 5
	group.share.enable = false
	group.share.heartbeat.interval.ms = 5000
	group.share.max.groups = 10
	group.share.max.heartbeat.interval.ms = 15000
	group.share.max.record.lock.duration.ms = 60000
	group.share.max.session.timeout.ms = 60000
	group.share.max.size = 200
	group.share.min.heartbeat.interval.ms = 5000
	group.share.min.record.lock.duration.ms = 15000
	group.share.min.session.timeout.ms = 45000
	group.share.partition.max.record.locks = 200
	group.share.persister.class.name = org.apache.kafka.server.share.persister.DefaultStatePersister
	group.share.record.lock.duration.ms = 30000
	group.share.session.timeout.ms = 45000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	k2.stack.builder.class.name = null
	k2.startup.timeout.ms = 60000
	k2.topic.metadata.refresh.ms = 10000
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.cleanup.policy.empty.validation = none
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.timestamp.after.max.ms = 3600000
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.connections.per.tenant = 0
	max.connections.protected.listeners = []
	max.connections.reap.amount = 0
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.authorizer.support.resource.ids = false
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 2
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	otel.exporter.otlp.custom.endpoint = default
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.fetch.max.wait.ms = 500
	remote.list.offsets.request.timeout.ms = 30000
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 2
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	share.coordinator.append.linger.ms = 5
	share.coordinator.load.buffer.size = 5242880
	share.coordinator.snapshot.update.records.per.snapshot = 500
	share.coordinator.state.topic.compression.codec = 0
	share.coordinator.state.topic.min.isr = 2
	share.coordinator.state.topic.num.partitions = 50
	share.coordinator.state.topic.prune.interval.ms = 300000
	share.coordinator.state.topic.replication.factor = 3
	share.coordinator.state.topic.segment.bytes = 104857600
	share.coordinator.threads = 1
	share.coordinator.write.timeout.ms = 5000
	share.fetch.max.fetch.records = 2147483647
	share.fetch.purgatory.purge.interval.requests = 1000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	throughput.quota.window.num = 11
	token.impersonation.validation = true
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.metadata.load.threads = 32
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unclean.leader.election.interval.ms = 300000
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,762] INFO Registering metric ActiveBalancerCount (io.confluent.databalancer.KafkaDataBalanceManager:%L)
[2025-06-22 05:26:38,763] INFO Instantiating ClusterBalanceManager with an instance of io.confluent.databalancer.SbcDataBalanceManager (ClusterBalanceManager:%L)
[2025-06-22 05:26:38,764] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler:%L)
[2025-06-22 05:26:38,764] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,766] INFO [ControllerServer id=1] FIPS mode enabled: false (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,768] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,769] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,769] INFO NamedConfigEnabled: false (io.confluent.security.audit.provider.ConfluentAuditLogProvider:%L)
[2025-06-22 05:26:38,769] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,770] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,770] INFO MultiTenantAuditLogConfig values: 
	confluent.security.event.logger.client.ip.enable = false
	confluent.security.event.logger.multitenant.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,770] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,770] INFO Audit Log rate limiter reconfigured: Authn: -1, Authz: -1, Kafka request: -1 (io.confluent.security.audit.provider.AuditLogRateLimiter:%L)
[2025-06-22 05:26:38,824] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,827] INFO Quota CONTROLLER-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota:%L)
[2025-06-22 05:26:38,827] INFO Quota CONTROLLER-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota:%L)
[2025-06-22 05:26:38,827] INFO Quota CONTROLLER-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota:%L)
[2025-06-22 05:26:38,827] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas:%L)
[2025-06-22 05:26:38,830] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,832] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,834] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,836] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) used TCP protocol (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,837] INFO [SharedServer id=1] Using localhost:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer:%L)
[2025-06-22 05:26:38,837] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer:%L)
[2025-06-22 05:26:38,864] INFO [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:38,865] INFO [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:38,865] INFO [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset $lastOffset (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:38,866] INFO Initialized snapshots with IDs SortedSet() from /var/lib/kafka/data/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$:%L)
[2025-06-22 05:26:38,873] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper:%L)
[2025-06-22 05:26:38,875] INFO [RaftManager id=1] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,876] INFO [RaftManager id=1] Starting voters are VoterSet(voters={1=VoterNode(voterKey=ReplicaKey(id=1, directoryId=<undefined>), listeners=Endpoints(endpoints={ListenerName(CONTROLLER)=localhost/127.0.0.1:29093}), supportedKRaftVersion=SupportedVersionRange[min_version:0, max_version:0])}) (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,876] INFO [RaftManager id=1] Starting request manager with static voters: [localhost:29093 (id: 1 rack: null isFenced: false)] (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,876] INFO [RaftManager id=1] Attempting durable transition to UnattachedState(epoch=0, leaderId=OptionalInt.empty, votedKey=Optional.empty, voters=[1], electionTimeoutMs=1859, highWatermark=Optional.empty) from null (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,880] INFO [RaftManager id=1] Completed transition to UnattachedState(epoch=0, leaderId=OptionalInt.empty, votedKey=Optional.empty, voters=[1], electionTimeoutMs=1859, highWatermark=Optional.empty) from null (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,881] INFO [RaftManager id=1] Completed transition to ProspectiveState(epoch=0, leaderId=OptionalInt.empty, retries=1, votedKey=Optional.empty, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), electionTimeoutMs=1818, highWatermark=Optional.empty) from UnattachedState(epoch=0, leaderId=OptionalInt.empty, votedKey=Optional.empty, voters=[1], electionTimeoutMs=1859, highWatermark=Optional.empty) (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,881] INFO [RaftManager id=1] Attempting durable transition to CandidateState(localId=1, localDirectoryId=y2hnhBSrRN9M9zSpC-BgBg, epoch=1, retries=1, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), highWatermark=Optional.empty, electionTimeoutMs=1201) from ProspectiveState(epoch=0, leaderId=OptionalInt.empty, retries=1, votedKey=Optional.empty, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), electionTimeoutMs=1818, highWatermark=Optional.empty) (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,883] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, localDirectoryId=y2hnhBSrRN9M9zSpC-BgBg, epoch=1, retries=1, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), highWatermark=Optional.empty, electionTimeoutMs=1201) from ProspectiveState(epoch=0, leaderId=OptionalInt.empty, retries=1, votedKey=Optional.empty, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), electionTimeoutMs=1818, highWatermark=Optional.empty) (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,883] INFO [RaftManager id=1] Attempting durable transition to Leader(localVoterNode=VoterNode(voterKey=ReplicaKey(id=1, directoryId=y2hnhBSrRN9M9zSpC-BgBg), listeners=Endpoints(endpoints={ListenerName(CONTROLLER)=localhost/<unresolved>:29093}), supportedKRaftVersion=SupportedVersionRange[min_version:0, max_version:1]), epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, localDirectoryId=y2hnhBSrRN9M9zSpC-BgBg, epoch=1, retries=1, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), highWatermark=Optional.empty, electionTimeoutMs=1201) (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,884] INFO [RaftManager id=1] Completed transition to Leader(localVoterNode=VoterNode(voterKey=ReplicaKey(id=1, directoryId=y2hnhBSrRN9M9zSpC-BgBg), listeners=Endpoints(endpoints={ListenerName(CONTROLLER)=localhost/<unresolved>:29093}), supportedKRaftVersion=SupportedVersionRange[min_version:0, max_version:1]), epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, localDirectoryId=y2hnhBSrRN9M9zSpC-BgBg, epoch=1, retries=1, epochElection=EpochElection(voterStates={1=VoterState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), state=GRANTED)}), highWatermark=Optional.empty, electionTimeoutMs=1201) (org.apache.kafka.raft.QuorumState:%L)
[2025-06-22 05:26:38,945] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread:%L)
[2025-06-22 05:26:38,945] INFO [kafka-1-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver:%L)
[2025-06-22 05:26:38,946] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=<undefined>), endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState:%L)
[2025-06-22 05:26:38,947] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,947] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,947] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,950] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@2104880776 (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,950] INFO [RaftManager id=1] Setting the next offset of org.apache.kafka.image.loader.MetadataLoader@2104880776 to 0 since there are no snapshots (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,950] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,950] INFO [ControllerServer id=1] Registering periodic task writeNoOpRecord to run every 500 ms (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,950] INFO [ControllerServer id=1] Registering periodic task maybeFenceStaleBroker to run every 1125 ms (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,950] INFO [ControllerServer id=1] Registering periodic task electUnclean to run every 300000 ms (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,950] INFO [ControllerServer id=1] Registering periodic task expireDelegationTokens to run every 3600000 ms (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,950] INFO [ControllerServer id=1] Registering periodic task generatePeriodicPerformanceMessage to run every 60000 ms (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,951] INFO [ControllerServer id=1] Creating new QuorumController with clusterId MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.controller.QuorumController:%L)
[2025-06-22 05:26:38,951] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@1740701627 (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,951] INFO [RaftManager id=1] Setting the next offset of org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@1740701627 to 0 since there are no snapshots (org.apache.kafka.raft.KafkaRaftClient:%L)
[2025-06-22 05:26:38,951] INFO [ControllerServer id=1] Becoming the active controller at epoch 1, next write offset 1. (org.apache.kafka.controller.QuorumController:%L)
[2025-06-22 05:26:38,951] WARN [ControllerServer id=1] Performing controller activation. The metadata log appears to be empty. Appending 1 bootstrap record(s) in metadata transaction at metadata.version 4.0-IV3A from bootstrap source 'the default bootstrap'. (org.apache.kafka.controller.QuorumController:%L)
[2025-06-22 05:26:38,951] INFO [ControllerServer id=1] Replayed BeginTransactionRecord(name='Bootstrap records') at offset 1. (org.apache.kafka.controller.OffsetControlManager:%L)
[2025-06-22 05:26:38,951] INFO [ControllerServer id=1] Replayed a Confluent FeatureLevelRecord setting metadata version to 4.0-IV3A (org.apache.kafka.controller.FeatureControlManager:%L)
[2025-06-22 05:26:38,952] INFO [ControllerServer id=1] Replayed EndTransactionRecord() at offset 3. (org.apache.kafka.controller.OffsetControlManager:%L)
[2025-06-22 05:26:38,952] INFO [ControllerServer id=1] Activated periodic tasks: electUnclean, expireDelegationTokens, generatePeriodicPerformanceMessage, maybeFenceStaleBroker, writeNoOpRecord (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:26:38,952] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,952] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,953] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,953] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,953] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,953] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,953] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClientRequestQuotaManager:%L)
[2025-06-22 05:26:38,953] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,953] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,954] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClusterLinkRequestQuotaManager:%L)
[2025-06-22 05:26:38,954] INFO [controller-1-ThrottledChannelReaper-ClusterLinkRequest]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,954] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,955] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:38,955] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,955] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,956] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,957] INFO [ControllerServer id=1] Self-Balancing Kafka is enabled and will be installed as a metadata publisher. (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,959] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,959] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,959] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,959] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,959] INFO Awaiting socket connections on localhost:29093. (kafka.network.DataPlaneAcceptor:%L)
[2025-06-22 05:26:38,960] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Waiting for multi-tenant metadata loader to be started (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Finished waiting for multi-tenant metadata loader to be started (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:38,961] INFO [ControllerRegistrationManager id=1 incarnation=fPcYExt9Rl-z07xud2d9qQ] initialized channel manager. (kafka.server.ControllerRegistrationManager:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerRegistrationManager id=1 incarnation=fPcYExt9Rl-z07xud2d9qQ] maybeSendControllerRegistration: cannot register yet because the metadata.version is not known yet. (kafka.server.ControllerRegistrationManager:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [ControllerServer id=1] Finished waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer:%L)
[2025-06-22 05:26:38,961] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:29093 (id: 1 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:38,961] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:38,961] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:38,965] INFO [BrokerServer id=1] FIPS mode enabled: false (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:38,965] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,965] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,965] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,965] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,965] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,966] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClientRequestQuotaManager:%L)
[2025-06-22 05:26:38,966] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,966] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig:%L)
[2025-06-22 05:26:38,966] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClusterLinkRequestQuotaManager:%L)
[2025-06-22 05:26:38,966] INFO [broker-1-ThrottledChannelReaper-ClusterLinkRequest]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,966] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:%L)
[2025-06-22 05:26:38,966] INFO FlexFanout is not enabled or there is no ClientQuotaCallback so does not start FlexFanoutQuotaManager. (kafka.server.FlexFanoutQuotaManager:%L)
[2025-06-22 05:26:38,968] INFO Skip DiskIOManager init: confluent.disk.io.manager.enable = false (kafka.server.resource.DiskIOManager:%L)
[2025-06-22 05:26:38,968] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO NamedConfigEnabled: false (io.confluent.security.audit.provider.ConfluentAuditLogProvider:%L)
[2025-06-22 05:26:38,968] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO MultiTenantAuditLogConfig values: 
	confluent.security.event.logger.client.ip.enable = false
	confluent.security.event.logger.multitenant.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
	confluent.security.event.router.named.config.enabled = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,968] INFO Audit Log rate limiter reconfigured: Authn: -1, Authz: -1, Kafka request: -1 (io.confluent.security.audit.provider.AuditLogRateLimiter:%L)
[2025-06-22 05:26:38,968] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:38,968] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:38,969] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:38,969] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:29093 (id: 1 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:38,969] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper:%L)
[2025-06-22 05:26:38,971] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:38,977] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 4 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [ControllerServer id=1] Loaded new metadata Features(metadataVersion=4.0-IV3A, finalizedFeatures={confluent.metadata.version=127}, finalizedFeaturesEpoch=3). (org.apache.kafka.metadata.publisher.FeaturesPublisher:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ConfluentControllerMetricsPublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [ConfluentControllerMetricsChanges id=1] Finished reloading all Confluent controller metrics in 0 ms. (org.apache.kafka.controller.metrics.ConfluentControllerMetricsChanges:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing CellControllerMetadataMetricsPublisher with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SbcDataBalanceManager with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 3 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:38,978] INFO Balancer received a new Replica Exclusions Image (image: , delta: BrokerReplicaExclusionsDelta{newExclusions=[], removedExclusions=[]}) (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO Handling event SbcAlteredExclusionsEvent-4 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO SBC Event SbcMetadataUpdateEvent-1 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-3]. Enqueuing... (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO Handling event SbcLeaderUpdateEvent-2 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO This balancer node is now the metadata quorum leader. Activating kafkadatabalance manager without alive broker snapshot. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,978] INFO [ControllerRegistrationManager id=1 incarnation=fPcYExt9Rl-z07xud2d9qQ] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=fPcYExt9Rl-z07xud2d9qQ, zkMigrationReady=false, listeners=[Listener(name='CONTROLLER', host='localhost', port=29093, securityProtocol=0)], features=[Feature(name='group.version', minSupportedVersion=0, maxSupportedVersion=1), Feature(name='confluent.metadata.version', minSupportedVersion=7, maxSupportedVersion=127), Feature(name='transaction.version', minSupportedVersion=0, maxSupportedVersion=2), Feature(name='eligible.leader.replicas.version', minSupportedVersion=0, maxSupportedVersion=1), Feature(name='kraft.version', minSupportedVersion=0, maxSupportedVersion=1), Feature(name='metadata.version', minSupportedVersion=7, maxSupportedVersion=25)], metadataEncryptors=[]) (kafka.server.ControllerRegistrationManager:%L)
[2025-06-22 05:26:38,978] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,979] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,979] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:38,982] INFO [SocketServer listenerType=BROKER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,982] INFO [ControllerServer id=1] Node 1 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:38,982] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:38,982] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:38,982] INFO [ControllerServer id=1] Replayed RegisterControllerRecord containing ControllerRegistration(id=1, incarnationId=fPcYExt9Rl-z07xud2d9qQ, zkMigrationReady=false, listeners=[Endpoint(listenerName='CONTROLLER', securityProtocol=PLAINTEXT, host='localhost', port=29093)], supportedFeatures={confluent.metadata.version: 7-127, eligible.leader.replicas.version: 0-1, group.version: 0-1, kraft.version: 0-1, metadata.version: 7-25, transaction.version: 0-2}, metadataEncryptors=[]). (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:38,983] INFO Quota PLAINTEXT-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota:%L)
[2025-06-22 05:26:38,983] INFO Quota PLAINTEXT-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota:%L)
[2025-06-22 05:26:38,983] INFO Quota PLAINTEXT-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota:%L)
[2025-06-22 05:26:38,983] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas:%L)
[2025-06-22 05:26:38,984] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,986] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,987] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,988] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) used TCP protocol (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,988] INFO [SocketServer listenerType=BROKER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer:%L)
[2025-06-22 05:26:38,989] INFO Quota PLAINTEXT_HOST-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota:%L)
[2025-06-22 05:26:38,989] INFO Quota PLAINTEXT_HOST-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota:%L)
[2025-06-22 05:26:38,989] INFO Quota PLAINTEXT_HOST-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota:%L)
[2025-06-22 05:26:38,989] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas:%L)
[2025-06-22 05:26:38,990] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:38,997] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,001] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,005] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) used TCP protocol (kafka.network.SocketServer:%L)
[2025-06-22 05:26:39,009] INFO [ControllerRegistrationManager id=1 incarnation=fPcYExt9Rl-z07xud2d9qQ] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager:%L)
[2025-06-22 05:26:39,009] INFO [ControllerRegistrationManager id=1 incarnation=fPcYExt9Rl-z07xud2d9qQ] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager:%L)
[2025-06-22 05:26:39,009] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,009] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:29093 (id: 1 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,013] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,013] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:29093 (id: 1 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,015] INFO [BrokerHealthManager]: Starting (kafka.availability.BrokerHealthManager:%L)
[2025-06-22 05:26:39,017] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,018] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,018] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,018] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,019] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,019] INFO ReplicationConfig values: 
	confluent.replication.linger.ms = 0
	confluent.replication.max.in.flight.requests = 1
	confluent.replication.max.memory.buffer.bytes = 209715200
	confluent.replication.max.replica.pushers = 4
	confluent.replication.max.wait.ms = 500
	confluent.replication.mode = PULL
	confluent.replication.num.pushers.per.broker = 1
	confluent.replication.push.internal.topics.enable = false
	confluent.replication.request.max.bytes = 52428800
	confluent.replication.request.max.partition.bytes = 52428800
	confluent.replication.request.timeout.ms = 5000
	confluent.replication.retry.timeout.ms = 10000
	confluent.replication.socket.send.buffer.bytes = 1048576
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,020] INFO [BrokerServer id=1] Using no op persister (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,021] INFO [group-coordinator-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper:%L)
[2025-06-22 05:26:39,076] INFO [group-coordinator-event-processor-0]: Starting (org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread:%L)
[2025-06-22 05:26:39,076] INFO [group-coordinator-event-processor-2]: Starting (org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread:%L)
[2025-06-22 05:26:39,076] INFO [group-coordinator-event-processor-1]: Starting (org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread:%L)
[2025-06-22 05:26:39,076] INFO [group-coordinator-event-processor-3]: Starting (org.apache.kafka.coordinator.common.runtime.MultiThreadedEventProcessor$EventProcessorThread:%L)
[2025-06-22 05:26:39,078] INFO Unable to read the broker epoch in /var/lib/kafka/data. (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,078] INFO [SharedServer id=1] Using localhost:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer:%L)
[2025-06-22 05:26:39,078] INFO [SharedServer id=1] Using localhost:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer:%L)
[2025-06-22 05:26:39,078] INFO [BrokerLifecycleManager id=1] Incarnation HAVdWLvgQLyNZfKdxKnPRg of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qk is now STARTING. (kafka.server.BrokerLifecycleManager:%L)
[2025-06-22 05:26:39,078] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,078] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node localhost:29093 (id: 1 rack: null isFenced: false) (kafka.server.NodeToControllerRequestThread:%L)
[2025-06-22 05:26:39,079] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,079] INFO [ExpirationReaper-0-null]: Starting (org.apache.kafka.server.purgatory.DelayedOperationPurgatory$ExpiredOperationReaper:%L)
[2025-06-22 05:26:39,079] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,079] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,079] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,079] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,080] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,080] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,080] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.per.connection = false
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,080] INFO [BrokerServer id=1] Waiting for broker metadata to catch up (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,080] INFO [ControllerServer id=1] No previous registration found for broker 1. New incarnation ID is HAVdWLvgQLyNZfKdxKnPRg.  Generated 0 record(s) to clean up previous incarnations. New broker epoch is 5. (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,080] INFO [ControllerServer id=1] Node 1 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,080] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,080] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,081] INFO [ControllerServer id=1] Replayed initial RegisterBrokerRecord for broker 1: RegisterBrokerRecord(brokerId=1, isMigratingZkBroker=false, incarnationId=HAVdWLvgQLyNZfKdxKnPRg, brokerEpoch=5, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='localhost', port=29092, securityProtocol=0), BrokerEndpoint(name='PLAINTEXT_HOST', host='localhost', port=9092, securityProtocol=0)], features=[BrokerFeature(name='group.version', minSupportedVersion=0, maxSupportedVersion=1), BrokerFeature(name='confluent.metadata.version', minSupportedVersion=7, maxSupportedVersion=127), BrokerFeature(name='transaction.version', minSupportedVersion=0, maxSupportedVersion=2), BrokerFeature(name='eligible.leader.replicas.version', minSupportedVersion=0, maxSupportedVersion=1), BrokerFeature(name='kraft.version', minSupportedVersion=0, maxSupportedVersion=1), BrokerFeature(name='metadata.version', minSupportedVersion=7, maxSupportedVersion=25)], rack=null, fenced=true, inControlledShutdown=false, degradedComponents=[], metadataEncryptors=[], logDirs=[y2hnhBSrRN9M9zSpC-BgBg]) (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,106] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,106] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,106] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,106] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,107] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 5 (kafka.server.BrokerLifecycleManager:%L)
[2025-06-22 05:26:39,107] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager:%L)
[2025-06-22 05:26:39,107] INFO [BrokerServer id=1] Finished waiting for broker metadata to catch up (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO Starting DynamicMetricsReportersScheduler. (kafka.server.DynamicMetricsReportersScheduler:%L)
[2025-06-22 05:26:39,109] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO Attempting to initiate DynamicMetricsReporters. Attempt: 1 (kafka.server.DynamicMetricsReportersScheduler:%L)
[2025-06-22 05:26:39,109] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,109] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing MetadataVersionPublisher(id=1) with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:39,109] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:39,109] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version Optional[4.0-IV3A]. (kafka.server.metadata.BrokerMetadataPublisher:%L)
[2025-06-22 05:26:39,109] INFO Loading logs from log dirs ArrayBuffer(/var/lib/kafka/data) (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,109] INFO No logs found to be loaded in /var/lib/kafka/data (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,110] INFO Loaded 0 logs in 1ms (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,110] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,110] INFO EventEmitterConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,110] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,110] INFO Starting log roller with a period of 300000 ms. (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,111] INFO Linux CPU collector enabled: true (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,111] INFO Using cpu metric: io\.confluent\.kafka\.server/server/linux_system_cpu_utilization_1m (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,111] INFO Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,112] INFO Starting the log cleaner (kafka.log.LogCleaner:%L)
[2025-06-22 05:26:39,112] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig:%L)
[2025-06-22 05:26:39,112] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,363] WARN Ignoring redefinition of existing telemetry label kafka.version (io.confluent.telemetry.ResourceBuilderFacade:%L)
[2025-06-22 05:26:39,363] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager:%L)
[2025-06-22 05:26:39,365] INFO Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,365] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread:%L)
[2025-06-22 05:26:39,365] INFO ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.cluster.id = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.external.client.metrics.exclude.labels = 
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io\.confluent\.system/(?:.*/)?(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|client_broker_topic/client_bytes_in/delta|client_broker_topic/client_bytes_out/delta|client_broker_topic/client_records_in/delta|client_broker_topic/client_records_out/delta|cluster_link/active_link_count|cluster_link/consumer_offset_committed_rate|cluster_link/consumer_offset_committed_total|cluster_link/fetch_throttle_time_avg|cluster_link/fetch_throttle_time_max|cluster_link/link_count|cluster_link/linked_leader_epoch_change_rate|cluster_link/linked_leader_epoch_change_total|cluster_link/linked_topic_partition_addition_rate|cluster_link/linked_topic_partition_addition_total|cluster_link/mirror_partition_count|cluster_link/mirror_topic_byte_total|cluster_link/mirror_topic_count|cluster_link/mirror_topic_lag|cluster_link/topic_config_update_rate|cluster_link/topic_config_update_total|cluster_link_fetcher/connection_count|cluster_link_fetcher/failed_reauthentication_rate|cluster_link_fetcher/failed_reauthentication_total|cluster_link_fetcher/incoming_byte_rate|cluster_link_fetcher/incoming_byte_total|cluster_link_fetcher/outgoing_byte_rate|cluster_link_fetcher/outgoing_byte_total|cluster_link_fetcher/reauthentication_latency_avg|cluster_link_fetcher_manager/max_lag|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/preferred_replica_imbalance_count|controller/tenant_partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|fetcher/bytes_per_sec|fetcher_lag/consumer_lag|group_coordinator/partition_load_time_max|log/log_end_offset|log/log_start_offset|log/total_size|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/blocked_on_mirror_source_partition_count|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_mirror_partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_mirror_partitions|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max|tenant/consumer_lag_offsets).*|.*org\.apache\.kafka\.(producer\.connection\.creation\.rate|producer\.node\.request\.latency\.avg|producer\.node\.request\.latency\.max|producer\.produce\.throttle\.time\.avg|producer\.produce\.throttle\.time\.max|producer\.record\.queue\.time\.avg|producer\.record\.queue\.time\.max|producer\.connection\.creation\.total|consumer\.connection\.creation\.rate|consumer\.connection\.creation\.total|consumer\.node\.request\.latency\.avg|consumer\.node\.request\.latency\.max|consumer\.poll\.idle\.ratio\.avg|consumer\.coordinator\.commit\.latency\.avg|consumer\.coordinator\.commit\.latency\.max|consumer\.coordinator\.assigned\.partitions|consumer\.coordinator\.rebalance\.latency\.avg|consumer\.coordinator\.rebalance\.latency\.max|consumer\.coordinator\.rebalance\.latency\.total|consumer\.fetch\.manager\.fetch\.latency\.avg|consumer\.fetch\.manager\.fetch\.latency\.max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,365] INFO VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,365] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:%L)
[2025-06-22 05:26:39,365] INFO HttpClientConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = httpTelemetryClient
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,365] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client = _confluentClient
	client.attempts.max = null
	client.base.url = null
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	remote.configurable = true
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,365] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig:%L)
[2025-06-22 05:26:39,366] INFO HttpExporterConfig values: 
	api.key = unused
	api.secret = [hidden]
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = 2000
	buffer.inflight.submissions.max = 10
	buffer.pending.batches.max = 25
	client = 
	client.attempts.max = null
	client.base.url = http://localhost:9090/api/v1/otlp
	client.compression = gzip
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = (io.confluent.kafka.server.request.(?!.*delta).*|io.confluent.kafka.server.server.broker.state|io.confluent.kafka.server.replica.manager.leader.count|io.confluent.kafka.server.request.queue.size|io.confluent.kafka.server.broker.topic.failed.produce.requests.rate.1_min|io.confluent.kafka.server.tier.archiver.total.lag|io.confluent.kafka.server.request.total.time.ms.p99|io.confluent.kafka.server.broker.topic.failed.fetch.requests.rate.1_min|io.confluent.kafka.server.log.total.size|io.confluent.kafka.server.broker.topic.total.fetch.requests.rate.1_min|io.confluent.kafka.server.partition.caught.up.replicas.count|io.confluent.kafka.server.partition.observer.replicas.count|io.confluent.kafka.server.tier.tasks.num.partitions.in.error|io.confluent.kafka.server.broker.topic.bytes.out.rate.1_min|io.confluent.kafka.server.request.total.time.ms.p95|io.confluent.kafka.server.controller.active.controller.count|io.confluent.kafka.server.session.expire.listener.zookeeper.disconnects.total|io.confluent.kafka.server.request.total.time.ms.p999|io.confluent.kafka.server.controller.active.broker.count|io.confluent.kafka.server.request.handler.pool.request.handler.avg.idle.percent.rate.1_min|io.confluent.kafka.server.session.expire.listener.zookeeper.disconnects.rate.1_min|io.confluent.kafka.server.controller.unclean.leader.elections.rate.1_min|io.confluent.kafka.server.replica.manager.partition.count|io.confluent.kafka.server.controller.unclean.leader.elections.total|io.confluent.kafka.server.partition.replicas.count|io.confluent.kafka.server.broker.topic.total.produce.requests.rate.1_min|io.confluent.kafka.server.controller.offline.partitions.count|io.confluent.kafka.server.socket.server.network.processor.avg.idle.percent|io.confluent.kafka.server.partition.under.replicated|io.confluent.kafka.server.log.log.start.offset|io.confluent.kafka.server.log.tier.size|io.confluent.kafka.server.log.size|io.confluent.kafka.server.tier.fetcher.bytes.fetched.total|io.confluent.kafka.server.request.total.time.ms.p50|io.confluent.kafka.server.tenant.consumer.lag.offsets|io.confluent.kafka.server.session.expire.listener.zookeeper.expires.rate.1_min|io.confluent.kafka.server.log.log.end.offset|io.confluent.kafka.server.log.num.log.segments|io.confluent.kafka.server.broker.topic.bytes.in.rate.1_min|io.confluent.kafka.server.partition.under.min.isr|io.confluent.kafka.server.partition.in.sync.replicas.count|io.confluent.telemetry.http.exporter.batches.dropped|io.confluent.telemetry.http.exporter.items.total|io.confluent.telemetry.http.exporter.items.succeeded|io.confluent.telemetry.http.exporter.send.time.total.millis|io.confluent.kafka.server.controller.leader.election.rate.(?!.*delta).*|io.confluent.telemetry.http.exporter.batches.failed)
	proxy.password = null
	proxy.url = null
	proxy.username = null
	remote.configurable = true
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,366] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager:%L)
[2025-06-22 05:26:39,366] INFO KafkaExporterConfig values: 
	client = 
	enabled = true
	events.enabled = true
	metrics.enabled = true
	metrics.include = (io\.confluent\.kafka\.server/fetch/broker_quota|io\.confluent\.kafka\.server/produce/broker_quota|io\.confluent\.kafka\.server/broker_load/broker_load_percent|io\.confluent\.kafka\.server/broker_topic/bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/fetch_from_follower_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/messages_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_follower_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/mirror_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_from_follower_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_produce_requests/rate/1_min|io\.confluent\.kafka\.server/log/size|io\.confluent\.kafka\.server/request/requests/rate/1_min|io\.confluent\.kafka\.server/request_handler_pool/request_handler_avg_idle_percent/rate/1_min|io\.confluent\.kafka\.server/server/linux_system_cpu_utilization_1m|io\.confluent\.kafka\.server/replica_manager/partition_count|io\.confluent\.system/volume/disk_total_bytes)
	producer.bootstrap.servers = localhost:29092
	remote.configurable = false
	topic.create = true
	topic.max.message.bytes = 10485760
	topic.name = _confluent-telemetry-metrics
	topic.partitions = 12
	topic.replicas = 1
	topic.retention.bytes = -1
	topic.retention.ms = 259200000
	topic.roll.ms = 14400000
	type = kafka
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,366] INFO PollingRemoteConfigurationConfig values: 
	enabled = true
	refresh.interval.ms = 60000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,366] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter:%L)
[2025-06-22 05:26:39,367] INFO EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.telemetry.events.exporter.http.EventHttpExporter
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,367] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = true
	events.enabled = true
	filtering.enabled = false
	filtering.routes.allowed = []
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,368] INFO [ClusterLinkTaskManager brokerId=1] Creating local admin client for task manager 0 (kafka.server.link.ClusterLinkTaskManager:%L)
[2025-06-22 05:26:39,368] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = cluster-link--local-admin-1
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,372] INFO These configurations '[confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, cluster.link.metadata.topic.replication.factor, confluent.license.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,372] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,372] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,372] INFO Kafka startTimeMs: 1750569999372 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,372] INFO [ClusterLinkRegionalMetadata-broker-1] Set local network type to persisted value NOT_SET (kafka.server.link.ClusterLinkRegionalMetadata:%L)
[2025-06-22 05:26:39,372] INFO [ClusterLinkManager-broker-1] ClusterLinkManager has started up. (kafka.server.link.ClusterLinkManager:%L)
[2025-06-22 05:26:39,372] INFO [GroupCoordinator id=1] Starting up. (org.apache.kafka.coordinator.group.GroupCoordinatorService:%L)
[2025-06-22 05:26:39,372] INFO [GroupCoordinator id=1] Startup complete. (org.apache.kafka.coordinator.group.GroupCoordinatorService:%L)
[2025-06-22 05:26:39,372] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:%L)
[2025-06-22 05:26:39,372] INFO [AdminClient clientId=cluster-link--local-admin-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:26:39,373] WARN [AdminClient clientId=cluster-link--local-admin-1] Connection to node -1 (localhost/127.0.0.1:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:26:39,373] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:%L)
[2025-06-22 05:26:39,373] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:%L)
[2025-06-22 05:26:39,373] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=1) with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:39,373] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,373] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ClusterLinkCoordinatorListener with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader:%L)
[2025-06-22 05:26:39,374] INFO KafkaConfig values: 
	add.partitions.to.txn.retry.backoff.max.ms = 100
	add.partitions.to.txn.retry.backoff.ms = 20
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = f4iDkhvfTGqMLnNPxFcjWw
	client.quota.callback.class = null
	client.quota.max.throttle.time.in.response.ms = 60000
	client.quota.max.throttle.time.ms = 5000
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	confluent.accp.enabled = false
	confluent.acks.equal.to.one.request.replication.lag.threshold.ms = -1
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.api.visibility = DEFAULT
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
	confluent.balancer.capacity.threshold.upper.limit = 0.95
	confluent.balancer.cell.load.upper.bound = 0.7
	confluent.balancer.cell.overload.detection.interval.ms = 3600000
	confluent.balancer.cell.overload.duration.ms = 86400000
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.cpu.balance.threshold = 1.1
	confluent.balancer.cpu.goal.act.as.capacity.goal = false
	confluent.balancer.cpu.low.utilization.threshold = 0.2
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.enable.network.capacity.metric.ingestion = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.flex.fanout.network.capacity.metrics.avg.period.ms = 1800000
	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.min.valid.windows = 5
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.inter.cell.balancing.enabled = false
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.minimum.reported.brokers.with.network.capacity.metrics.percentage = 0.8
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.num.concurrent.replica.movements.as.destination.per.broker = 18
	confluent.balancer.num.concurrent.replica.movements.as.source.per.broker = 12
	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.sbc.metrics.parser.enabled = false
	confluent.balancer.self.healing.maximum.rounds = 1
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.tenant.maximum.movements = 0
	confluent.balancer.tenant.suspension.ms = 86400000
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.balancing.itrdg.with.hard.goals.enabled = false
	confluent.balancer.topic.partition.maximum.movements = 3
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.movements.history.limit = 900
	confluent.balancer.topic.partition.suspension.ms = 3600000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.balancer.v2.addition.enabled = false
	confluent.balancer.v2.addition.reassignment.cancellations.enabled = false
	confluent.balancer.v2.executor.enabled = false
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.assertion.claim.aud = null
	confluent.bearer.assertion.claim.exp.minutes = null
	confluent.bearer.assertion.claim.iss = null
	confluent.bearer.assertion.claim.jti.include = null
	confluent.bearer.assertion.claim.nbf.include = null
	confluent.bearer.assertion.claim.sub = null
	confluent.bearer.assertion.file = null
	confluent.bearer.assertion.private.key.file = null
	confluent.bearer.assertion.private.key.passphrase = null
	confluent.bearer.assertion.template.file = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.advertised.limit.load = 0.8
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.tenant.metric.enable = false
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.calling.resource.identity.type.map = 
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.full.configs.enable = false
	confluent.catalog.collector.max.bytes.per.snapshot = 850000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.multitenant.topics.enable = true
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cell.metrics.refresh.period.ms = 60000
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.k2.base.broker.index = -1
	confluent.cells.load.refresher.enable = true
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.client.topic.max.metrics.count = 1000
	confluent.client.topic.metrics.expiry.sec = 3600
	confluent.client.topic.metrics.manager = class org.apache.kafka.server.metrics.ClientTopicMetricsManager$NoOpClientTopicMetricsManager
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.list.object.thread_pool.size = 1
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.admin.max.in.flight.requests = 1000
	confluent.cluster.link.admin.request.batch.size = 1
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.allow.legacy.message.format = false
	confluent.cluster.link.allow.truncation.below.hwm = false
	confluent.cluster.link.availability.check.mode = ALL
	confluent.cluster.link.background.thread.affinity = LINK
	confluent.cluster.link.bootstrap.translation.feature.enable = true
	confluent.cluster.link.clients.max.idle.ms = 3153600000000
	confluent.cluster.link.enable = true
	confluent.cluster.link.enable.local.admin = false
	confluent.cluster.link.enable.metrics.reduction = false
	confluent.cluster.link.enable.metrics.reduction.advanced = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.fetcher.auto.tune.enable = false
	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
	confluent.cluster.link.insync.fetch.response.min.bytes = 1
	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.intranet.connectivity.denied.org.ids = []
	confluent.cluster.link.intranet.connectivity.enable = false
	confluent.cluster.link.intranet.connectivity.migration.enable = false
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.local.admin.multitenant.enable = false
	confluent.cluster.link.local.reverse.connection.listener.map = null
	confluent.cluster.link.max.client.connections = 2147483647
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.mirror.transition.batch.size = 10
	confluent.cluster.link.num.background.threads = 1
	confluent.cluster.link.periodic.task.batch.size = 2147483647
	confluent.cluster.link.periodic.task.min.interval.ms = 1000
	confluent.cluster.link.persistent.connection.backoff.max.ms = 0
	confluent.cluster.link.replica.fetch.connections.mode = combined
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.cluster.link.request.quota.capacity = 400
	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
	confluent.cluster.link.switchover.disabled.principals = []
	confluent.cluster.link.switchover.enable = false
	confluent.cluster.link.switchover.listeners = []
	confluent.cluster.link.switchover.server.states = []
	confluent.cluster.link.tenant.replication.quota.enable = false
	confluent.cluster.link.tenant.request.quota.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
	confluent.cluster.metadata.snapshot.tier.upload.enable = false
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
	confluent.consumer.fetch.partition.pruning.enable = true
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.dataflow.policy.watch.monitor.ms = 300000
	confluent.default.data.policy.enforcement = true
	confluent.defer.isr.shrink.enable = false
	confluent.describe.topic.partitions.enabled = true
	confluent.disk.io.manager.enable = false
	confluent.disk.throughput.headroom = 10485760
	confluent.disk.throughput.limit = 10485760000
	confluent.disk.throughput.quota.tier.archive = 1048576000
	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.log.ignored.event.types = 
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 1
	confluent.e2e_checksum.protection.enabled = false
	confluent.e2e_checksum.protection.files = [none]
	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
	confluent.elastic.cku.enabled = false
	confluent.elastic.cku.scaletozero.enabled = false
	confluent.eligible.controllers = []
	confluent.enable.broker.reporting.min.usage.mode = true
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fail.unsatisfied.placement.constraints = false
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.flexible.fanout.broker.max.fetch.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.max.produce.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.min.producer.percentage = 10.0
	confluent.flexible.fanout.broker.network.out.bytes.per.second = 6200000
	confluent.flexible.fanout.broker.recompute.interval.ms = 30000
	confluent.flexible.fanout.broker.storage.bytes.per.second = 512000000
	confluent.flexible.fanout.enabled = false
	confluent.flexible.fanout.lazy.evaluation.threshold = 0.5
	confluent.flexible.fanout.mode = TENANT_QUOTA
	confluent.floor.connection.rate.per.ip = -1.0
	confluent.floor.connection.rate.per.tenant = -1.0
	confluent.group.coordinator.dynamic.append.linger.enable = false
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.coordinator.txn.offset.validation.enable = false
	confluent.group.highest.offset.commit.rates.log.count = 10
	confluent.group.highest.offset.commit.rates.log.enable = false
	confluent.group.highest.offset.commit.rates.log.interval.ms = 300000
	confluent.group.metadata.load.threads = 32
	confluent.group.subscription.pattern.log.interval.ms = -1
	confluent.heap.tenured.notify.bytes = 0
	confluent.heap.tenured.notify.enabled = false
	confluent.hot.partition.ratio = 0.8
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.internal.rest.server.ssl.enable = false
	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
	confluent.leader.epoch.checkpoint.checksum.enabled = false
	confluent.listener.protocol = TCP
	confluent.log.cleaner.timestamp.validation.enable = true
	confluent.log.placement.constraints = 
	confluent.max.broker.load = 1.0
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
	confluent.max.connection.rate.per.ip = -1.0
	confluent.max.connection.rate.per.tenant = -1.0
	confluent.max.connection.throttle.ms = null
	confluent.max.segment.ms = 9223372036854775807
	confluent.metadata.active.encryptor = null
	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.required = false
	confluent.metadata.encryptor.secret.file = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.jvm.warmup.ms = 60000
	confluent.metadata.leader.balance.slice.delay.ms = 100
	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
	confluent.metadata.max.leader.balance.changes.per.slice = 1000
	confluent.metadata.reject.when.throttled.enable = false
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.connection.throttle.ms = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 20000
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.mtls.build.client.cert.chain.enable = false
	confluent.mtls.enable = false
	confluent.mtls.listener.name = EXTERNAL
	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
	confluent.mtls.truststore.alter.configs.timeout.ms = 300000
	confluent.mtls.truststore.manager.class.name = null
	confluent.multitenant.authorizer.enable.acl.state = false
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.lkc.id.enable = false
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.external.listener.name = EXTERNAL
	confluent.network.health.manager.externalconnectivitystartup.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.oauth.flat.networking.verification.enable = false
	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
	confluent.offsets.topic.placement.constraints = 
	confluent.omit.network.processor.metric.tag = false
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.ppv2.endpoint.scheme.bootstrap.broker.template.mappings = 
	confluent.ppv2.endpoint.scheme.enable = false
	confluent.ppv2.endpoint.scheme.map.broker.zone.to.gateway.zone = false
	confluent.ppv2.endpoint.scheme.template.variable.cloud = 
	confluent.ppv2.endpoint.scheme.template.variable.domain = 
	confluent.ppv2.endpoint.scheme.template.variable.region = 
	confluent.ppv2.endpoint.scheme.template.variables = 
	confluent.ppv2.endpoint.scheme.templates = 
	confluent.prefer.tier.fetch.ms = -1
	confluent.produce.throttle.pre.check.enable = false
	confluent.produce.throttle.pre.check.for.new.connection.enable = false
	confluent.producer.id.cache.broker.hard.limit = -1
	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
	confluent.producer.id.cache.extra.eviction.percentage = 0
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.cache.partition.hard.limit = -1
	confluent.producer.id.cache.tenant.hard.limit = -1
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.quota.window.num = 11
	confluent.producer.id.quota.window.size.seconds = 1
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.mode.local.default = false
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.computing.usage.adjustment = 0.5
	confluent.quota.dynamic.adjustment.min.usage = 102400
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.internal.broker.max.consumer.rate = 9223372036854775807
	confluent.quota.tenant.internal.broker.max.producer.rate = 9223372036854775807
	confluent.quota.tenant.internal.throttling.enable = false
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.rack.id.mapping = null
	confluent.regional.metadata.client.class = null
	confluent.regional.resource.manager.client.scheduler.threads = 2
	confluent.regional.resource.manager.endpoint = null
	confluent.regional.resource.manager.watch.endpoint = null
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.replication.mode = PULL
	confluent.replication.push.feature.enable = false
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = true
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.calling.resource.identity = false
	confluent.require.compatible.keystore.updates = true
	confluent.require.confluent.issuer = false
	confluent.roll.check.interval.ms = 300000
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validation.context.name.enable = false
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.bc.approved.mode.enable = false
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.config = 
	confluent.security.revoked.certificate.ids = 
	confluent.segment.eager.roll.enable = false
	confluent.segment.speculative.prefetch.enable = false
	confluent.share.metadata.load.threads = 32
	confluent.spiffe.id.principal.extraction.rules = 
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.step.connection.rate.per.ip = -1.0
	confluent.step.connection.rate.per.tenant = -1.0
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.subdomain.prefix = null
	confluent.subdomain.separator.map = null
	confluent.subdomain.separator.variable = %sep
	confluent.system.time.roll.enable = false
	confluent.telemetry.enabled = false
	confluent.telemetry.external.client.metrics.delta.temporality = true
	confluent.telemetry.external.client.metrics.instance.cache.size = 16384
	confluent.telemetry.external.client.metrics.push.enabled = false
	confluent.telemetry.external.client.metrics.subscription.interval.ms.list = null
	confluent.telemetry.external.client.metrics.subscription.match.list = null
	confluent.telemetry.external.client.metrics.subscription.metrics.list = null
	confluent.tenant.latency.metric.enabled = false
	confluent.tenantaware.encryption.key.manager.enable = false
	confluent.tenantaware.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.tenantaware.encryption.key.manager.tenant.cache.eviction.time.sec = 172800
	confluent.tenantaware.encryption.key.manager.tenant.cache.size = 100
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.async.enable = false
	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 1
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.prefetch.cache.enable = false
	confluent.tier.prefetch.cache.entry.size.bytes = 1048576
	confluent.tier.prefetch.cache.range.bytes = 5242880
	confluent.tier.prefetch.cache.total.size.bytes = 209715200
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.ipv6.enabled = true
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.security.providers = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.provider = null
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.storage.class.override = 
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.s3.v2.enabled = false
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
	confluent.tier.topic.data.loss.validation.fencing.enable = false
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.head.data.loss.validation.enable = true
	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
	confluent.tier.topic.materialization.from.snapshot.enable = false
	confluent.tier.topic.producer.enable.idempotence = true
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.metadata.throttle.pre.check.partition.count.threshold = 1000
	confluent.topic.partition.default.placement = 2
	confluent.topic.policy.use.computed.assignments = false
	confluent.topic.replica.assignor.builder.class = 
	confluent.track.api.key.per.ip = false
	confluent.track.per.ip.max.size = 100000
	confluent.track.tenant.id.per.ip = false
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.traffic.network.type = 
	confluent.transaction.2pc.timeout.ms = -1
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.unique.deprecated.request.metrics.per.tenant = 1000
	confluent.valid.broker.rack.set = null
	confluent.valid.sni.hostnames = 
	confluent.valid.sni.hostnames.exclude.suffix = 
	confluent.verify.group.subscription.prefix = false
	confluent.virtual.topic.creation.enabled = false
	confluent.zone.tagged.request.metrics.enable = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	controlled.shutdown.enable = true
	controller.listener.names = CONTROLLER
	controller.performance.always.log.threshold.ms = 2000
	controller.performance.sample.period.ms = 60000
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.consumer.assignors = [uniform, range]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = bidirectional
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 5
	group.coordinator.new.enable = true
	group.coordinator.rebalance.protocols = [classic, consumer]
	group.coordinator.threads = 4
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	group.share.delivery.count.limit = 5
	group.share.enable = false
	group.share.heartbeat.interval.ms = 5000
	group.share.max.groups = 10
	group.share.max.heartbeat.interval.ms = 15000
	group.share.max.record.lock.duration.ms = 60000
	group.share.max.session.timeout.ms = 60000
	group.share.max.size = 200
	group.share.min.heartbeat.interval.ms = 5000
	group.share.min.record.lock.duration.ms = 15000
	group.share.min.session.timeout.ms = 45000
	group.share.partition.max.record.locks = 200
	group.share.persister.class.name = org.apache.kafka.server.share.persister.DefaultStatePersister
	group.share.record.lock.duration.ms = 30000
	group.share.session.timeout.ms = 45000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	k2.stack.builder.class.name = null
	k2.startup.timeout.ms = 60000
	k2.topic.metadata.refresh.ms = 10000
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.cleanup.policy.empty.validation = none
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.timestamp.after.max.ms = 3600000
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.connections.per.tenant = 0
	max.connections.protected.listeners = []
	max.connections.reap.amount = 0
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.authorizer.support.resource.ids = false
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 2
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	otel.exporter.otlp.custom.endpoint = default
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.fetch.max.wait.ms = 500
	remote.list.offsets.request.timeout.ms = 30000
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 2
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	share.coordinator.append.linger.ms = 5
	share.coordinator.load.buffer.size = 5242880
	share.coordinator.snapshot.update.records.per.snapshot = 500
	share.coordinator.state.topic.compression.codec = 0
	share.coordinator.state.topic.min.isr = 2
	share.coordinator.state.topic.num.partitions = 50
	share.coordinator.state.topic.prune.interval.ms = 300000
	share.coordinator.state.topic.replication.factor = 3
	share.coordinator.state.topic.segment.bytes = 104857600
	share.coordinator.threads = 1
	share.coordinator.write.timeout.ms = 5000
	share.fetch.max.fetch.records = 2147483647
	share.fetch.purgatory.purge.interval.requests = 1000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	throughput.quota.window.num = 11
	token.impersonation.validation = true
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.metadata.load.threads = 32
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unclean.leader.election.interval.ms = 300000
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,375] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,375] INFO [ControllerServer id=1] The request from broker 1 to unfence has been granted because it has caught up with the offset of its register broker record 5. (org.apache.kafka.controller.BrokerHeartbeatManager:%L)
[2025-06-22 05:26:39,375] INFO [ControllerServer id=1] Replayed BrokerRegistrationChangeRecord modifying the registration for broker 1: BrokerRegistrationChangeRecord(brokerId=1, brokerEpoch=5, fenced=-1, inControlledShutdown=0, degradedComponents=null, logDirs=[]) (org.apache.kafka.controller.ClusterControlManager:%L)
[2025-06-22 05:26:39,387] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = true
	events.enabled = true
	filtering.enabled = false
	filtering.routes.allowed = []
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,395] INFO Creating kafka exporter named '_local' (io.confluent.telemetry.reporter.TelemetryReporter:%L)
[2025-06-22 05:26:39,395] INFO Kafka Exporter _local getting producer client  (io.confluent.telemetry.exporter.kafka.KafkaExporter:%L)
[2025-06-22 05:26:39,395] INFO Creating new non-static producer client (io.confluent.telemetry.exporter.kafka.KafkaClientFactory:%L)
[2025-06-22 05:26:39,396] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:29092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = zstd
	compression.zstd.level = 3
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = class io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.telemetry.serde.OpenTelemetryMetricsSerde
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,397] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,397] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,397] INFO Kafka startTimeMs: 1750569999397 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,398] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:26:39,398] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Connection to node -1 (localhost/127.0.0.1:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:26:39,398] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Bootstrap broker localhost:29092 (id: -1 rack: null isFenced: false) disconnected (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:26:39,401] INFO SBC Event SbcMetadataUpdateEvent-8 generated 1 more events to enqueue in the following order - [SbcKraftBrokerAdditionEvent-9]. Enqueuing... (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,401] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO Balancer notified of a config change: ConfigurationsDelta(changes={}) (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO Configs metadata not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO Handling event SbcKraftBrokerAdditionEvent-9 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,402] INFO Topics Image not present, pausing broker addition event of brokers (new brokers: [1]) until it is received. (io.confluent.databalancer.event.SbcKraftBrokerAdditionEvent:%L)
[2025-06-22 05:26:39,402] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager:%L)
[2025-06-22 05:26:39,402] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,402] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer:%L)
[2025-06-22 05:26:39,402] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor:%L)
[2025-06-22 05:26:39,403] INFO Awaiting socket connections on localhost:29092. (kafka.network.DataPlaneAcceptor:%L)
[2025-06-22 05:26:39,403] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,403] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,403] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,403] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:39,405] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter:%L)
[2025-06-22 05:26:39,405] INFO KafkaConfig values: 
	add.partitions.to.txn.retry.backoff.max.ms = 100
	add.partitions.to.txn.retry.backoff.ms = 20
	advertised.listeners = PLAINTEXT://localhost:29092,PLAINTEXT_HOST://localhost:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = f4iDkhvfTGqMLnNPxFcjWw
	client.quota.callback.class = null
	client.quota.max.throttle.time.in.response.ms = 60000
	client.quota.max.throttle.time.ms = 5000
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	confluent.accp.enabled = false
	confluent.acks.equal.to.one.request.replication.lag.threshold.ms = -1
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.api.visibility = DEFAULT
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
	confluent.balancer.capacity.threshold.upper.limit = 0.95
	confluent.balancer.cell.load.upper.bound = 0.7
	confluent.balancer.cell.overload.detection.interval.ms = 3600000
	confluent.balancer.cell.overload.duration.ms = 86400000
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.cpu.balance.threshold = 1.1
	confluent.balancer.cpu.goal.act.as.capacity.goal = false
	confluent.balancer.cpu.low.utilization.threshold = 0.2
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = false
	confluent.balancer.enable.network.capacity.metric.ingestion = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.flex.fanout.network.capacity.metrics.avg.period.ms = 1800000
	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.min.valid.windows = 5
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.inter.cell.balancing.enabled = false
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.minimum.reported.brokers.with.network.capacity.metrics.percentage = 0.8
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.num.concurrent.replica.movements.as.destination.per.broker = 18
	confluent.balancer.num.concurrent.replica.movements.as.source.per.broker = 12
	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.sbc.metrics.parser.enabled = false
	confluent.balancer.self.healing.maximum.rounds = 1
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.tenant.maximum.movements = 0
	confluent.balancer.tenant.suspension.ms = 86400000
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.balancing.itrdg.with.hard.goals.enabled = false
	confluent.balancer.topic.partition.maximum.movements = 3
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.movements.history.limit = 900
	confluent.balancer.topic.partition.suspension.ms = 3600000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.balancer.v2.addition.enabled = false
	confluent.balancer.v2.addition.reassignment.cancellations.enabled = false
	confluent.balancer.v2.executor.enabled = false
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.assertion.claim.aud = null
	confluent.bearer.assertion.claim.exp.minutes = null
	confluent.bearer.assertion.claim.iss = null
	confluent.bearer.assertion.claim.jti.include = null
	confluent.bearer.assertion.claim.nbf.include = null
	confluent.bearer.assertion.claim.sub = null
	confluent.bearer.assertion.file = null
	confluent.bearer.assertion.private.key.file = null
	confluent.bearer.assertion.private.key.passphrase = null
	confluent.bearer.assertion.template.file = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.advertised.limit.load = 0.8
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.tenant.metric.enable = false
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.calling.resource.identity.type.map = 
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.full.configs.enable = false
	confluent.catalog.collector.max.bytes.per.snapshot = 850000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.multitenant.topics.enable = true
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cell.metrics.refresh.period.ms = 60000
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.k2.base.broker.index = -1
	confluent.cells.load.refresher.enable = true
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.client.topic.max.metrics.count = 1000
	confluent.client.topic.metrics.expiry.sec = 3600
	confluent.client.topic.metrics.manager = class org.apache.kafka.server.metrics.ClientTopicMetricsManager$NoOpClientTopicMetricsManager
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.list.object.thread_pool.size = 1
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.admin.max.in.flight.requests = 1000
	confluent.cluster.link.admin.request.batch.size = 1
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.allow.legacy.message.format = false
	confluent.cluster.link.allow.truncation.below.hwm = false
	confluent.cluster.link.availability.check.mode = ALL
	confluent.cluster.link.background.thread.affinity = LINK
	confluent.cluster.link.bootstrap.translation.feature.enable = true
	confluent.cluster.link.clients.max.idle.ms = 3153600000000
	confluent.cluster.link.enable = true
	confluent.cluster.link.enable.local.admin = false
	confluent.cluster.link.enable.metrics.reduction = false
	confluent.cluster.link.enable.metrics.reduction.advanced = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.fetcher.auto.tune.enable = false
	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
	confluent.cluster.link.insync.fetch.response.min.bytes = 1
	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.intranet.connectivity.denied.org.ids = []
	confluent.cluster.link.intranet.connectivity.enable = false
	confluent.cluster.link.intranet.connectivity.migration.enable = false
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.local.admin.multitenant.enable = false
	confluent.cluster.link.local.reverse.connection.listener.map = null
	confluent.cluster.link.max.client.connections = 2147483647
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.mirror.transition.batch.size = 10
	confluent.cluster.link.num.background.threads = 1
	confluent.cluster.link.periodic.task.batch.size = 2147483647
	confluent.cluster.link.periodic.task.min.interval.ms = 1000
	confluent.cluster.link.persistent.connection.backoff.max.ms = 0
	confluent.cluster.link.replica.fetch.connections.mode = combined
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.cluster.link.request.quota.capacity = 400
	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
	confluent.cluster.link.switchover.disabled.principals = []
	confluent.cluster.link.switchover.enable = false
	confluent.cluster.link.switchover.listeners = []
	confluent.cluster.link.switchover.server.states = []
	confluent.cluster.link.tenant.replication.quota.enable = false
	confluent.cluster.link.tenant.request.quota.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
	confluent.cluster.metadata.snapshot.tier.upload.enable = false
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
	confluent.consumer.fetch.partition.pruning.enable = true
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.dataflow.policy.watch.monitor.ms = 300000
	confluent.default.data.policy.enforcement = true
	confluent.defer.isr.shrink.enable = false
	confluent.describe.topic.partitions.enabled = true
	confluent.disk.io.manager.enable = false
	confluent.disk.throughput.headroom = 10485760
	confluent.disk.throughput.limit = 10485760000
	confluent.disk.throughput.quota.tier.archive = 1048576000
		confluent.disk.throughput.quota.tier.archive.throttled = 104857600
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.log.ignored.event.types = 
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 1
	confluent.e2e_checksum.protection.enabled = false
	confluent.e2e_checksum.protection.files = [none]
	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
	confluent.elastic.cku.enabled = false
	confluent.elastic.cku.scaletozero.enabled = false
	confluent.eligible.controllers = []
	confluent.enable.broker.reporting.min.usage.mode = true
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fail.unsatisfied.placement.constraints = false
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.flexible.fanout.broker.max.fetch.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.max.produce.bytes.per.second = 9223372036854775807
	confluent.flexible.fanout.broker.min.producer.percentage = 10.0
	confluent.flexible.fanout.broker.network.out.bytes.per.second = 6200000
	confluent.flexible.fanout.broker.recompute.interval.ms = 30000
	confluent.flexible.fanout.broker.storage.bytes.per.second = 512000000
	confluent.flexible.fanout.enabled = false
	confluent.flexible.fanout.lazy.evaluation.threshold = 0.5
	confluent.flexible.fanout.mode = TENANT_QUOTA
	confluent.floor.connection.rate.per.ip = -1.0
	confluent.floor.connection.rate.per.tenant = -1.0
	confluent.group.coordinator.dynamic.append.linger.enable = false
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.coordinator.txn.offset.validation.enable = false
	confluent.group.highest.offset.commit.rates.log.count = 10
	confluent.group.highest.offset.commit.rates.log.enable = false
	confluent.group.highest.offset.commit.rates.log.interval.ms = 300000
	confluent.group.metadata.load.threads = 32
	confluent.group.subscription.pattern.log.interval.ms = -1
	confluent.heap.tenured.notify.bytes = 0
	confluent.heap.tenured.notify.enabled = false
	confluent.hot.partition.ratio = 0.8
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.internal.rest.server.ssl.enable = false
	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
	confluent.leader.epoch.checkpoint.checksum.enabled = false
	confluent.listener.protocol = TCP
	confluent.log.cleaner.timestamp.validation.enable = true
	confluent.log.placement.constraints = 
	confluent.max.broker.load = 1.0
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
	confluent.max.connection.rate.per.ip = -1.0
	confluent.max.connection.rate.per.tenant = -1.0
	confluent.max.connection.throttle.ms = null
	confluent.max.segment.ms = 9223372036854775807
	confluent.metadata.active.encryptor = null
	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.required = false
	confluent.metadata.encryptor.secret.file = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.jvm.warmup.ms = 60000
	confluent.metadata.leader.balance.slice.delay.ms = 100
	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
	confluent.metadata.max.leader.balance.changes.per.slice = 1000
	confluent.metadata.reject.when.throttled.enable = false
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.connection.throttle.ms = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 20000
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.mtls.build.client.cert.chain.enable = false
	confluent.mtls.enable = false
	confluent.mtls.listener.name = EXTERNAL
	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
	confluent.mtls.truststore.alter.configs.timeout.ms = 300000
	confluent.mtls.truststore.manager.class.name = null
	confluent.multitenant.authorizer.enable.acl.state = false
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.lkc.id.enable = false
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.external.listener.name = EXTERNAL
	confluent.network.health.manager.externalconnectivitystartup.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.oauth.flat.networking.verification.enable = false
	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
	confluent.offsets.topic.placement.constraints = 
	confluent.omit.network.processor.metric.tag = false
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.ppv2.endpoint.scheme.bootstrap.broker.template.mappings = 
	confluent.ppv2.endpoint.scheme.enable = false
	confluent.ppv2.endpoint.scheme.map.broker.zone.to.gateway.zone = false
	confluent.ppv2.endpoint.scheme.template.variable.cloud = 
	confluent.ppv2.endpoint.scheme.template.variable.domain = 
	confluent.ppv2.endpoint.scheme.template.variable.region = 
	confluent.ppv2.endpoint.scheme.template.variables = 
	confluent.ppv2.endpoint.scheme.templates = 
	confluent.prefer.tier.fetch.ms = -1
	confluent.produce.throttle.pre.check.enable = false
	confluent.produce.throttle.pre.check.for.new.connection.enable = false
	confluent.producer.id.cache.broker.hard.limit = -1
	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
	confluent.producer.id.cache.extra.eviction.percentage = 0
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.cache.partition.hard.limit = -1
	confluent.producer.id.cache.tenant.hard.limit = -1
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.quota.window.num = 11
	confluent.producer.id.quota.window.size.seconds = 1
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.mode.local.default = false
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.computing.usage.adjustment = 0.5
	confluent.quota.dynamic.adjustment.min.usage = 102400
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.internal.broker.max.consumer.rate = 9223372036854775807
	confluent.quota.tenant.internal.broker.max.producer.rate = 9223372036854775807
	confluent.quota.tenant.internal.throttling.enable = false
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.rack.id.mapping = null
	confluent.regional.metadata.client.class = null
	confluent.regional.resource.manager.client.scheduler.threads = 2
	confluent.regional.resource.manager.endpoint = null
	confluent.regional.resource.manager.watch.endpoint = null
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.replication.mode = PULL
	confluent.replication.push.feature.enable = false
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = true
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.calling.resource.identity = false
	confluent.require.compatible.keystore.updates = true
	confluent.require.confluent.issuer = false
	confluent.roll.check.interval.ms = 300000
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validation.context.name.enable = false
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.bc.approved.mode.enable = false
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.config = 
	confluent.security.revoked.certificate.ids = 
	confluent.segment.eager.roll.enable = false
	confluent.segment.speculative.prefetch.enable = false
	confluent.share.metadata.load.threads = 32
	confluent.spiffe.id.principal.extraction.rules = 
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.step.connection.rate.per.ip = -1.0
	confluent.step.connection.rate.per.tenant = -1.0
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.subdomain.prefix = null
	confluent.subdomain.separator.map = null
	confluent.subdomain.separator.variable = %sep
	confluent.system.time.roll.enable = false
	confluent.telemetry.enabled = false
	confluent.telemetry.external.client.metrics.delta.temporality = true
	confluent.telemetry.external.client.metrics.instance.cache.size = 16384
	confluent.telemetry.external.client.metrics.push.enabled = false
	confluent.telemetry.external.client.metrics.subscription.interval.ms.list = null
	confluent.telemetry.external.client.metrics.subscription.match.list = null
	confluent.telemetry.external.client.metrics.subscription.metrics.list = null
	confluent.tenant.latency.metric.enabled = false
	confluent.tenantaware.encryption.key.manager.enable = false
	confluent.tenantaware.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.tenantaware.encryption.key.manager.tenant.cache.eviction.time.sec = 172800
	confluent.tenantaware.encryption.key.manager.tenant.cache.size = 100
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.async.enable = false
	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 1
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.prefetch.cache.enable = false
	confluent.tier.prefetch.cache.entry.size.bytes = 1048576
	confluent.tier.prefetch.cache.range.bytes = 5242880
	confluent.tier.prefetch.cache.total.size.bytes = 209715200
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.ipv6.enabled = true
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.security.providers = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.provider = null
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.storage.class.override = 
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.s3.v2.enabled = false
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
	confluent.tier.topic.data.loss.validation.fencing.enable = false
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.head.data.loss.validation.enable = true
	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
	confluent.tier.topic.materialization.from.snapshot.enable = false
	confluent.tier.topic.producer.enable.idempotence = true
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.metadata.throttle.pre.check.partition.count.threshold = 1000
	confluent.topic.partition.default.placement = 2
	confluent.topic.policy.use.computed.assignments = false
	confluent.topic.replica.assignor.builder.class = 
	confluent.track.api.key.per.ip = false
	confluent.track.per.ip.max.size = 100000
	confluent.track.tenant.id.per.ip = false
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.traffic.network.type = 
	confluent.transaction.2pc.timeout.ms = -1
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.unique.deprecated.request.metrics.per.tenant = 1000
	confluent.valid.broker.rack.set = null
	confluent.valid.sni.hostnames = 
	confluent.valid.sni.hostnames.exclude.suffix = 
	confluent.verify.group.subscription.prefix = false
	confluent.virtual.topic.creation.enabled = false
	confluent.zone.tagged.request.metrics.enable = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	controlled.shutdown.enable = true
	controller.listener.names = CONTROLLER
	controller.performance.always.log.threshold.ms = 2000
	controller.performance.sample.period.ms = 60000
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:29093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.consumer.assignors = [uniform, range]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = bidirectional
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 5
	group.coordinator.new.enable = true
	group.coordinator.rebalance.protocols = [classic, consumer]
	group.coordinator.threads = 4
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	group.share.delivery.count.limit = 5
	group.share.enable = false
	group.share.heartbeat.interval.ms = 5000
	group.share.max.groups = 10
	group.share.max.heartbeat.interval.ms = 15000
	group.share.max.record.lock.duration.ms = 60000
	group.share.max.session.timeout.ms = 60000
	group.share.max.size = 200
	group.share.min.heartbeat.interval.ms = 5000
	group.share.min.record.lock.duration.ms = 15000
	group.share.min.session.timeout.ms = 45000
	group.share.partition.max.record.locks = 200
	group.share.persister.class.name = org.apache.kafka.server.share.persister.DefaultStatePersister
	group.share.record.lock.duration.ms = 30000
	group.share.session.timeout.ms = 45000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	k2.stack.builder.class.name = null
	k2.startup.timeout.ms = 60000
	k2.topic.metadata.refresh.ms = 10000
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://localhost:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.cleanup.policy.empty.validation = none
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.timestamp.after.max.ms = 3600000
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.connections.per.tenant = 0
	max.connections.protected.listeners = []
	max.connections.reap.amount = 0
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.authorizer.support.resource.ids = false
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 2
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	otel.exporter.otlp.custom.endpoint = default
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.fetch.max.wait.ms = 500
	remote.list.offsets.request.timeout.ms = 30000
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 2
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	share.coordinator.append.linger.ms = 5
	share.coordinator.load.buffer.size = 5242880
	share.coordinator.snapshot.update.records.per.snapshot = 500
	share.coordinator.state.topic.compression.codec = 0
	share.coordinator.state.topic.min.isr = 2
	share.coordinator.state.topic.num.partitions = 50
	share.coordinator.state.topic.prune.interval.ms = 300000
	share.coordinator.state.topic.replication.factor = 3
	share.coordinator.state.topic.segment.bytes = 104857600
	share.coordinator.threads = 1
	share.coordinator.write.timeout.ms = 5000
	share.fetch.max.fetch.records = 2147483647
	share.fetch.purgatory.purge.interval.requests = 1000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	throughput.quota.window.num = 11
	token.impersonation.validation = true
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.metadata.load.threads = 32
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unclean.leader.election.interval.ms = 300000
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,407] INFO Unexpected credentials store injected: null (io.confluent.kafkarest.servlet.KafkaRestApplicationProvider:%L)
[2025-06-22 05:26:39,407] INFO For rest-app with listener null, configuring custom request logging (io.confluent.kafkarest.KafkaRestApplication:%L)
[2025-06-22 05:26:39,410] INFO EventEmitterConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,410] INFO EventEmitterConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,410] INFO Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,411] INFO ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.cluster.id = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.external.client.metrics.exclude.labels = 
	confluent.telemetry.metrics.collector.include = .*io\.confluent\.system/(?:.*/)?(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO HttpClientConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = httpTelemetryClient
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client = _confluentClient
	client.attempts.max = null
	client.base.url = null
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	remote.configurable = true
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig:%L)
[2025-06-22 05:26:39,411] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,411] INFO PollingRemoteConfigurationConfig values: 
	enabled = true
	refresh.interval.ms = 60000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] WARN Ignoring redefinition of existing telemetry label kafka_rest.version (io.confluent.telemetry.ResourceBuilderFacade:%L)
[2025-06-22 05:26:39,411] INFO Applying value of confluent.telemetry.enabled flag for default '_confluent' http exporter as confluent.telemetry.exporter._confluent.enabled isn't passed (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,411] INFO ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.cluster.id = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.external.client.metrics.exclude.labels = 
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io\.confluent\.system/(?:.*/)?(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO HttpClientConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = httpTelemetryClient
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client = _confluentClient
	client.attempts.max = null
	client.base.url = null
	client.compression = null
	client.connect.timeout.ms = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	remote.configurable = true
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig:%L)
[2025-06-22 05:26:39,411] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig:%L)
[2025-06-22 05:26:39,411] INFO PollingRemoteConfigurationConfig values: 
	enabled = true
	refresh.interval.ms = 60000
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter:%L)
[2025-06-22 05:26:39,411] ERROR Unable to submit events without credentials (io.confluent.telemetry.events.exporter.http.HttpExporter:%L)
[2025-06-22 05:26:39,411] INFO EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.telemetry.events.exporter.http.EventHttpExporter
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,411] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = true
	events.enabled = true
	filtering.enabled = false
	filtering.routes.allowed = []
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,412] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = true
	events.enabled = true
	filtering.enabled = false
	filtering.routes.allowed = []
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,418] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter:%L)
[2025-06-22 05:26:39,419] ERROR Unable to submit events without credentials (io.confluent.telemetry.events.exporter.http.HttpExporter:%L)
[2025-06-22 05:26:39,419] INFO Application provider 'KafkaRestApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader:%L)
[2025-06-22 05:26:39,420] INFO Application provider 'MetadataApiApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader:%L)
[2025-06-22 05:26:39,420] INFO MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = null
	confluent.metadata.server.enable = false
	confluent.metadata.server.kraft.controller.enabled = false
	confluent.metadata.server.listeners = null
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,420] INFO Application provider 'RbacApplicationProvider' did not provide any instances. (io.confluent.http.server.KafkaHttpApplicationLoader:%L)
[2025-06-22 05:26:39,422] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer:%L)
[2025-06-22 05:26:39,448] INFO Adding listener with HTTP/2: NamedURI{uri=http://0.0.0.0:8090, name='null'} (io.confluent.rest.ApplicationServer:%L)
[2025-06-22 05:26:39,458] INFO DynamicMetricsReporters initiated successfully. (kafka.server.DynamicMetricsReportersScheduler:%L)
[2025-06-22 05:26:39,458] INFO Stopping DynamicMetricsReportersScheduler. (kafka.server.DynamicMetricsReportersScheduler:%L)
[2025-06-22 05:26:39,463] INFO Loaded KafkaHttpServer implementation class io.confluent.http.server.KafkaHttpServerImpl (io.confluent.kafka.http.server.KafkaHttpServerLoader:%L)
[2025-06-22 05:26:39,463] INFO KafkaHttpServer transitioned from NEW to STARTING.. (io.confluent.http.server.KafkaHttpServerImpl:%L)
[2025-06-22 05:26:39,468] INFO Registered CombinedNetworkTrafficListener to network connector null of listener: null (io.confluent.rest.ApplicationServer:%L)
[2025-06-22 05:26:39,474] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig:%L)
[2025-06-22 05:26:39,475] INFO SchemaRegistryConfig values: 
	auto.register.schemas = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.schema.id.deserializer = class io.confluent.kafka.serializers.schema.id.DualSchemaIdDeserializer
	key.schema.id.serializer = class io.confluent.kafka.serializers.schema.id.PrefixSchemaIdSerializer
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.retries = 3
	max.schemas.per.subject = 1000
	normalize.schemas = false
	propagate.schema.tags = false
	proxy.host = 
	proxy.port = -1
	retries.max.wait.ms = 20000
	retries.wait.ms = 1000
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	schema.registry.url.randomize = false
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.schema.id.deserializer = class io.confluent.kafka.serializers.schema.id.DualSchemaIdDeserializer
	value.schema.id.serializer = class io.confluent.kafka.serializers.schema.id.PrefixSchemaIdSerializer
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,479] INFO Binding EmbeddedKafkaRestApplication to all listeners. (io.confluent.rest.Application:%L)
[2025-06-22 05:26:39,479] INFO Registered CombinedNetworkTrafficListener to network connector null of listener: null (io.confluent.rest.ApplicationServer:%L)
[2025-06-22 05:26:39,480] INFO [ControllerServer id=1] Using the default placer, StripedReplicaPlacer, to make the assignment for topic _confluent-link-metadata. (kafka.assignor.ConfluentReplicaPlacer:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=1, assignments=[], configs=[CreatableTopicConfig(name='cleanup.policy', value='compact'), CreatableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-link-metadata with topic ID G-9S9N2rSd2_OyvjHChE3w. (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-link-metadata') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-link-metadata') which set configuration min.insync.replicas to 2 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-0 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-1 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,481] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-2 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO Binding MetadataApiApplication to all listeners. (io.confluent.rest.Application:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-3 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-4 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-5 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-6 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-7 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-8 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-9 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-10 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-11 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-12 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-13 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,482] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-14 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-15 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-16 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-17 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-18 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-19 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-20 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-21 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-22 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-23 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-24 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-25 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-26 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-27 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,483] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-28 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-29 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-30 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-31 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-32 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-33 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-34 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-35 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-36 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-37 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-38 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-39 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-40 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-41 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-42 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,484] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-43 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-44 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-45 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-46 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-47 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-48 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,485] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-49 with topic ID G-9S9N2rSd2_OyvjHChE3w and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,514] INFO [Broker id=1] Transitioning 50 partition(s) to local leaders. (state.change.logger:%L)
[2025-06-22 05:26:39,514] INFO SBC Event SbcMetadataUpdateEvent-11 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-12]. Enqueuing... (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,514] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,514] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-link-metadata-43, _confluent-link-metadata-10, _confluent-link-metadata-39, _confluent-link-metadata-6, _confluent-link-metadata-18, _confluent-link-metadata-47, _confluent-link-metadata-14, _confluent-link-metadata-27, _confluent-link-metadata-23, _confluent-link-metadata-35, _confluent-link-metadata-2, _confluent-link-metadata-31, _confluent-link-metadata-42, _confluent-link-metadata-13, _confluent-link-metadata-38, _confluent-link-metadata-9, _confluent-link-metadata-21, _confluent-link-metadata-46, _confluent-link-metadata-17, _confluent-link-metadata-26, _confluent-link-metadata-22, _confluent-link-metadata-34, _confluent-link-metadata-5, _confluent-link-metadata-30, _confluent-link-metadata-1, _confluent-link-metadata-45, _confluent-link-metadata-12, _confluent-link-metadata-41, _confluent-link-metadata-8, _confluent-link-metadata-20, _confluent-link-metadata-49, _confluent-link-metadata-16, _confluent-link-metadata-29, _confluent-link-metadata-25, _confluent-link-metadata-37, _confluent-link-metadata-4, _confluent-link-metadata-33, _confluent-link-metadata-0, _confluent-link-metadata-11, _confluent-link-metadata-44, _confluent-link-metadata-7, _confluent-link-metadata-40, _confluent-link-metadata-19, _confluent-link-metadata-15, _confluent-link-metadata-48, _confluent-link-metadata-28, _confluent-link-metadata-24, _confluent-link-metadata-3, _confluent-link-metadata-36, _confluent-link-metadata-32) (kafka.server.ReplicaFetcherManager:%L)
[2025-06-22 05:26:39,514] INFO [Broker id=1] Creating new partition _confluent-link-metadata-43 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,514] INFO Balancer Status state for brokers [1] transitioned from BALANCER_EVENT_RECEIVED to DISABLED due to event BALANCER_DISABLED. (io.confluent.databalancer.operation.StateMachine:%L)
[2025-06-22 05:26:39,514] INFO DataBalancer: Skipping DataBalancer startup. BalancerEnabledConfig: BalancerEnabledConfig{isEnabled=false} (io.confluent.databalancer.KafkaDataBalanceManager:%L)
[2025-06-22 05:26:39,514] INFO Handling event SbcKraftBrokerAdditionEvent-9 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,514] INFO Processing SbcKraftBrokerAdditionEvent-9 event with data: empty_brokers: [], new_brokers: [1] (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,515] WARN Notified of broker additions (empty broker ids [], new brokers [1]) but DataBalancer is disabled -- ignoring for now (io.confluent.databalancer.KafkaDataBalanceManager:%L)
[2025-06-22 05:26:39,515] INFO Handling event SbcConfigUpdateEvent-12 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,515] INFO Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":3,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":1,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":29999,"validateOnly":false},"response":{"responseData":"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","errorCode":0},"connection":"127.0.0.1:29093-127.0.0.1:34176-2-0","clientAddress":"127.0.0.1","totalTimeMs":35.039,"requestQueueTimeMs":0.04,"localTimeMs":0.083,"remoteTimeMs":34.695,"throttleTimeMs":0,"responseQueueTimeMs":0.106,"sendTimeMs":0.114,"sendIoTimeMs":0.074,"responseSize":2544,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":175056999947900002} (kafka.request.logger:%L)
[2025-06-22 05:26:39,515] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-link-metadata')=ConfigurationDelta(changedKeys=[cleanup.policy, min.insync.replicas])}) (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,515] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,515] INFO Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":3,"clientId":"cluster-link--local-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-link-metadata","numPartitions":50,"replicationFactor":1,"assignments":[],"configs":[{"name":"cleanup.policy","value":"compact"},{"name":"min.insync.replicas","value":"2"}]}],"timeoutMs":29999,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-link-metadata","topicId":"G-9S9N2rSd2_OyvjHChE3w","errorCode":0,"errorMessage":null,"numPartitions":50,"replicationFactor":1,"configs":[{"configName":"cleanup.policy","value":"compact","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"compression.gzip.level","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.lz4.level","value":"9","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.type","value":"producer","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.zstd.level","value":"3","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.append.record.interceptor.classes","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.cluster.link.allow.legacy.message.format","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.compacted.topic.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.log.cleaner.timestamp.validation.enable","value":"true","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.max.segment.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.min.segment.ms","value":"1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.placement.constraints","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.schema.validation.context.name.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.segment.speculative.prefetch.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.delete.delay.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.max.deletions.per.run","value":"72","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.system.time.roll.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.min.efficiency","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.segment.min.bytes","value":"20971520","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.dual.compaction","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.min.cleanable.ratio","value":"0.75","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.bytes","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.segment.hotset.roll.min.bytes","value":"104857600","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.topic.type","value":"standard","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"delete.retention.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"file.delete.delay.ms","value":"60000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.messages","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"follower.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"index.interval.bytes","value":"4096","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"leader.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.bytes","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.ms","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.compaction.lag.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.message.bytes","value":"1048588","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.after.max.ms","value":"3600000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.before.max.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.type","value":"CreateTime","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.cleanable.dirty.ratio","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.compaction.lag.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.insync.replicas","value":"2","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"preallocate","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.copy.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.delete.on.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.storage.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"retention.bytes","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"retention.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.bytes","value":"1073741824","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.index.bytes","value":"10485760","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.jitter.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"unclean.leader.election.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false}]}]},"connection":"127.0.0.1:29092-127.0.0.1:53406-1-0","clientAddress":"127.0.0.1","totalTimeMs":37.78,"requestQueueTimeMs":0.03,"localTimeMs":0.801,"remoteTimeMs":36.686,"throttleTimeMs":0,"responseQueueTimeMs":0.088,"sendTimeMs":0.174,"sendIoTimeMs":0.142,"responseSize":2534,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"8.0.0-0-ce"},"isDisconnectedClient":false,"requestId":175056999947700101} (kafka.request.logger:%L)
[2025-06-22 05:26:39,515] INFO [Broker id=1] Creating new partition _confluent-link-metadata-10 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,515] INFO [Broker id=1] Creating new partition _confluent-link-metadata-39 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-6 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-18 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-47 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-14 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-27 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-23 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-35 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-2 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-31 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,517] INFO jetty-12.0.16; built: 2024-12-09T21:02:54.535Z; git: c3f88bafb4e393f23204dc14dc57b042e84debc7; jvm 21.0.2+13-jvmci-23.1-b30 (org.eclipse.jetty.server.Server:%L)
[2025-06-22 05:26:39,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-42 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-13 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,518] INFO [Broker id=1] Creating new partition _confluent-link-metadata-38 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,518] INFO [Broker id=1] Creating new partition _confluent-link-metadata-9 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,519] INFO [Broker id=1] Creating new partition _confluent-link-metadata-21 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,520] INFO [Broker id=1] Creating new partition _confluent-link-metadata-46 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,520] INFO [Broker id=1] Creating new partition _confluent-link-metadata-17 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,520] INFO [Broker id=1] Creating new partition _confluent-link-metadata-26 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,520] INFO [Broker id=1] Creating new partition _confluent-link-metadata-22 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,521] INFO [Broker id=1] Creating new partition _confluent-link-metadata-34 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,521] INFO [Broker id=1] Creating new partition _confluent-link-metadata-5 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,521] INFO [Broker id=1] Creating new partition _confluent-link-metadata-30 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,521] INFO [Broker id=1] Creating new partition _confluent-link-metadata-1 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,522] INFO [Broker id=1] Creating new partition _confluent-link-metadata-45 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,522] INFO [Broker id=1] Creating new partition _confluent-link-metadata-12 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,522] INFO [Broker id=1] Creating new partition _confluent-link-metadata-41 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-8 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-20 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-49 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-16 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-29 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-25 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-37 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-4 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-33 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-0 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-11 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,525] INFO Session workerName=node0 (org.eclipse.jetty.session.DefaultSessionIdManager:%L)
[2025-06-22 05:26:39,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-44 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-7 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-40 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-19 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,525] INFO Started oeje10s.ServletContextHandler@6d711502{/kafka,/kafka,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@2a49570f{STARTED}} (org.eclipse.jetty.server.handler.ContextHandler:%L)
[2025-06-22 05:26:39,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-15 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-48 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-28 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-24 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-3 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-36 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,527] INFO [Broker id=1] Creating new partition _confluent-link-metadata-32 with topic id G-9S9N2rSd2_OyvjHChE3w. (state.change.logger:%L)
[2025-06-22 05:26:39,527] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 50 partitions (state.change.logger:%L)
[2025-06-22 05:26:39,530] INFO [MergedLog partition=_confluent-link-metadata-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,531] INFO Created log for partition _confluent-link-metadata-25 in /var/lib/kafka/data/_confluent-link-metadata-25 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,531] INFO [Partition _confluent-link-metadata-25 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-25 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,531] INFO [Partition _confluent-link-metadata-25 broker=1] Log loaded for partition _confluent-link-metadata-25 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,531] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-25 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,531] INFO [MergedLog partition=_confluent-link-metadata-25, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,532] INFO [Broker id=1] Leader _confluent-link-metadata-25 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,532] INFO [MergedLog partition=_confluent-link-metadata-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,532] INFO Created log for partition _confluent-link-metadata-21 in /var/lib/kafka/data/_confluent-link-metadata-21 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,532] INFO [Partition _confluent-link-metadata-21 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-21 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,532] INFO [Partition _confluent-link-metadata-21 broker=1] Log loaded for partition _confluent-link-metadata-21 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,532] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-21 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,532] INFO [MergedLog partition=_confluent-link-metadata-21, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,532] INFO [Broker id=1] Leader _confluent-link-metadata-21 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,534] INFO [MergedLog partition=_confluent-link-metadata-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,534] INFO Created log for partition _confluent-link-metadata-24 in /var/lib/kafka/data/_confluent-link-metadata-24 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,534] INFO [Partition _confluent-link-metadata-24 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-24 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,534] INFO [Partition _confluent-link-metadata-24 broker=1] Log loaded for partition _confluent-link-metadata-24 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,534] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-24 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,534] INFO [MergedLog partition=_confluent-link-metadata-24, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,535] INFO [Broker id=1] Leader _confluent-link-metadata-24 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,535] INFO [MergedLog partition=_confluent-link-metadata-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,536] INFO Created log for partition _confluent-link-metadata-23 in /var/lib/kafka/data/_confluent-link-metadata-23 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,536] INFO [Partition _confluent-link-metadata-23 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-23 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,536] INFO [Partition _confluent-link-metadata-23 broker=1] Log loaded for partition _confluent-link-metadata-23 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,536] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-23 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,536] INFO [MergedLog partition=_confluent-link-metadata-23, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,536] INFO [Broker id=1] Leader _confluent-link-metadata-23 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,536] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig:%L)
[2025-06-22 05:26:39,536] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig:%L)
[2025-06-22 05:26:39,537] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig:%L)
[2025-06-22 05:26:39,538] INFO [MergedLog partition=_confluent-link-metadata-41, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,538] INFO Created log for partition _confluent-link-metadata-41 in /var/lib/kafka/data/_confluent-link-metadata-41 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,538] INFO [Partition _confluent-link-metadata-41 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-41 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,538] INFO [Partition _confluent-link-metadata-41 broker=1] Log loaded for partition _confluent-link-metadata-41 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,538] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-41 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,538] INFO [MergedLog partition=_confluent-link-metadata-41, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,538] INFO [Broker id=1] Leader _confluent-link-metadata-41 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,538] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig:%L)
[2025-06-22 05:26:39,540] INFO [MergedLog partition=_confluent-link-metadata-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,540] INFO Created log for partition _confluent-link-metadata-20 in /var/lib/kafka/data/_confluent-link-metadata-20 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,540] INFO [Partition _confluent-link-metadata-20 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-20 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,540] INFO [Partition _confluent-link-metadata-20 broker=1] Log loaded for partition _confluent-link-metadata-20 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,540] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-20 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,541] INFO [MergedLog partition=_confluent-link-metadata-20, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,541] INFO [Broker id=1] Leader _confluent-link-metadata-20 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,542] INFO [MergedLog partition=_confluent-link-metadata-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,543] INFO Created log for partition _confluent-link-metadata-7 in /var/lib/kafka/data/_confluent-link-metadata-7 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,543] INFO [Partition _confluent-link-metadata-7 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-7 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,543] INFO [Partition _confluent-link-metadata-7 broker=1] Log loaded for partition _confluent-link-metadata-7 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,543] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-7 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,543] INFO [MergedLog partition=_confluent-link-metadata-7, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,543] INFO [Broker id=1] Leader _confluent-link-metadata-7 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,544] INFO [MergedLog partition=_confluent-link-metadata-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,544] INFO Created log for partition _confluent-link-metadata-19 in /var/lib/kafka/data/_confluent-link-metadata-19 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,544] INFO [Partition _confluent-link-metadata-19 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-19 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,544] INFO [Partition _confluent-link-metadata-19 broker=1] Log loaded for partition _confluent-link-metadata-19 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,544] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-19 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,544] INFO [MergedLog partition=_confluent-link-metadata-19, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,544] INFO [Broker id=1] Leader _confluent-link-metadata-19 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,547] INFO [MergedLog partition=_confluent-link-metadata-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,548] INFO Created log for partition _confluent-link-metadata-45 in /var/lib/kafka/data/_confluent-link-metadata-45 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,548] INFO [Partition _confluent-link-metadata-45 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-45 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,548] INFO [Partition _confluent-link-metadata-45 broker=1] Log loaded for partition _confluent-link-metadata-45 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,548] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-45 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,548] INFO [MergedLog partition=_confluent-link-metadata-45, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,548] INFO [Broker id=1] Leader _confluent-link-metadata-45 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,549] INFO [MergedLog partition=_confluent-link-metadata-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,549] INFO Created log for partition _confluent-link-metadata-33 in /var/lib/kafka/data/_confluent-link-metadata-33 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,549] INFO [Partition _confluent-link-metadata-33 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-33 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,549] INFO [Partition _confluent-link-metadata-33 broker=1] Log loaded for partition _confluent-link-metadata-33 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,549] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-33 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,549] INFO [MergedLog partition=_confluent-link-metadata-33, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,549] INFO [Broker id=1] Leader _confluent-link-metadata-33 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,550] INFO [MergedLog partition=_confluent-link-metadata-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,551] INFO Created log for partition _confluent-link-metadata-29 in /var/lib/kafka/data/_confluent-link-metadata-29 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,551] INFO [Partition _confluent-link-metadata-29 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-29 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,551] INFO [Partition _confluent-link-metadata-29 broker=1] Log loaded for partition _confluent-link-metadata-29 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,551] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-29 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,551] INFO [MergedLog partition=_confluent-link-metadata-29, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,551] INFO [Broker id=1] Leader _confluent-link-metadata-29 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,552] INFO [MergedLog partition=_confluent-link-metadata-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,552] INFO Created log for partition _confluent-link-metadata-46 in /var/lib/kafka/data/_confluent-link-metadata-46 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,552] INFO [Partition _confluent-link-metadata-46 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-46 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,552] INFO [Partition _confluent-link-metadata-46 broker=1] Log loaded for partition _confluent-link-metadata-46 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,552] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-46 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,552] INFO [MergedLog partition=_confluent-link-metadata-46, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,552] INFO [Broker id=1] Leader _confluent-link-metadata-46 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,553] INFO [MergedLog partition=_confluent-link-metadata-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,553] INFO Created log for partition _confluent-link-metadata-28 in /var/lib/kafka/data/_confluent-link-metadata-28 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,553] INFO [Partition _confluent-link-metadata-28 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-28 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,553] INFO [Partition _confluent-link-metadata-28 broker=1] Log loaded for partition _confluent-link-metadata-28 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,553] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-28 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,553] INFO [MergedLog partition=_confluent-link-metadata-28, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,553] INFO [Broker id=1] Leader _confluent-link-metadata-28 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,554] INFO [MergedLog partition=_confluent-link-metadata-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,555] INFO Created log for partition _confluent-link-metadata-27 in /var/lib/kafka/data/_confluent-link-metadata-27 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,555] INFO [Partition _confluent-link-metadata-27 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-27 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,555] INFO [Partition _confluent-link-metadata-27 broker=1] Log loaded for partition _confluent-link-metadata-27 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,555] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-27 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,555] INFO [MergedLog partition=_confluent-link-metadata-27, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,555] INFO [Broker id=1] Leader _confluent-link-metadata-27 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,556] INFO [MergedLog partition=_confluent-link-metadata-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,556] INFO Created log for partition _confluent-link-metadata-8 in /var/lib/kafka/data/_confluent-link-metadata-8 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,556] INFO [Partition _confluent-link-metadata-8 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-8 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,556] INFO [Partition _confluent-link-metadata-8 broker=1] Log loaded for partition _confluent-link-metadata-8 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,556] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-8 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,556] INFO [MergedLog partition=_confluent-link-metadata-8, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,556] INFO [Broker id=1] Leader _confluent-link-metadata-8 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,557] INFO [MergedLog partition=_confluent-link-metadata-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,557] INFO Created log for partition _confluent-link-metadata-49 in /var/lib/kafka/data/_confluent-link-metadata-49 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,557] INFO [Partition _confluent-link-metadata-49 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-49 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,557] INFO [Partition _confluent-link-metadata-49 broker=1] Log loaded for partition _confluent-link-metadata-49 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,557] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-49 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,557] INFO [MergedLog partition=_confluent-link-metadata-49, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,557] INFO [Broker id=1] Leader _confluent-link-metadata-49 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,558] INFO [MergedLog partition=_confluent-link-metadata-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,558] INFO Created log for partition _confluent-link-metadata-40 in /var/lib/kafka/data/_confluent-link-metadata-40 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,558] INFO [Partition _confluent-link-metadata-40 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-40 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,558] INFO [Partition _confluent-link-metadata-40 broker=1] Log loaded for partition _confluent-link-metadata-40 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,558] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-40 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,558] INFO [MergedLog partition=_confluent-link-metadata-40, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,558] INFO [Broker id=1] Leader _confluent-link-metadata-40 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,559] INFO [MergedLog partition=_confluent-link-metadata-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,559] INFO Created log for partition _confluent-link-metadata-15 in /var/lib/kafka/data/_confluent-link-metadata-15 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,559] INFO [Partition _confluent-link-metadata-15 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-15 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,559] INFO [Partition _confluent-link-metadata-15 broker=1] Log loaded for partition _confluent-link-metadata-15 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,559] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-15 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,559] INFO [MergedLog partition=_confluent-link-metadata-15, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,559] INFO [Broker id=1] Leader _confluent-link-metadata-15 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,561] INFO [MergedLog partition=_confluent-link-metadata-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,561] INFO Created log for partition _confluent-link-metadata-12 in /var/lib/kafka/data/_confluent-link-metadata-12 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,561] INFO [Partition _confluent-link-metadata-12 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-12 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,561] INFO [Partition _confluent-link-metadata-12 broker=1] Log loaded for partition _confluent-link-metadata-12 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,561] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-12 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,561] INFO [MergedLog partition=_confluent-link-metadata-12, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,561] INFO [Broker id=1] Leader _confluent-link-metadata-12 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,563] INFO [MergedLog partition=_confluent-link-metadata-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,563] INFO Created log for partition _confluent-link-metadata-0 in /var/lib/kafka/data/_confluent-link-metadata-0 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,563] INFO [Partition _confluent-link-metadata-0 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,563] INFO [Partition _confluent-link-metadata-0 broker=1] Log loaded for partition _confluent-link-metadata-0 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,563] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-0 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,563] INFO [MergedLog partition=_confluent-link-metadata-0, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,563] INFO [Broker id=1] Leader _confluent-link-metadata-0 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,564] INFO [MergedLog partition=_confluent-link-metadata-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,564] INFO Created log for partition _confluent-link-metadata-37 in /var/lib/kafka/data/_confluent-link-metadata-37 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,565] INFO [Partition _confluent-link-metadata-37 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-37 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,565] INFO [Partition _confluent-link-metadata-37 broker=1] Log loaded for partition _confluent-link-metadata-37 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,565] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-37 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,565] INFO [MergedLog partition=_confluent-link-metadata-37, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,565] INFO [Broker id=1] Leader _confluent-link-metadata-37 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,567] INFO [MergedLog partition=_confluent-link-metadata-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,567] INFO Created log for partition _confluent-link-metadata-17 in /var/lib/kafka/data/_confluent-link-metadata-17 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,567] INFO [Partition _confluent-link-metadata-17 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-17 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,567] INFO [Partition _confluent-link-metadata-17 broker=1] Log loaded for partition _confluent-link-metadata-17 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,567] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-17 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,567] INFO [MergedLog partition=_confluent-link-metadata-17, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,568] INFO [Broker id=1] Leader _confluent-link-metadata-17 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,570] INFO [MergedLog partition=_confluent-link-metadata-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,570] INFO Created log for partition _confluent-link-metadata-32 in /var/lib/kafka/data/_confluent-link-metadata-32 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,570] INFO [Partition _confluent-link-metadata-32 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-32 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,570] INFO [Partition _confluent-link-metadata-32 broker=1] Log loaded for partition _confluent-link-metadata-32 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,570] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-32 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,570] INFO [MergedLog partition=_confluent-link-metadata-32, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,570] INFO [Broker id=1] Leader _confluent-link-metadata-32 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,574] INFO [MergedLog partition=_confluent-link-metadata-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,575] INFO Created log for partition _confluent-link-metadata-31 in /var/lib/kafka/data/_confluent-link-metadata-31 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,575] INFO [Partition _confluent-link-metadata-31 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-31 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,575] INFO [Partition _confluent-link-metadata-31 broker=1] Log loaded for partition _confluent-link-metadata-31 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,575] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-31 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,575] INFO [MergedLog partition=_confluent-link-metadata-31, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,575] INFO [Broker id=1] Leader _confluent-link-metadata-31 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,576] INFO [MergedLog partition=_confluent-link-metadata-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,576] INFO Created log for partition _confluent-link-metadata-3 in /var/lib/kafka/data/_confluent-link-metadata-3 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,576] INFO [Partition _confluent-link-metadata-3 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-3 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,576] INFO [Partition _confluent-link-metadata-3 broker=1] Log loaded for partition _confluent-link-metadata-3 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,576] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-3 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,576] INFO [MergedLog partition=_confluent-link-metadata-3, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,576] INFO [Broker id=1] Leader _confluent-link-metadata-3 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,578] INFO [MergedLog partition=_confluent-link-metadata-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,578] INFO Created log for partition _confluent-link-metadata-16 in /var/lib/kafka/data/_confluent-link-metadata-16 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,578] INFO [Partition _confluent-link-metadata-16 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-16 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,578] INFO [Partition _confluent-link-metadata-16 broker=1] Log loaded for partition _confluent-link-metadata-16 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,578] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-16 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,578] INFO [MergedLog partition=_confluent-link-metadata-16, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,578] INFO [Broker id=1] Leader _confluent-link-metadata-16 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,579] INFO [MergedLog partition=_confluent-link-metadata-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,579] INFO Created log for partition _confluent-link-metadata-11 in /var/lib/kafka/data/_confluent-link-metadata-11 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,579] INFO [Partition _confluent-link-metadata-11 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-11 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,579] INFO [Partition _confluent-link-metadata-11 broker=1] Log loaded for partition _confluent-link-metadata-11 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,579] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-11 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,579] INFO [MergedLog partition=_confluent-link-metadata-11, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,579] INFO [Broker id=1] Leader _confluent-link-metadata-11 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,720] INFO [MergedLog partition=_confluent-link-metadata-48, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,721] INFO Created log for partition _confluent-link-metadata-48 in /var/lib/kafka/data/_confluent-link-metadata-48 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,721] INFO [Partition _confluent-link-metadata-48 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-48 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,721] INFO [Partition _confluent-link-metadata-48 broker=1] Log loaded for partition _confluent-link-metadata-48 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,721] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-48 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,721] INFO [MergedLog partition=_confluent-link-metadata-48, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,721] INFO [Broker id=1] Leader _confluent-link-metadata-48 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,722] INFO [MergedLog partition=_confluent-link-metadata-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,722] INFO Created log for partition _confluent-link-metadata-18 in /var/lib/kafka/data/_confluent-link-metadata-18 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,722] INFO [Partition _confluent-link-metadata-18 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-18 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,722] INFO [Partition _confluent-link-metadata-18 broker=1] Log loaded for partition _confluent-link-metadata-18 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,722] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-18 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,722] INFO [MergedLog partition=_confluent-link-metadata-18, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,722] INFO [Broker id=1] Leader _confluent-link-metadata-18 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,724] INFO [MergedLog partition=_confluent-link-metadata-35, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,724] INFO Created log for partition _confluent-link-metadata-35 in /var/lib/kafka/data/_confluent-link-metadata-35 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,724] INFO [Partition _confluent-link-metadata-35 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-35 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,724] INFO [Partition _confluent-link-metadata-35 broker=1] Log loaded for partition _confluent-link-metadata-35 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,724] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-35 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,724] INFO [MergedLog partition=_confluent-link-metadata-35, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,724] INFO [Broker id=1] Leader _confluent-link-metadata-35 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,726] INFO [MergedLog partition=_confluent-link-metadata-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,726] INFO Created log for partition _confluent-link-metadata-4 in /var/lib/kafka/data/_confluent-link-metadata-4 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,726] INFO [Partition _confluent-link-metadata-4 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-4 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,726] INFO [Partition _confluent-link-metadata-4 broker=1] Log loaded for partition _confluent-link-metadata-4 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,726] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-4 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,726] INFO [MergedLog partition=_confluent-link-metadata-4, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,726] INFO [Broker id=1] Leader _confluent-link-metadata-4 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,727] INFO [MergedLog partition=_confluent-link-metadata-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,727] INFO Created log for partition _confluent-link-metadata-38 in /var/lib/kafka/data/_confluent-link-metadata-38 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,727] INFO [Partition _confluent-link-metadata-38 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-38 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,727] INFO [Partition _confluent-link-metadata-38 broker=1] Log loaded for partition _confluent-link-metadata-38 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,727] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-38 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,727] INFO [MergedLog partition=_confluent-link-metadata-38, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,728] INFO [Broker id=1] Leader _confluent-link-metadata-38 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,729] INFO [MergedLog partition=_confluent-link-metadata-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,729] INFO Created log for partition _confluent-link-metadata-39 in /var/lib/kafka/data/_confluent-link-metadata-39 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,729] INFO [Partition _confluent-link-metadata-39 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-39 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,729] INFO [Partition _confluent-link-metadata-39 broker=1] Log loaded for partition _confluent-link-metadata-39 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,729] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-39 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,729] INFO [MergedLog partition=_confluent-link-metadata-39, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,729] INFO [Broker id=1] Leader _confluent-link-metadata-39 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,730] INFO [MergedLog partition=_confluent-link-metadata-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,730] INFO Created log for partition _confluent-link-metadata-34 in /var/lib/kafka/data/_confluent-link-metadata-34 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,730] INFO [Partition _confluent-link-metadata-34 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-34 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,730] INFO [Partition _confluent-link-metadata-34 broker=1] Log loaded for partition _confluent-link-metadata-34 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,730] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-34 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,730] INFO [MergedLog partition=_confluent-link-metadata-34, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,730] INFO [Broker id=1] Leader _confluent-link-metadata-34 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,731] INFO [MergedLog partition=_confluent-link-metadata-36, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,731] INFO Created log for partition _confluent-link-metadata-36 in /var/lib/kafka/data/_confluent-link-metadata-36 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,731] INFO [Partition _confluent-link-metadata-36 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-36 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,731] INFO [Partition _confluent-link-metadata-36 broker=1] Log loaded for partition _confluent-link-metadata-36 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,731] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-36 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,731] INFO [MergedLog partition=_confluent-link-metadata-36, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,732] INFO [Broker id=1] Leader _confluent-link-metadata-36 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,734] INFO [MergedLog partition=_confluent-link-metadata-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,734] INFO Created log for partition _confluent-link-metadata-43 in /var/lib/kafka/data/_confluent-link-metadata-43 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,734] INFO [Partition _confluent-link-metadata-43 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-43 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,734] INFO [Partition _confluent-link-metadata-43 broker=1] Log loaded for partition _confluent-link-metadata-43 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,734] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-43 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,734] INFO [MergedLog partition=_confluent-link-metadata-43, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,734] INFO [Broker id=1] Leader _confluent-link-metadata-43 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,736] INFO [MergedLog partition=_confluent-link-metadata-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,736] INFO Created log for partition _confluent-link-metadata-44 in /var/lib/kafka/data/_confluent-link-metadata-44 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,736] INFO [Partition _confluent-link-metadata-44 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-44 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,736] INFO [Partition _confluent-link-metadata-44 broker=1] Log loaded for partition _confluent-link-metadata-44 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,736] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-44 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,736] INFO [MergedLog partition=_confluent-link-metadata-44, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,736] INFO [Broker id=1] Leader _confluent-link-metadata-44 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,738] INFO [MergedLog partition=_confluent-link-metadata-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,738] INFO Created log for partition _confluent-link-metadata-22 in /var/lib/kafka/data/_confluent-link-metadata-22 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,738] INFO [Partition _confluent-link-metadata-22 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-22 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,738] INFO [Partition _confluent-link-metadata-22 broker=1] Log loaded for partition _confluent-link-metadata-22 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,738] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-22 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,738] INFO [MergedLog partition=_confluent-link-metadata-22, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,738] INFO [Broker id=1] Leader _confluent-link-metadata-22 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,739] INFO HV000001: Hibernate Validator 8.0.1.Final (org.hibernate.validator.internal.util.Version:%L)
[2025-06-22 05:26:39,740] INFO [MergedLog partition=_confluent-link-metadata-47, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,740] INFO Created log for partition _confluent-link-metadata-47 in /var/lib/kafka/data/_confluent-link-metadata-47 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,740] INFO [Partition _confluent-link-metadata-47 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-47 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,740] INFO [Partition _confluent-link-metadata-47 broker=1] Log loaded for partition _confluent-link-metadata-47 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,740] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-47 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,740] INFO [MergedLog partition=_confluent-link-metadata-47, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,740] INFO [Broker id=1] Leader _confluent-link-metadata-47 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,742] INFO [MergedLog partition=_confluent-link-metadata-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,742] INFO Created log for partition _confluent-link-metadata-2 in /var/lib/kafka/data/_confluent-link-metadata-2 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,743] INFO [Partition _confluent-link-metadata-2 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-2 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,743] INFO [Partition _confluent-link-metadata-2 broker=1] Log loaded for partition _confluent-link-metadata-2 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,743] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-2 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,743] INFO [MergedLog partition=_confluent-link-metadata-2, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,743] INFO [Broker id=1] Leader _confluent-link-metadata-2 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,744] INFO [MergedLog partition=_confluent-link-metadata-42, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,744] INFO Created log for partition _confluent-link-metadata-42 in /var/lib/kafka/data/_confluent-link-metadata-42 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,744] INFO [Partition _confluent-link-metadata-42 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-42 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,744] INFO [Partition _confluent-link-metadata-42 broker=1] Log loaded for partition _confluent-link-metadata-42 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,744] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-42 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,744] INFO [MergedLog partition=_confluent-link-metadata-42, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,744] INFO [Broker id=1] Leader _confluent-link-metadata-42 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,745] INFO [MergedLog partition=_confluent-link-metadata-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,745] INFO Created log for partition _confluent-link-metadata-9 in /var/lib/kafka/data/_confluent-link-metadata-9 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,745] INFO [Partition _confluent-link-metadata-9 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-9 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,745] INFO [Partition _confluent-link-metadata-9 broker=1] Log loaded for partition _confluent-link-metadata-9 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,745] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-9 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,745] INFO [MergedLog partition=_confluent-link-metadata-9, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,745] INFO [Broker id=1] Leader _confluent-link-metadata-9 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,747] INFO [MergedLog partition=_confluent-link-metadata-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,747] INFO Created log for partition _confluent-link-metadata-6 in /var/lib/kafka/data/_confluent-link-metadata-6 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,747] INFO [Partition _confluent-link-metadata-6 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-6 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,747] INFO [Partition _confluent-link-metadata-6 broker=1] Log loaded for partition _confluent-link-metadata-6 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,747] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-6 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,747] INFO [MergedLog partition=_confluent-link-metadata-6, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,747] INFO [Broker id=1] Leader _confluent-link-metadata-6 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,748] INFO [MergedLog partition=_confluent-link-metadata-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,748] INFO Created log for partition _confluent-link-metadata-5 in /var/lib/kafka/data/_confluent-link-metadata-5 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,748] INFO [Partition _confluent-link-metadata-5 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-5 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,748] INFO [Partition _confluent-link-metadata-5 broker=1] Log loaded for partition _confluent-link-metadata-5 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,748] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-5 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,748] INFO [MergedLog partition=_confluent-link-metadata-5, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,748] INFO [Broker id=1] Leader _confluent-link-metadata-5 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,749] INFO [MergedLog partition=_confluent-link-metadata-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,749] INFO Created log for partition _confluent-link-metadata-26 in /var/lib/kafka/data/_confluent-link-metadata-26 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,749] INFO [Partition _confluent-link-metadata-26 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-26 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,749] INFO [Partition _confluent-link-metadata-26 broker=1] Log loaded for partition _confluent-link-metadata-26 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,749] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-26 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,749] INFO [MergedLog partition=_confluent-link-metadata-26, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,749] INFO [Broker id=1] Leader _confluent-link-metadata-26 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,750] INFO [MergedLog partition=_confluent-link-metadata-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,750] INFO Created log for partition _confluent-link-metadata-10 in /var/lib/kafka/data/_confluent-link-metadata-10 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,750] INFO [Partition _confluent-link-metadata-10 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-10 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,750] INFO [Partition _confluent-link-metadata-10 broker=1] Log loaded for partition _confluent-link-metadata-10 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,750] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-10 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,750] INFO [MergedLog partition=_confluent-link-metadata-10, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,750] INFO [Broker id=1] Leader _confluent-link-metadata-10 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,751] INFO [MergedLog partition=_confluent-link-metadata-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,751] INFO Created log for partition _confluent-link-metadata-30 in /var/lib/kafka/data/_confluent-link-metadata-30 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,751] INFO [Partition _confluent-link-metadata-30 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-30 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,751] INFO [Partition _confluent-link-metadata-30 broker=1] Log loaded for partition _confluent-link-metadata-30 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,751] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-30 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,751] INFO [MergedLog partition=_confluent-link-metadata-30, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,751] INFO [Broker id=1] Leader _confluent-link-metadata-30 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,752] INFO [MergedLog partition=_confluent-link-metadata-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,753] INFO Created log for partition _confluent-link-metadata-14 in /var/lib/kafka/data/_confluent-link-metadata-14 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,753] INFO [Partition _confluent-link-metadata-14 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-14 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,753] INFO [Partition _confluent-link-metadata-14 broker=1] Log loaded for partition _confluent-link-metadata-14 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,753] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-14 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,753] INFO [MergedLog partition=_confluent-link-metadata-14, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,753] INFO [Broker id=1] Leader _confluent-link-metadata-14 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,754] INFO [MergedLog partition=_confluent-link-metadata-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,754] INFO Created log for partition _confluent-link-metadata-13 in /var/lib/kafka/data/_confluent-link-metadata-13 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,754] INFO [Partition _confluent-link-metadata-13 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-13 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,754] INFO [Partition _confluent-link-metadata-13 broker=1] Log loaded for partition _confluent-link-metadata-13 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,754] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-13 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,754] INFO [MergedLog partition=_confluent-link-metadata-13, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,754] INFO [Broker id=1] Leader _confluent-link-metadata-13 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,755] INFO [MergedLog partition=_confluent-link-metadata-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,755] INFO Created log for partition _confluent-link-metadata-1 in /var/lib/kafka/data/_confluent-link-metadata-1 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,755] INFO [Partition _confluent-link-metadata-1 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-1 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,755] INFO [Partition _confluent-link-metadata-1 broker=1] Log loaded for partition _confluent-link-metadata-1 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,755] INFO Setting topicIdPartition G-9S9N2rSd2_OyvjHChE3w:_confluent-link-metadata-1 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,755] INFO [MergedLog partition=_confluent-link-metadata-1, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-link-metadata-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,755] INFO [Broker id=1] Leader _confluent-link-metadata-1 with topic id Some(G-9S9N2rSd2_OyvjHChE3w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,756] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-link-metadata with new configuration : cleanup.policy -> compact,min.insync.replicas -> 2 (kafka.server.metadata.DynamicConfigPublisher:%L)
[2025-06-22 05:26:39,889] INFO Started oeje10s.ServletContextHandler@6d711502{/kafka,/kafka,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@2a49570f{STARTED}} (org.eclipse.jetty.ee10.servlet.ServletContextHandler:%L)
[2025-06-22 05:26:39,889] INFO Started oeje10s.ServletContextHandler@69b78c1e{/v1/metadata,/v1/metadata,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@2d6bbdbe{STARTED}} (org.eclipse.jetty.server.handler.ContextHandler:%L)
Jun 22, 2025 5:26:39 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider io.confluent.metadataapi.resources.MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.metadataapi.resources.MetadataResource will be ignored. 
[2025-06-22 05:26:39,896] INFO Started oeje10s.ServletContextHandler@69b78c1e{/v1/metadata,/v1/metadata,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@2d6bbdbe{STARTED}} (org.eclipse.jetty.ee10.servlet.ServletContextHandler:%L)
[2025-06-22 05:26:39,898] INFO Started oeje10s.ServletContextHandler@1f4dfaef{/ws,/ws,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@1058b396{STARTED}} (org.eclipse.jetty.server.handler.ContextHandler:%L)
[2025-06-22 05:26:39,898] INFO Started oeje10s.ServletContextHandler@1f4dfaef{/ws,/ws,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@1058b396{STARTED}} (org.eclipse.jetty.ee10.servlet.ServletContextHandler:%L)
[2025-06-22 05:26:39,898] INFO Started oeje10s.ServletContextHandler@7418981d{/ws,/ws,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@4b972d75{STARTED}} (org.eclipse.jetty.server.handler.ContextHandler:%L)
[2025-06-22 05:26:39,898] INFO Started oeje10s.ServletContextHandler@7418981d{/ws,/ws,b=null,a=AVAILABLE,h=oeje10s.SessionHandler@4b972d75{STARTED}} (org.eclipse.jetty.ee10.servlet.ServletContextHandler:%L)
[2025-06-22 05:26:39,899] INFO Getter/setter type mismatch for mbean attribute formEncodedMethods in class org.eclipse.jetty.server.HttpConfiguration, attribute will be read-only (org.eclipse.jetty.jmx.MetaData:%L)
[2025-06-22 05:26:39,900] INFO Started NetworkTrafficServerConnector@336a1881{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8090} (org.eclipse.jetty.server.AbstractConnector:%L)
[2025-06-22 05:26:39,900] INFO Started icr.ApplicationServer@62526b55{STARTING}[12.0.16,sto=5000] @1306ms (org.eclipse.jetty.server.Server:%L)
[2025-06-22 05:26:39,900] INFO KafkaHttpServer transitioned from STARTING to RUNNING.. (io.confluent.http.server.KafkaHttpServerImpl:%L)
[2025-06-22 05:26:39,900] INFO LicenseConfig values: 
	confluent.license = [hidden]
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 1
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,900] INFO LicenseConfig values: 
	confluent.license = [hidden]
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 1
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,901] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-1
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,901] INFO These configurations '[replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, min.insync.replicas, cluster.link.metadata.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,901] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,901] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,901] INFO Kafka startTimeMs: 1750569999901 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,916] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,917] INFO Starting License Store (io.confluent.license.LicenseStore:%L)
[2025-06-22 05:26:39,917] INFO Starting KafkaBasedLog with topic _confluent-command reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:%L)
[2025-06-22 05:26:39,917] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-1
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,917] INFO These configurations '[replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, min.insync.replicas, cluster.link.metadata.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,917] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,917] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,917] INFO Kafka startTimeMs: 1750569999917 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] Using the default placer, StripedReplicaPlacer, to make the assignment for topic _confluent-command. (kafka.assignor.ConfluentReplicaPlacer:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreatableTopicConfig(name='min.insync.replicas', value='1'), CreatableTopicConfig(name='cleanup.policy', value='compact')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-command with topic ID SxP9Ygb5TjS9pL_-neZ5jA. (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-command') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-command') which set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:26:39,920] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-command-0 with topic ID SxP9Ygb5TjS9pL_-neZ5jA and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:26:39,946] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger:%L)
[2025-06-22 05:26:39,946] INFO SBC Event SbcMetadataUpdateEvent-13 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-14]. Enqueuing... (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,946] INFO Handling event SbcConfigUpdateEvent-14 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,946] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-command')=ConfigurationDelta(changedKeys=[cleanup.policy, min.insync.replicas])}) (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,946] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager:%L)
[2025-06-22 05:26:39,946] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:26:39,946] INFO [Broker id=1] Creating new partition _confluent-command-0 with topic id SxP9Ygb5TjS9pL_-neZ5jA. (state.change.logger:%L)
[2025-06-22 05:26:39,946] INFO Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":1,"assignments":[],"configs":[{"name":"min.insync.replicas","value":"1"},{"name":"cleanup.policy","value":"compact"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"responseData":"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","errorCode":0},"connection":"127.0.0.1:29093-127.0.0.1:34176-2-0","clientAddress":"127.0.0.1","totalTimeMs":26.306,"requestQueueTimeMs":0.033,"localTimeMs":0.072,"remoteTimeMs":26.014,"throttleTimeMs":0,"responseQueueTimeMs":0.117,"sendTimeMs":0.069,"sendIoTimeMs":0.053,"responseSize":2538,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":175056999991900002} (kafka.request.logger:%L)
[2025-06-22 05:26:39,946] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 1 partitions (state.change.logger:%L)
[2025-06-22 05:26:39,946] INFO Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"_confluent-license-admin-1","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-command","numPartitions":1,"replicationFactor":1,"assignments":[],"configs":[{"name":"min.insync.replicas","value":"1"},{"name":"cleanup.policy","value":"compact"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-command","topicId":"SxP9Ygb5TjS9pL_-neZ5jA","errorCode":0,"errorMessage":null,"numPartitions":1,"replicationFactor":1,"configs":[{"configName":"cleanup.policy","value":"compact","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"compression.gzip.level","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.lz4.level","value":"9","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.type","value":"producer","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.zstd.level","value":"3","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.append.record.interceptor.classes","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.cluster.link.allow.legacy.message.format","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.compacted.topic.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.log.cleaner.timestamp.validation.enable","value":"true","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.max.segment.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.min.segment.ms","value":"1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.placement.constraints","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.schema.validation.context.name.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.segment.speculative.prefetch.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.delete.delay.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.max.deletions.per.run","value":"72","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.system.time.roll.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.min.efficiency","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.segment.min.bytes","value":"20971520","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.dual.compaction","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.min.cleanable.ratio","value":"0.75","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.bytes","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.segment.hotset.roll.min.bytes","value":"104857600","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.topic.type","value":"standard","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"delete.retention.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"file.delete.delay.ms","value":"60000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.messages","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"follower.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"index.interval.bytes","value":"4096","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"leader.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.bytes","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.ms","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.compaction.lag.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.message.bytes","value":"1048588","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.after.max.ms","value":"3600000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.before.max.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.type","value":"CreateTime","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.cleanable.dirty.ratio","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.compaction.lag.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.insync.replicas","value":"1","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"preallocate","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.copy.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.delete.on.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.storage.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"retention.bytes","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"retention.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.bytes","value":"1073741824","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.index.bytes","value":"10485760","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.jitter.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"unclean.leader.election.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false}]}]},"connection":"127.0.0.1:29092-127.0.0.1:53444-2-1","clientAddress":"127.0.0.1","totalTimeMs":26.746,"requestQueueTimeMs":0.036,"localTimeMs":0.047,"remoteTimeMs":26.573,"throttleTimeMs":0,"responseQueueTimeMs":0.042,"sendTimeMs":0.047,"sendIoTimeMs":0.035,"responseSize":2528,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"8.0.0-0-ce"},"isDisconnectedClient":false,"requestId":175056999991900202} (kafka.request.logger:%L)
[2025-06-22 05:26:39,946] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,947] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-consumer-1
	client.rack = 
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,947] INFO [MergedLog partition=_confluent-command-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:26:39,947] INFO Created log for partition _confluent-command-0 in /var/lib/kafka/data/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=1} (kafka.log.LogManager:%L)
[2025-06-22 05:26:39,947] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,947] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:26:39,948] INFO Setting topicIdPartition SxP9Ygb5TjS9pL_-neZ5jA:_confluent-command-0 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:26:39,948] INFO [MergedLog partition=_confluent-command-0, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-command-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:26:39,948] INFO [Broker id=1] Leader _confluent-command-0 with topic id Some(SxP9Ygb5TjS9pL_-neZ5jA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:26:39,948] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-command with new configuration : cleanup.policy -> compact,min.insync.replicas -> 1 (kafka.server.metadata.DynamicConfigPublisher:%L)
[2025-06-22 05:26:39,950] INFO These configurations '[confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, cluster.link.metadata.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,950] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,950] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,950] INFO Kafka startTimeMs: 1750569999950 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,951] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata:%L)
[2025-06-22 05:26:39,952] INFO App info kafka.consumer for _confluent-license-consumer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,952] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:29092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = false
	interceptor.classes = []
	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	linger.ms = 5
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,953] INFO These configurations '[cluster.link.metadata.topic.replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,953] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,953] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,953] INFO Kafka startTimeMs: 1750569999953 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,953] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-consumer-1
	client.rack = 
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,953] INFO These configurations '[cluster.link.metadata.topic.replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:39,953] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,953] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,953] INFO Kafka startTimeMs: 1750569999953 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:39,954] INFO [Producer clientId=_confluent-license-producer-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata:%L)
[2025-06-22 05:26:39,954] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata:%L)
[2025-06-22 05:26:39,954] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Assigned to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:%L)
[2025-06-22 05:26:39,954] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Seeking to AutoOffsetResetStrategy{type=earliest} offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:%L)
[2025-06-22 05:26:39,956] INFO Finished reading KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog:%L)
[2025-06-22 05:26:39,956] INFO Started KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog:%L)
[2025-06-22 05:26:39,956] INFO Started License Store (io.confluent.license.LicenseStore:%L)
[2025-06-22 05:26:39,995] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata:%L)
[2025-06-22 05:26:40,460] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-1
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:40,461] INFO These configurations '[replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, min.insync.replicas, cluster.link.metadata.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:40,461] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,461] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,461] INFO Kafka startTimeMs: 1750570000461 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,464] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,476] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-1
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:40,476] INFO These configurations '[replication.factor, confluent.link.metadata.topic.replication.factor, confluent.balancer.topics.replication.factor, confluent.command.topic.replication, min.insync.replicas, cluster.link.metadata.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:26:40,476] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,476] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,476] INFO Kafka startTimeMs: 1750570000476 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,479] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,479] INFO License for single cluster, single node (io.confluent.license.LicenseManager:%L)
[2025-06-22 05:26:40,480] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer:%L)
[2025-06-22 05:26:40,480] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,480] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,480] INFO Kafka startTimeMs: 1750570000480 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:26:40,480] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer:%L)
[2025-06-22 05:27:09,111] INFO Beginning log roller... (kafka.log.LogManager:%L)
[2025-06-22 05:27:09,112] INFO Log roller completed in 0 seconds (kafka.log.LogManager:%L)
[2025-06-22 05:27:38,954] INFO [ControllerServer id=1] In the last 60000 ms period, 338 controller events were completed, which took an average of 10.81 ms each. The slowest event was writeNoOpRecord(1648315861), which took 42.47 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:27:38,959] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:27:39,423] INFO AdminClientConfig values: 
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	confluent.admin.client.describe.topic.partitions.enabled = true
	confluent.client.switchover.disable = false
	confluent.lkc.id = null
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.mode = PROXY
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = false
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metadata.recovery.rebootstrap.trigger.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = [org.apache.kafka.common.metrics.JmxReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.jaas.config.jndi.allowlist = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.assertion.claim.aud = null
	sasl.oauthbearer.assertion.claim.exp.minutes = 5
	sasl.oauthbearer.assertion.claim.iss = null
	sasl.oauthbearer.assertion.claim.jti.include = false
	sasl.oauthbearer.assertion.claim.nbf.include = false
	sasl.oauthbearer.assertion.claim.sub = null
	sasl.oauthbearer.assertion.file = null
	sasl.oauthbearer.assertion.private.key.file = null
	sasl.oauthbearer.assertion.private.key.passphrase = null
	sasl.oauthbearer.assertion.template.file = null
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.iat.validation.enabled = false
	sasl.oauthbearer.jti.validation.enabled = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:27:39,428] INFO These configurations '[compression.type, enable.idempotence, acks, key.serializer, max.request.size, value.serializer, partitioner.class, interceptor.classes, max.in.flight.requests.per.connection, linger.ms]' were supplied but are not used yet. (org.apache.kafka.common.config.AbstractConfig:%L)
[2025-06-22 05:27:39,428] INFO Kafka version: 8.0.0-0-ce (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:27:39,428] INFO Kafka commitId: ae3653aa4c7c98fe (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:27:39,428] INFO Kafka startTimeMs: 1750570059428 (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:27:39,475] INFO [ControllerServer id=1] Using the default placer, StripedReplicaPlacer, to make the assignment for topic _confluent-telemetry-metrics. (kafka.assignor.ConfluentReplicaPlacer:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=1, assignments=[], configs=[CreatableTopicConfig(name='max.message.bytes', value='10485760'), CreatableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreatableTopicConfig(name='min.insync.replicas', value='1'), CreatableTopicConfig(name='retention.ms', value='259200000'), CreatableTopicConfig(name='segment.ms', value='14400000'), CreatableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-telemetry-metrics with topic ID WJs2_1GlS2iUCcZR9G1uRw. (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration max.message.bytes to 10485760 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration segment.ms to 14400000 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-0 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-1 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-2 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,477] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-3 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-4 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-5 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-6 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-7 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-8 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-9 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-10 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,478] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-11 with topic ID WJs2_1GlS2iUCcZR9G1uRw and PartitionRegistration(replicas=[1], observers=[], directories=[y2hnhBSrRN9M9zSpC-BgBg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager:%L)
[2025-06-22 05:27:39,505] INFO SBC Event SbcMetadataUpdateEvent-134 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-135]. Enqueuing... (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:27:39,505] INFO [Broker id=1] Transitioning 12 partition(s) to local leaders. (state.change.logger:%L)
[2025-06-22 05:27:39,505] INFO Handling event SbcConfigUpdateEvent-135 (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:27:39,505] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11, _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2) (kafka.server.ReplicaFetcherManager:%L)
[2025-06-22 05:27:39,505] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-3 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,505] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics')=ConfigurationDelta(changedKeys=[max.message.bytes, message.timestamp.type, min.insync.replicas, retention.ms, segment.ms, retention.bytes])}) (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:27:39,505] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent:%L)
[2025-06-22 05:27:39,506] INFO Completed request:{"isForwarded":true,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"confluent-telemetry-reporter-local-producer","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-telemetry-metrics","numPartitions":12,"replicationFactor":1,"assignments":[],"configs":[{"name":"max.message.bytes","value":"10485760"},{"name":"message.timestamp.type","value":"CreateTime"},{"name":"min.insync.replicas","value":"1"},{"name":"retention.ms","value":"259200000"},{"name":"segment.ms","value":"14400000"},{"name":"retention.bytes","value":"-1"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"responseData":"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","errorCode":0},"connection":"127.0.0.1:29093-127.0.0.1:34176-2-0","clientAddress":"127.0.0.1","totalTimeMs":31.856,"requestQueueTimeMs":0.077,"localTimeMs":0.199,"remoteTimeMs":31.216,"throttleTimeMs":0,"responseQueueTimeMs":0.147,"sendTimeMs":0.216,"sendIoTimeMs":0.169,"responseSize":2547,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"CONTROLLER","clientInformation":{"softwareName":"unknown","softwareVersion":"unknown"},"isDisconnectedClient":false,"requestId":175057005947300002} (kafka.request.logger:%L)
[2025-06-22 05:27:39,506] INFO Created telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter:%L)
[2025-06-22 05:27:39,506] INFO Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":19,"requestApiVersion":7,"correlationId":5,"clientId":"confluent-telemetry-reporter-local-producer","requestApiKeyName":"CREATE_TOPICS"},"request":{"topics":[{"name":"_confluent-telemetry-metrics","numPartitions":12,"replicationFactor":1,"assignments":[],"configs":[{"name":"max.message.bytes","value":"10485760"},{"name":"message.timestamp.type","value":"CreateTime"},{"name":"min.insync.replicas","value":"1"},{"name":"retention.ms","value":"259200000"},{"name":"segment.ms","value":"14400000"},{"name":"retention.bytes","value":"-1"}]}],"timeoutMs":30000,"validateOnly":false},"response":{"throttleTimeMs":0,"topics":[{"name":"_confluent-telemetry-metrics","topicId":"WJs2_1GlS2iUCcZR9G1uRw","errorCode":0,"errorMessage":null,"numPartitions":12,"replicationFactor":1,"configs":[{"configName":"cleanup.policy","value":"delete","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.gzip.level","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.lz4.level","value":"9","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.type","value":"producer","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"compression.zstd.level","value":"3","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.append.record.interceptor.classes","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.cluster.link.allow.legacy.message.format","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.compacted.topic.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.key.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.log.cleaner.timestamp.validation.enable","value":"true","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.max.segment.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.min.segment.ms","value":"1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.placement.constraints","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.prefer.tier.fetch.ms","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.schema.validation.context.name.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.segment.speculative.prefetch.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.delete.delay.ms","value":"604800000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.stray.log.max.deletions.per.run","value":"72","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.system.time.roll.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.min.efficiency","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.compact.segment.min.bytes","value":"20971520","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.dual.compaction","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.cleaner.min.cleanable.ratio","value":"0.75","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.bytes","value":"-1","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.local.hotset.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.tier.segment.hotset.roll.min.bytes","value":"104857600","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.topic.type","value":"standard","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.schema.validation","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"confluent.value.subject.name.strategy","value":"io.confluent.kafka.serializers.subject.TopicNameStrategy","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"delete.retention.ms","value":"86400000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"file.delete.delay.ms","value":"60000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.messages","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"flush.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"follower.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"index.interval.bytes","value":"4096","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"leader.replication.throttled.replicas","value":"","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.bytes","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"local.retention.ms","value":"-2","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.compaction.lag.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"max.message.bytes","value":"10485760","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"message.timestamp.after.max.ms","value":"3600000","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.before.max.ms","value":"9223372036854775807","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"message.timestamp.type","value":"CreateTime","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"min.cleanable.dirty.ratio","value":"0.5","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.compaction.lag.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"min.insync.replicas","value":"1","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"preallocate","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.copy.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.log.delete.on.disable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"remote.storage.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"retention.bytes","value":"-1","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"retention.ms","value":"259200000","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"segment.bytes","value":"1073741824","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.index.bytes","value":"10485760","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.jitter.ms","value":"0","readOnly":false,"configSource":5,"isSensitive":false},{"configName":"segment.ms","value":"14400000","readOnly":false,"configSource":1,"isSensitive":false},{"configName":"unclean.leader.election.enable","value":"false","readOnly":false,"configSource":5,"isSensitive":false}]}]},"connection":"127.0.0.1:29092-127.0.0.1:60732-1-5","clientAddress":"127.0.0.1","totalTimeMs":33.626,"requestQueueTimeMs":0.091,"localTimeMs":0.503,"remoteTimeMs":32.793,"throttleTimeMs":0,"responseQueueTimeMs":0.088,"sendTimeMs":0.148,"sendIoTimeMs":0.108,"responseSize":2537,"securityProtocol":"PLAINTEXT","principal":{"class":"KafkaPrincipal","type":"User","name":"ANONYMOUS","tokenAuthenticated":false},"listener":"PLAINTEXT","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"8.0.0-0-ce"},"isDisconnectedClient":false,"requestId":175057005947200001} (kafka.request.logger:%L)
[2025-06-22 05:27:39,506] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser:%L)
[2025-06-22 05:27:39,507] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-4 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,508] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-5 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,509] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-6 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,510] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-7 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,510] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-8 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,511] INFO Partitioner has null list of partitions to produce to. Calculating partitions to produce to (io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner:%L)
[2025-06-22 05:27:39,511] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-9 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,511] INFO Kafka Producer producing to the following subset partitions: {_confluent-telemetry-metrics=[5, 11]} (io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner:%L)
[2025-06-22 05:27:39,512] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-10 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,512] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-11 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,513] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-0 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,514] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-1 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,515] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-2 with topic id WJs2_1GlS2iUCcZR9G1uRw. (state.change.logger:%L)
[2025-06-22 05:27:39,516] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 12 partitions (state.change.logger:%L)
[2025-06-22 05:27:39,519] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,520] INFO Created log for partition _confluent-telemetry-metrics-7 in /var/lib/kafka/data/_confluent-telemetry-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,521] INFO [Partition _confluent-telemetry-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-7 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,521] INFO [Partition _confluent-telemetry-metrics-7 broker=1] Log loaded for partition _confluent-telemetry-metrics-7 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,521] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-7 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,521] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,521] INFO [Broker id=1] Leader _confluent-telemetry-metrics-7 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,524] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,524] INFO Created log for partition _confluent-telemetry-metrics-0 in /var/lib/kafka/data/_confluent-telemetry-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,524] INFO [Partition _confluent-telemetry-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,524] INFO [Partition _confluent-telemetry-metrics-0 broker=1] Log loaded for partition _confluent-telemetry-metrics-0 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,524] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-0 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,524] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,525] INFO [Broker id=1] Leader _confluent-telemetry-metrics-0 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,527] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,528] INFO Created log for partition _confluent-telemetry-metrics-1 in /var/lib/kafka/data/_confluent-telemetry-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,528] INFO [Partition _confluent-telemetry-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-1 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,528] INFO [Partition _confluent-telemetry-metrics-1 broker=1] Log loaded for partition _confluent-telemetry-metrics-1 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,528] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-1 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,528] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,528] INFO [Broker id=1] Leader _confluent-telemetry-metrics-1 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,530] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,531] INFO Created log for partition _confluent-telemetry-metrics-5 in /var/lib/kafka/data/_confluent-telemetry-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,531] INFO [Partition _confluent-telemetry-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-5 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,531] INFO [Partition _confluent-telemetry-metrics-5 broker=1] Log loaded for partition _confluent-telemetry-metrics-5 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,531] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-5 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,531] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,531] INFO [Broker id=1] Leader _confluent-telemetry-metrics-5 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,533] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,534] INFO Created log for partition _confluent-telemetry-metrics-11 in /var/lib/kafka/data/_confluent-telemetry-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,534] INFO [Partition _confluent-telemetry-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-11 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,534] INFO [Partition _confluent-telemetry-metrics-11 broker=1] Log loaded for partition _confluent-telemetry-metrics-11 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,534] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-11 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,534] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,534] INFO [Broker id=1] Leader _confluent-telemetry-metrics-11 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,536] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,537] INFO Created log for partition _confluent-telemetry-metrics-9 in /var/lib/kafka/data/_confluent-telemetry-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,537] INFO [Partition _confluent-telemetry-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-9 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,537] INFO [Partition _confluent-telemetry-metrics-9 broker=1] Log loaded for partition _confluent-telemetry-metrics-9 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,537] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-9 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,537] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,537] INFO [Broker id=1] Leader _confluent-telemetry-metrics-9 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,539] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,539] INFO Created log for partition _confluent-telemetry-metrics-3 in /var/lib/kafka/data/_confluent-telemetry-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,539] INFO [Partition _confluent-telemetry-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-3 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,539] INFO [Partition _confluent-telemetry-metrics-3 broker=1] Log loaded for partition _confluent-telemetry-metrics-3 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,539] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-3 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,539] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,539] INFO [Broker id=1] Leader _confluent-telemetry-metrics-3 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,541] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,541] INFO Created log for partition _confluent-telemetry-metrics-8 in /var/lib/kafka/data/_confluent-telemetry-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,542] INFO [Partition _confluent-telemetry-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-8 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,542] INFO [Partition _confluent-telemetry-metrics-8 broker=1] Log loaded for partition _confluent-telemetry-metrics-8 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,542] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-8 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,542] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,542] INFO [Broker id=1] Leader _confluent-telemetry-metrics-8 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,544] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,545] INFO Created log for partition _confluent-telemetry-metrics-2 in /var/lib/kafka/data/_confluent-telemetry-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,545] INFO [Partition _confluent-telemetry-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-2 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,545] INFO [Partition _confluent-telemetry-metrics-2 broker=1] Log loaded for partition _confluent-telemetry-metrics-2 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,545] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-2 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,545] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,545] INFO [Broker id=1] Leader _confluent-telemetry-metrics-2 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,547] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,548] INFO Created log for partition _confluent-telemetry-metrics-6 in /var/lib/kafka/data/_confluent-telemetry-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,548] INFO [Partition _confluent-telemetry-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-6 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,548] INFO [Partition _confluent-telemetry-metrics-6 broker=1] Log loaded for partition _confluent-telemetry-metrics-6 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,548] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-6 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,548] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,548] INFO [Broker id=1] Leader _confluent-telemetry-metrics-6 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,549] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,550] INFO Created log for partition _confluent-telemetry-metrics-10 in /var/lib/kafka/data/_confluent-telemetry-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,550] INFO [Partition _confluent-telemetry-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-10 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,550] INFO [Partition _confluent-telemetry-metrics-10 broker=1] Log loaded for partition _confluent-telemetry-metrics-10 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,550] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-10 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,550] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,550] INFO [Broker id=1] Leader _confluent-telemetry-metrics-10 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,552] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.MergedLogUtils:%L)
[2025-06-22 05:27:39,552] INFO Created log for partition _confluent-telemetry-metrics-4 in /var/lib/kafka/data/_confluent-telemetry-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager:%L)
[2025-06-22 05:27:39,552] INFO [Partition _confluent-telemetry-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-4 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,552] INFO [Partition _confluent-telemetry-metrics-4 broker=1] Log loaded for partition _confluent-telemetry-metrics-4 with initial high watermark 0 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,552] INFO Setting topicIdPartition WJs2_1GlS2iUCcZR9G1uRw:_confluent-telemetry-metrics-4 (kafka.tier.state.FileTierPartitionState:%L)
[2025-06-22 05:27:39,553] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/var/lib/kafka/data] Initializing tier metadata without recovery for _confluent-telemetry-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog:%L)
[2025-06-22 05:27:39,553] INFO [Broker id=1] Leader _confluent-telemetry-metrics-4 with topic id Some(WJs2_1GlS2iUCcZR9G1uRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger:%L)
[2025-06-22 05:27:39,554] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-telemetry-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher:%L)
[2025-06-22 05:27:39,574] INFO [Partition _confluent-telemetry-metrics-5 broker=1] roll: _confluent-telemetry-metrics-5: first produce received, lastOffset: 24, leaderEpoch: 0, numMessages:25, time diff: 57 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,574] INFO [Partition _confluent-telemetry-metrics-5 broker=1] roll: _confluent-telemetry-metrics-5: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 25, become leader time: 1750570059516 ms, time diff: 58 ms (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,574] INFO [Partition _confluent-telemetry-metrics-11 broker=1] roll: _confluent-telemetry-metrics-11: first produce received, lastOffset: 19, leaderEpoch: 0, numMessages:20, time diff: 58 (kafka.cluster.Partition:%L)
[2025-06-22 05:27:39,574] INFO [Partition _confluent-telemetry-metrics-11 broker=1] roll: _confluent-telemetry-metrics-11: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 20, become leader time: 1750570059516 ms, time diff: 58 ms (kafka.cluster.Partition:%L)
[2025-06-22 05:28:38,955] INFO [ControllerServer id=1] In the last 60000 ms period, 326 controller events were completed, which took an average of 10.68 ms each. The slowest event was writeNoOpRecord(1613906694), which took 42.39 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:28:38,959] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:29:38,961] INFO [ControllerServer id=1] In the last 60000 ms period, 322 controller events were completed, which took an average of 10.77 ms each. The slowest event was writeNoOpRecord(475995535), which took 37.67 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:29:38,961] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:30:38,961] INFO [ControllerServer id=1] In the last 60000 ms period, 324 controller events were completed, which took an average of 10.94 ms each. The slowest event was writeNoOpRecord(1735527281), which took 50.26 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:30:38,962] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:31:38,958] INFO [ControllerServer id=1] Periodic task electUnclean generated 0 records in 45 microseconds. (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:31:38,963] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:31:38,963] INFO [ControllerServer id=1] In the last 60000 ms period, 326 controller events were completed, which took an average of 10.57 ms each. The slowest event was writeNoOpRecord(1254791513), which took 39.55 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:31:39,629] INFO [AdminClient clientId=cluster-link--local-admin-1] Cancelled in-flight METADATA request with correlation id 4 due to node 1 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:32:09,113] INFO Beginning log roller... (kafka.log.LogManager:%L)
[2025-06-22 05:32:09,114] INFO Log roller completed in 0 seconds (kafka.log.LogManager:%L)
[2025-06-22 05:32:38,965] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:32:38,965] INFO [ControllerServer id=1] In the last 60000 ms period, 322 controller events were completed, which took an average of 11.09 ms each. The slowest event was writeNoOpRecord(1969361712), which took 40.37 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:33:38,969] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:33:38,970] INFO [ControllerServer id=1] In the last 60000 ms period, 322 controller events were completed, which took an average of 11.21 ms each. The slowest event was writeNoOpRecord(1221033970), which took 63.11 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:34:38,969] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:34:38,971] INFO [ControllerServer id=1] In the last 60000 ms period, 324 controller events were completed, which took an average of 10.96 ms each. The slowest event was writeNoOpRecord(136740768), which took 42.74 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:35:38,972] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:35:38,973] INFO [ControllerServer id=1] In the last 60000 ms period, 322 controller events were completed, which took an average of 10.98 ms each. The slowest event was writeNoOpRecord(305895777), which took 43.84 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:36:38,959] INFO [ControllerServer id=1] Periodic task electUnclean generated 0 records in 52 microseconds. (org.apache.kafka.controller.PeriodicTaskControlManager:%L)
[2025-06-22 05:36:38,972] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:36:38,981] INFO [ControllerServer id=1] In the last 60000 ms period, 325 controller events were completed, which took an average of 10.68 ms each. The slowest event was writeNoOpRecord(64665598), which took 36.71 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:36:39,168] INFO [NodeToControllerChannelManager id=1 name=registration] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:37:09,114] INFO Beginning log roller... (kafka.log.LogManager:%L)
[2025-06-22 05:37:09,115] INFO Log roller completed in 0 seconds (kafka.log.LogManager:%L)
[2025-06-22 05:37:38,979] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:37:38,986] INFO [ControllerServer id=1] In the last 60000 ms period, 322 controller events were completed, which took an average of 10.81 ms each. The slowest event was writeNoOpRecord(232162290), which took 76.26 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:37:39,526] INFO [NodeToControllerChannelManager id=1 name=forwarding] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:%L)
[2025-06-22 05:38:38,989] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:38:38,997] INFO [ControllerServer id=1] In the last 60000 ms period, 324 controller events were completed, which took an average of 10.77 ms each. The slowest event was writeNoOpRecord(829983948), which took 76.14 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:39:38,991] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:39:39,002] INFO [ControllerServer id=1] In the last 60000 ms period, 323 controller events were completed, which took an average of 10.87 ms each. The slowest event was writeNoOpRecord(738396655), which took 45.74 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)
[2025-06-22 05:40:38,997] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher:%L)
[2025-06-22 05:40:39,003] INFO [ControllerServer id=1] In the last 60000 ms period, 324 controller events were completed, which took an average of 10.95 ms each. The slowest event was writeNoOpRecord(393890521), which took 58.23 ms. (org.apache.kafka.controller.EventPerformanceMonitor:%L)